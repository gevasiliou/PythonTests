AWK:CHEATS BY GV
http://awk.freeshell.org/ #also check freenod irc - channel awk
http://awk.freeshell.org/AwkTips
https://www.gnu.org/software/gawk/manual/gawk.html
https://www.tutorialspoint.com/awk/awk_arithmetic_functions.htm
http://www.pement.org/awk.htm
http://www.theunixschool.com/2012/09/grep-vs-awk-examples-for-pattern-search.html
https://www.yumpu.com/en/document/view/25827537/sed-and-awk-101-hacks#
https://stackoverflow.com/tags/awk/info

AWK reserved variables :http://www.thegeekstuff.com/2010/01/8-powerful-awk-built-in-variables-fs-ofs-rs-ors-nr-nf-filename-fnr/?ref=binfind.com/web

https://www.shortcutfoo.com/app/dojos/awk/cheatsheet
http://web.mit.edu/gnu/doc/html/gawk_5.html

AWK XML parsing: http://awk.freeshell.org/XMLScraping

Idiomatic AWK: http://backreference.org/2010/02/10/idiomatic-awk/

AWK:CHEAT SHEET TXT 2 MANPAGE FORMATTING
##READ THIS CHEAT FILE WITH MAN PAGES:
http://technicalprose.blogspot.gr/2011/06/how-to-write-unix-man-page.html
man formatting: man 7 man & man 7 man-pages
groff programming: http://web.cecs.pdx.edu/~trent/gnu/groff/groff.html#IDX123
man pages making: https://liw.fi/manpages/
https://linux.die.net/man/1/help2man

##WORKING COMMAND:
man --nj <(h=".TH man 1 2017 1.0 cheats page";sed "1i $h" cheatsheets/utils*gv.txt |sed 's/^UTILS:/.SH UTILS:/g; s/^$/\.LP/g; s/^##/\.SS /g; s/\\/\\e/g;G' |sed 's/^$/\.br/g')
You can also combine with --nh 
PS: man options --nj = not auto justified , --nh = not auto break words with hyphen on line changes.

##MAN AND GROFF/troff require special handling.
man ignores normal line feeds at end of lines ($); empty lines (^$) are recognized and displayed
Line feeds in man pages can be done by inserting .br between two lines.
More .br in series of lines are ignored by man and got intepreted as a single line feed - not multiple new lines.
Man pages should start with a .TH line
Man sections / header start with .SH. 
Subsection start with .SS. Alternativelly you can use .B to make this line bold. .B follows text identation - .SS has it's own idents.
The backslash \ works as escape in groff, so you need to escape the backslash with \e (or \\ can also work)
The example tr -d '\n' will become tr -d '\en' with \e escaping, or will become tr -d '\\n' with \\ escaping.

For sed explanation see the sed cheatsheet.

AWK:BASICS
Depending on the application you can call AWK once to load the file and then you can manipulate the fields within AWK.
The whole awk syntax is based on the patter awk 'condition1{action1}condition2{action2}...'
If action is missing then print $0 is performed (default actions) (i.e awk 'NR==3' :prints the third line
If condition is missing then condition 1=true is used (default) and the action is always performed. i.e awk '{print $2}'

Typical usage advantage is when you need to read multiple patterns / values /columns / data from the same file.
If you do that with loop & grep you most probably it will be necessary to grep many times the same file and this makes the script slow.
Instead you can just once AWK the file and do whatever nanipulation you need inside AWK.
For complicated data manipulation is usual to have a seperate file full with AWK code and then call AWK with -f flag (=from file) to apply the code in your file/input
Remember the 48H log example that you need to see events logged in any minute of the 48H time frame. The use of loop and grep per minute leads to 3000 greps of the file, while you can do it with one AWK access.

##AWK RESERVED WORDS
awk '/pattern/ {action}' file         # Execute action for matched 'pattern' condition 
$0                                     # Reference current record line
FS                                     # Field separator of input file (default whitespace).
                                    # If unset (FS="" of -F"") then field separator is single char (like grep '.')
                                    # FS can accept values like FS="\x22" for double quotes or FS="\42" in octal
-F                                     # Command line option to specify input field delimiter before starting awk script.
OFS             # Output Field separator (default whitespace). Mind that this can be assigned in command line like awk '{your script}' OFS=: file or awk -F" " -v OFS="\t"
NF                 # Number of fields in current record/line (i.e {print $NF})
NR                 # Record Line number of the input stream. It keeps increasing in the second file if provided (awk '...' file1 file2)
FNR             # Reference number of the current record relative to current input file . If more than one file is given in awk, it is reset
FILENAME         # Reference current input file
ORS             # Output Record separator (default newline). Can be set to another value, or can be used to avoid double print (print a;print$0 <==> print aORS$0)
RS                 # Record separator of input file (default newline)
RT                # Record Terminator in use. Holds the RS in use - dynamically updated to keep the current used separator/terminator. Thus it's setting varies acc to multi RS value , i.e RS=":|\n"
FPAT            # FPAT = "([^,]+)|(\"[^\"]+\")"  -Defines what the field contains. FS (field separator) defines what a field does NOT contain.
FIELDWIDTHS     # Defines a fixed field width. Goes to BEGIN section.
^ , $                             # Match beginning/end of field
==                                 # Equal comparison operator for if
~                                 # Match regex opterator for if (i.e if ($2 ~ /thing/) print $0 - will print lines in which $2 matches anything,something,etc - may require double quotes) 
!~                                 # Regex not equal operator - negation of previous ~ . Alternative : (!($1~/pattern/))
BEGIN                                 # Denotes block executed once at start of the script (No input processed yet)
END                                 # Denotes block executed once at end of the script - after all files read
a=str1 OFS str2                     # Space Concatenates str1 , OFS and str2
{print $1}                             # Prints the first field of input (as read using FS separator)
{print $1,$2; print $3,$4}         # Prints field 1 & 2 in one line, and field 3 & 4 on next line.(print sthing with ; = "\n") 
'BEGIN {FS=OFS="\t"};             # Sets Input Field Separator (FS) and Output Separator (OFS) to tab. This works ony for printing fields/texts separated with comma (i.e print $1,$2) and does NOT work for $0 since $0 is one big field.
while (cond) {actions}                # While loop. Do is not required. Brackets not required for one command. 
do {actions} while (conditions)        # Alternative while loop syntax. Break, Continue and exit can be used in loops 
awk -c                              # This switch disables all gnu extensions and runs as the old awk - based on the book

##AWK BUILTIN FUNCTIONS
https://www.gnu.org/software/gawk/manual/html_node/String-Functions.html
http://awk.freeshell.org/Backreferences   #apply to gensub,etc

sub(regex, sub, string)         # Substitute sub for first occurrence of regex in string  ($0 is used if ommited)

sub & gsub(r,t,s)               #Substitute t for all occurrences of regex r in string s - Accepts & in replacement part within double quotes= backref to replaced part. Does not accept \\1 like gensub

gensub(regexp, replacement, how [, target])  #Accepts a target other than current FNR (vs sub and gsub who work with FNR) and return the modified string instead of the number of substitutions. Seems that does not accept \& but accepts groups like \\1,\\2

substr(str, start, l)   # Returns substring of string str, starting at index start of length l. If length is omitted, the suffix of str starting at index start is returned.

system(cmd)             # Execute cmd and return exit status. Usefull if you need to insert a file between records (instead to load in memory, cat it)

tolower(s)              # String s to lowercase

toupper(s)              # String s to uppercase

getline                 # Set $0 to next input record from current input file. 

getline <file2          # Goes to next input record of file2.From this point onwards, awk fields $0,$1, etc refer to file2
getline var <file2      # Read file2 and assign to var the whole record/line of file2 ($0). Without <file current file is used

next                    # stops processing and goes to next record of the current file

nextfile                # goes to the next file - changes the flow of the script (does not return back)

operators               # https://www.tutorialspoint.com/awk/awk_operators.htm

length(s)               #return the length of s. Works even with arrays,fields,etc

match(s ,r[,a])         #return index of where s matches r or 0 if there is no match; set RSTART and RLENGTH.If array a is given is filled with matches

split(s,a,r,seps)       #splits s into array a on regex r;returns number of fields; seps is an array holding each separator used (similar to RT for records)

sprintf (fmt, list)     #return list formatted according to fmt.can be assigned to variable (printf can not do that)

substr ( s, i, n)       #return the n-character substring of s starting at i;if n is omitted, return the suffix of s starting at i

index(s, t)             #Return  the index of the string t in the string s, returns 0 if t is not present.

simple conditions checks       # Can be expressed like awk '$5 == "abc123"' - prints lines that field5 match abc123. Print is ommited since it is the default awk action
advanced conditions checks     # {if (expression) {commands for true} else {commands for false}}
expr?true:false                # Conditional Expression - ternary operator. expression is evaluated and true command is run if expression=true
Directories on the command line are fatal for standard awk; gawk ignores them if not in POSIX mode.
convert a field to number : add 0 to it
convert a field to string : concatenate with ""

AWK:FUNCTIONS EXAMPLES
awk '{print "NR:",NR,"FNR:",FNR,"fname:",FILENAME,"Field:",$1}' file1 file2                   #Both files will be printed in series

awk 'BEGIN{a[1]="g";a[1]="v" FS a[1];print a[1]}'            # prints "v g" . Value of a[1] is replaced by a new value and the old value of a[1]. Mind the gap between new oldvalue that means concatenate.

awk 'BEGIN{a="g";a="v" FS a;print a}'                        #Same as above but without arrays. Needs space since space is concatenate operator in awk

awk '{getline <"file2";print $1}{print $1}' file1           #Reading is done by file2 , all fields correspond to file2

awk '{getline var2 <"file2";print var2}{print $1}' file1    #each line of file2 goes to var2 unsplit but now all Fields ($1,etc) refer to file1.

awk '{getline n;print $1,$2}' file                          #For a file with five lines, second and forth line is stored in var n , 1st,3rd and 5th lines are printed.

awk '{getline n;print n;print $1,$2}' file                  # Prints lines 2(n)-1($1,$2)-4(n)-3($1,$2)-4(n)-5($1,$2)

awk '{getline n;print $1,$2;print n}' file                  # Prints lines 1-2-3-4-5 ($1,$2 in series) and then line 4 (last n). Mind that print n in the end prints only after the file is finished

awk '{getline n1;getline n2;print $1,$2}' file              #Prints lines 1 and 4. getlne n1 gets line2 , getline n2 gets line3, line1 is printed, line4 is printed, line5 goes to n1 - no more lines - end.

awk '{a=$1;getline;print a,$2}' file                        # a gets $1 of current line1. getline goes to line2. prints a($1 of line1) and $2 of line2.Next iteration goes to line 3 - a=$1 of line3 , getline gets line4 , print a,$2 prints $1 of line3 and $2 of line4.

awk '{if ($1>="3") nextfile;print $0}' file2 file1          # This prints line 1 and 2 from file2 and then line 1 and 2 from file1, no matter if file1 has more than 3 lines. Can be usefull with regex match (~)

awk '{print ($0==p?"":$0); p=$0}' file                      # Checks consecutive lines;print with condition check. If $0 equals to var p then print "" else print $0. At the end assign p=$0.

awk 'BEGIN {while ((getline a[++c] < "file1") > 0) { } print c}' #Reads file1 and prints the total number of lines. Mind the array indexes.

PROCINFO["/dev/stdin", "READ_TIMEOUT"] = 5000
while ((getline < "/dev/stdin") > 0) print $0               # Read user input with timeout

awk 'BEGIN{system("echo 1 > f")}'

awk 'BEGIN{ORS="\n\n"}{print}' ./tmp/file4                    # Double space a file. ORS=Output Record Separator

##SHORTHAND NUMERIC ASSIGNMENTS
https://www.tutorialspoint.com/awk/awk_assignment_operators.htm
$ echo "1 2 3" |awk '{$NF *=10;print}' # Equivalent to $NF=$NF*10   ----> 1 2 30
$ echo "1 2 3" |awk '{$NF +=10;print}' # Equivalent to $NF=$NF+10   ----> 1 2 13
$ echo "1 2 3" |awk '{$NF -=10;print}' # Equivalent to $NF=$NF-10   ----> 1 2 -7
$ echo "1 2 3" |awk '{$NF /=10;print}' # Equivalent to $NF=$NF/10   ----> 1 2 0.3
# echo "1 2 3" |awk '{$NF ^=2;print}'  # Equivalent to $NF=$NF^2    ----> 1 2 9
# echo "1 2 19" |awk '{$NF %=10;print}'# Equivalent to $NF=$NF%10    ----> 1 2 9
Modulo: Divide 19 by 10 and return the remaining decimal part. 19/10 = 1.9 --> returns 9
Modulo can be used to automate the case "every Nth line". For example every 2nd line print the line
$ awk 'NR%2==0{print NR,$0}' file9
2 aaaaaaaaa
4 > Charles Darwin
6 bbbbbbb
8 > Charles Chaplin

Or even "do not print every 2nd line":
$ awk 'NR%2!=0{print NR,$0}' file9
1 > Rosa Luxemburg
3 aaaaaaaaaa
5 bbbbb
7 bb
9 cccc

Alternative: awk 'NR%2{print NR,$0}' file9
This will skip by default the lines where the division NR/2 leaves 0 remainders since in awk when an arithmetic
expression equals to zero (or not set) is evaluated as FALSE and thus does not perform the coming action.

.B More shorthand assignments:
$ echo "1 2 3" |awk '{$NF++;print}'    # Equivalent to $NF=$NF+1    ----> 1 2 4
$ echo "1 2 3" |awk '{$NF--;print}'    # Equivalent to $NF=$NF-1    ----> 1 2 2

.B A GREAT MODULO EXERCISE
Description: If we list all the natural numbers below 10 that are multiples of 3 or 5, we get 3, 5, 6 and 9. 
The sum of the multiples is 23. Find the sum of all the multiples of 3 or 5 below 1000.

Solution:
$ awk 'BEGIN{for (i=1;i<1000;i++) {if (i%3==0 || i%5==0) {print i;sum+=i}};print sum}'

Golfing Solutions: 
$ awk '$1%3==0 || $1%5==0{sum+=$1}END{print sum}' <(seq 999)
$ awk '!($1%3) || !($1%5){sum+=$1}END{print sum}' <(seq 999)
$ awk '($1%3 && $1%5)==0{sum+=$1}END{print sum}' <(seq 999)
$ awk '!($1%3 && $1%5){sum+=$1}END{print sum}' <(seq 999)

##FIELD SEPARATOR - SPECIAL USE OF WHITESPACE
awk 'BEGIN{FS=" ";OFS="_"}{$1=$1}1' <<<"xxx ccc vvv bbb        333  444  555"    ---> xxx_ccc_vvv_bbb_333_444_555
awk -v OFS="_" '{$1=$1}1' <<<"xxx ccc vvv bbb        333  444  555"              ---> xxx_ccc_vvv_bbb_333_444_555
awk 'BEGIN{FS="\\s";OFS="_"}{$1=$1}1' <<<"xxx ccc vvv bbb        333  444  555"  ---> xxx_ccc_vvv_bbb________333__444__555

##FIELDS DEFINITION WITH FPAT (define what a field is)
Exercise: Print the last numeric chars of a string/line
grep solution: 
gv@debian:~$ echo 234ef85 |grep -Eo '[0-9]+$'          ## --> 85
gv@debian:~$ echo 234ef856 |grep -Eo '[0-9]+$'         ## --> 856
gv@debian:~$ echo 234ef856a |grep -Eo '[0-9]+$'        ## --> no result

FPAT Solution: Define that fields are only numeric chars 
echo djfs1d2.3 | awk -v FPAT="[0-9]+" '{print $NF}'    ##--> 3
echo djfs1d2.3asdf | awk -v FPAT="[0-9]+" '{print $NF}'    ##--> 3 (last numeric chars found on string)
(the last behavior may or may not be desirable)

##FIELDWIDTHS - Fixed width fields
When using fixed widths the FS is not used and width is given in chars.
awk 'BEGIN{FIELDWIDTHS="4 4";OFS=","}{$1=$1}1' <<<"02 4170529"   ---> 02 4,1705    #29 in the end is ignored. 
Space is treated as char == part of the field.                       |_$1_||_$2_|
                       

##GSUB & SUB BACKEREFERENCE TO REPLACED PART
awk -F":" '{gsub(/^.[^:]+/,"*&*")}1' <<<"Paco Gutierrez:835-365-1284:454"  #Output: *Paco Gutierrez*:835-365-1284:454
Backreference to work must be in double quotes. Can also be written as sub(/pattern/,"*" "&" "*")

echo "123 abc" |awk '{sub(/^[0-9]/,"-&")}1'   ##Output: -123 abc

Alternative: awk -F":" '{gsub($1,"*" $1 "*")}1'
By default sub/gsub apply substitutions in $0. You can define a different "target" in the end (a specific field or a text or whatever).

Using sub/gsub the $0 structure is NOT affected.
So in above examples, though that OFS is undefined = default = space , the output of 1 prints $0 as has been replaced by sub, but without reconstruction the output with OFS.

The sub/gsub bug:
a command like sub($5,"something") you might think that will replace fifth field with something.
In reality, this will replace the text of $5 found in $0 with something.

Example : echo "123 abcdef ab" |awk '{sub($3,"some")}1'   ## 123 somecdef ab
sub replaced first found text of $3 = ab in $0, and that was on field 2.
If you provide $3 in sub like awk '{sub($3,"some",$3)}' you force sub to perform replace on $3 but this technique WILL REDESIGN the whole $0 - Structure of $0 is altered.

##AWK FIELDS SPLITTING - SPLIT FUNCTION 
http://awk.freeshell.org/Frequently_Asked_Questions#toc1
http://awk.freeshell.org/RangeOfFields   #more field splitting techniques in this link

You can set FS to multiple chars like -F"[:;]" and the $0 will be splitted in many pieces.
Though there is no way to refer to the delimiter used for each fiels. Even if you print FS it will print literally [:;].
As a result, in such situation the output will respect OFS (space by default) and will be a mess.

A good alternative similar to RS and RT (=last separator used to split records) is to use SPLIT function .
Split will split the $0 in pieces according to a customized regex, and will store each piece in an array and each separator used for splitting 
in a separate "seps" array.
Most important if you combine split with printf you can rebuild the record accuratelly = identical to Input Record $0

##SPLIT TEST:

$ echo "$b"
alert tcp any any -> any any (msg: "this is a "dummy" rule (to test) the rule"; flow:to server; sid:1233; rev:1; no case; content: "nothing";)

$ echo "$b" |awk '{fsep="[\";:]";split($0,f,"y|[\";:]",sep);for (k=1;k<=length(f);k++) printf("No=%s \t|Val=%s \t\t|Seperator=%s\n", k,f[k],sep[k])}'
No=1     |Val=alert tcp an         |Seperator=y
No=2     |Val= an                 |Seperator=y
No=3     |Val= -> an             |Seperator=y
No=4     |Val= an                 |Seperator=y
No=5     |Val= (msg                 |Seperator=:
No=6     |Val=                      |Seperator="
No=7     |Val=this is a          |Seperator="
No=8     |Val=dumm                 |Seperator=y
No=9     |Val=                     |Seperator="
No=10     |Val= rule (to test) the rule         |Seperator="
No=11     |Val=             |Seperator=;
No=12     |Val= flow         |Seperator=:
No=13     |Val=to server     |Seperator=;
No=14     |Val= sid         |Seperator=:
No=15     |Val=1233         |Seperator=;
No=16     |Val= rev         |Seperator=:
No=17     |Val=1             |Seperator=;
No=18     |Val= no case     |Seperator=;
No=19     |Val= content     |Seperator=:
No=20     |Val=              |Seperator="
No=21     |Val=nothing     |Seperator="
No=22     |Val=             |Seperator=;
No=23     |Val=)             |Seperator=



##MATCH FUNCTION
usually usage is match(string,substring).
awk '{print match($0,"-")}' <<<""abc-cde-fgh"       -> prints 4= the position of substr inside string. Even if the search term is "-c" will return 4. 

match can accept a third argument = array var. 
In this array all captured regex groups/expressions are stored.
awk '{print match($0,"-cd",a)}{for (i in a) print i,",",a[i]}' <<<"abc-cde-fgh"
4 #print of match
0Rstart , 4   #print of the array. 
0Rlength , 3  #print of the array
0 , -cd       #print of the array.Index 0,value "-cd"

##ADVANCED MATCH USAGE AS A REGEXP MAPPING TOOL
match can return different start,length and array indexes for more matches at once. 
Also can return in array elements each regex group with a separate index that can be used in whole awk program
awk 'match($0,/(.*)-...-(.*)/,a){for (i in a) print i,a[i]}' <<<"abc-cde-fgh"  #
0start 1        #Array printing of start position and length of each pattern
0length 11    #Printing is random, since for i in a prints in random order by default
1start 1
2start 9
2length 3
1length 3
0 abc-cde-fgh    #a[0] : Contains the whole matched substring. Equals to the original string in this case since the pattern matches the whole input string
1 abc            #a[1] : contains the first regex group. You can call a[1] from now on, and this will print abc
2 fgh            #a[2] : contains the second regex group


##ANOTHER MATCH EXAMPLE WITH PARTIAL REGEX MATCHING
awk '{print match($0,/-(...)-/,a)}{for (i in a) print i,a[i]}' <<<"abc-cde-fgh"     #a[0]="-cde-" and "a[1]=cde"


##REGEX NON GREEDY OPERATOR EQUIVALENT BY CHAZELAS:
http://unix.stackexchange.com/questions/49601/how-to-reduce-the-greed-of-a-regular-expression-in-awk
Perl (.*?)(;) = match everything up to first ;  <===> (.[^;]*)(;)  or (.[^;]*;) if you want ; to be included in the match

##ADVANCED MATCH
echo "$a"
Text1 somethingAA0123456something,elseAA9876543foo text1
$ awk '{match($0,/(\w+\s)(\w+)(\w\w[0-9]{7})(\w+,\w+)(\w\w[0-9]{7})(\w+\s)(\w+)/,a);print a[1],a[3],",",a[5],a[7]}' <<<"$a"
Text1 AA0123456,AA9876543 text1

-----------------------------------------------------------------------------------------------------------------------------
##SKIP COLUMNS/FIELDS Techniques
http://stackoverflow.com/questions/15361632/delete-a-column-with-awk-or-sed
http://stackoverflow.com/questions/43196719/drop-4-first-columns/43201869#43201869

You can apply some techniques capable to delete some fields, but the result is higly depends on the FS used.

The most easy solution using default FS is this
awk '{$1=$2=$3="";print}' <<<"Field1 Field2 Field3 Field4 Field5 Field6" 
   Field4 Field5 Field6   # gaps inserted in the beginning. Usefull method to "delete" middle columns.
PS: Actually you do not really delete those columns but you replace them with null data. 

This solution breaks with other than default FS:
awk -F"|" '{$1=$2=$3=""}1' <<<"Field1|Field2|Field4_1|Field4|Field5|Field6"              --->#   Field4 Field5 Field6 
awk -F"|" -v OFS="|" '{$1=$2=$3=""}1' <<<"Field1|Field2|Field4_1|Field4|Field5|Field6"   --->#|||Field4|Field5|Field6

Another Solution Could be:
echo "Field1 Field2 Field3 Field4 Field5 Field6"  |awk '{print substr($0, index($0,$4))}' -->#Field4 Field5 Field6
Here we don't have gaps in beginning, but this method fails if fields have similar text

echo "Field1|Field2|Field4_1|Field4|Field5|Field6"  |awk -F"|" '{print substr($0, index($0,$4))}' 
#Gets confused and prints $3 also ---> Field4_1|Field4|Field5|Field6


Another solution that works good with standard FS but not with other FS is this:
echo "Field1 Field2 Field3 Field4 Field5 Field6"  |awk '{gsub($1FS$2FS$3FS$4,$4);print}'  --->Field4 Field5 Field6 #no gaps
echo "Field1|Field2|Field3|Field4|Field5|Field6"  |awk -F"|" '{gsub($1FS$2FS$3FS$4,$4)}1' --->Field4|Field4|Field4|Field4|Field5|Field6
echo "Field1|Field2|Field3|Field4|Field5|Field6"  |awk -F"|" '{gsub($1FS$2FS$3FS$4,"")}1' --->||||Field5|Field6

For a fixed known number of columns , you can always do 'print $4,$5,$6'
For a known/fixed number of columns you can also delete completely a middle column with reconstructuring $0 like : awk '$0=$1FS$2FS$4'

##THE CORRECT WAY TO REMOVE COLUMNS/field as advised by Ed Morton
http://stackoverflow.com/questions/15361632/delete-a-column-with-awk-or-sed
echo "Field1|Field2|Field4_1|Field4    |Field5|Field6"  |awk -F"|" '{sub(/([^|]+\|){3}/,"")}1'  #Field4    |Field5|Field6

##REMOVING COLUMNS FROM THE END:
awk 'NF--'         #Removes last column
awk 'NF-=2'        #Removes last two columns
awk 'NF=$3'        #Removes all columns after column 3

## FORCE OUTPUT TO RESPECT OFS WHEN PRINTING OF $0
Normally OFS is not affecting the print of $0. if you just {print} the input line will be printed as it is, ignoring OFS.
You can workaround this either by awk -F"," 'gensub(FS,OFS,"G",$0)' OFS=: <<<"one,two,three"
or more cleverly: 
awk -F"," -v OFS=":" '{ $1 = $1; print }' <<<"one,two,three" #Prints one:two:three
The statement $1=$1 will reconstruct the $0 according to OFS. If you just print $0, will be printed with comma.

PS: printing fields i.e {print $1,$2) will be done by respecting OFS.

##CONDITIONAL EXPRESSIONS CLARIFICATION
Mind that the compact awk usage allows awk 'condition{action}' without if - then
Action can be ommited; in this case the default action (print $0) is performed. 
On the condition side we can have either boolean condition check (true /false) or arithmetical checks (i.e awk 'NR>10')

Moreover all vars have a value. You can either built an expression based on the actual value of the variable , or based on the fact if this variable has a value or not (boolean testing).
In Boolean testing, awk will evaluated as true whatever variable has a non-zero / non-null value, no matter if it is negative or positive.

See this difference:
.B (A) awk 'a[$0]++' file      #Prints all non unique lines of a file.
Here the status of the a[$0]++ is checked (before increasement). If a[$0]++ has any valid value will be printed
First line a[$0] is null=no printing and then increasing value from null/zero to 1 (++)
All the other times that same $0 is found will have a value different than zero thus will be printed.

.B (B) awk '(a[$0]++ == 1)' file #Prints all lines that found two times . Emulates uniq -d
Here the real value of a[$0]++ is checked before increasement and printing is done only when a[$0] is 1 
On the first line a[$0] will return null=false=0 
On the second line a[$0] will have a value of 1 = printing
On the third line a[$0] will have a valuer of 2 = non printing

.B (C) Code Golf Challenge: How to emulate uniq -D with as less code as possible.
uniq -d prints the lines that found to be repeated two or more times  
So even if line1 is repeated 10 times, uniq -d will just print line1 (one time) and this is what (B) does.

uniq -D print ALL repeating lines . If line1 repeats 10times, uniq -D will print line1 10 times.
This is close to awk (A) but (A) is not printing the first occurence of a future repeated line. 
awk (A) prints only the next occurences after first. So for a 10 times found line1 awk (A) will print Line1 9 times.

We could force an extra separated print a[$0] for all duplicated lines (i.e in END) but this may mess output format. 
It is more challenging to print duplicates along with their NR in which they found in file (print them as you found them)

Another alternative would be in END to print array elements a[$0] with a value >1. But for big files this should be inefficient.

##TRANSFORM HEADER ONLY LINE TO HEADER + field 1 +field2
http://stackoverflow.com/questions/42669125/how-to-convert-heading-of-file-content-into-column-using-awk/42672530#42672530

My way: awk 'BEGIN{FS=OFS="|"}NF==1{h=$0;next}$0=h OFS $0' file1 #Fallpit: No blank line is printed between headers change

Smart way: awk 'BEGIN{FS=OFS="|"} NF==1{h=$0} {print (NF==1?"": h OFS $0)}' file1 #Includes blank line on every header change

Another Smart Way : awk -F '|' '{if(NF==2)$0=F"|"$0;else{F=$1;$0=""}}NR>1' YourFile


##FIELDS SPLITTING WITH GENSUB
gensub is great because the manipulated string returns in gensub and not in the text/field under manipulation as gsub/sub does

lynx -dump "http://www.meteorologos.gr/" |awk '/Αθήνα/{a=1;next}a==1{print gensub(/(...)(..)(.*)/,"\\2 βαθμοί",1,$0);exit}' |espeak -vel+f3 -s10 -k3
splits the $0 in three parts: 3 chars - 2 chars - rest chars.                          ^^Regex      ^^Replace  ^identifier (1=first intance, 4=forth instance, "g"=global

## PRINTF
The great about printf("%s",var) is that does not print new lines along ALL the input lines, and thus you can concatenate fields of ALL lines with just one command.
If you apply "%s\n" will emulate the behavior of classic print in awk = will print the desired field on each separate line.
in a script like '{printf("%s",$0) all lines of input file will become one single line = new lines ignored.

Compare:
echo -e "a b c\nd e f\n" |awk '{print $0}' --> prints a b c in one line and d e f in the next line - AS EXPECTED
echo -e "a b c\nd e f\n" |awk '{printf("%s", $0)}' --> a b cd e f  - all in one line - new lines / ORS ignored
echo -e "a b c\nd e f\n" |awk '{printf("%s\n", $0)}' --> same as print $0
echo -e "a b c\nd e f\n" |awk '{printf("%s%s", $0,RT)}' --> same as print $0 since RT is new line = RS

echo -e "a,b,c\nd,e,f\n" |awk -v RS="," '{printf("%s%s", $1,$3)}' ---> Will print in ONE line abcef = The first field of all the lines.

## PRINTF - CUSTOM RS AND RT
Combining printf with multi RS and RT you can easily reconstructure the original input record no matter in how many pieces has been split by awk and multi RS.
Actually RT is the dynamic RS used each time to split the records.

Tests:

echo -e "a,b,c\nd,e,f\n" |awk -v RS="," '{print NR,$1,$2}'
1 a 
2 b 
3 c d   #Mind that d is $2 because between c and d there is not comma but newline
4 e     #This is why usually when you want to split completelly all fields to records, in RS you include also the new line i.e RS=",|\n"
5 f     

echo -e "a,b,c\nd,e,f\n" |awk -v RS=",|\n" '{print NR,$1,$2}'
1 a 
2 b 
3 c 
4 d 
5 e 
6 f 
7  

And now this is how Printf and RT can come in play to make the record back to normal. RT will change values dynamically.
For line1 (a) , RT will be comma. But for line 4 (d) RT will be new line.

echo -e "a,b,c\nd,e,f\n" |awk -v RS=",|\n" '{print $1,RT}'
a ,   # Line 1 - RT comma 
b ,   # Line 2 - RT comma
c     # Line 3 - RT new line
        # Just an empty new line that follows field c - before d
d ,   # Line 4 - RT comma
e ,   # Line 5 - RT comma
f     # Line 6 - RT new line

But above printing does not even close to the original record. What about this one:

# echo -e "a,b,c\nd,e,f\n" |awk -v RS=",|\n" '{printf("%s%s", $1,RT)}'
a,b,c
d,e,f

Yes this is identical to input record using printf (ignoring new lines) and printing RT dynamic value.
The new line is produced because RT will be \n when reading field "d".

With this method, you can break down all the fields to records, you can manipulate/change them , and then print them back to the original format
using the printf (no - new lines) instead of print and making use of RT dynamically changing value along the corresponding records.
You just need to include \n in the customized RS for this shit to work.
PS: If you try to set ORS=RS="multi Record separators here" it does not work. Print will just print the whole regex as ORS instead of comma or new line or whatever.


Tip:
Especially in Fields Splitting, you can apply a kind of ternary if at printf : for (k=1;k<=NF,k++) printf("%s%s",$k,(k==NF)?ORS:OFS)

Remark:
It seems that RS accepts OR in regex in format RS=",|\n" but ORS accepts regex but not OR;thus ORS=RS=regex fails if OR is given

----------------------------------------------------------------------------------------------------------------
AWK:PRINT WITH IF CONDITION
##PRINT LINES IF SPECIFIC COLUMNS HAVE SAME VALUE
awk -F ':' '$3==$4' file.txt -->  Prints only when $3 equals to $4. print can be ommited since it is the default action.
echo "Geo 123 bg ty 123" |awk -F" " '$2==$5' -> Geo 123 bg ty 123  # Print lines in which field 2 = field 5, otherwise returns nothing.
echo "Geo 123 bg ty 123 Geo" |awk -F" " '$1==$6' --> Geo 123 bg ty 123 Geo # Print if field1=filed6 , meaning Geo=Geo. Works even with strings!!!

----------------------------------------------------------------------------------------------------------------
AWK:EXTERNAL VARIABLES
##IMPORT VARIABLES
You can import a shell variable by awk -v awkvar=$shellvar (i.e awk -v dt=$date).
Inside awk you need to refer to awkvraible without $.
Each external var requires it's own -v declaration.

##EXPORT AWK VARIABLES
$ mkfifo fifo
$ echo MYSCRIPT_RESULT=1 | awk '{ print > "fifo" }' &
$ IFS== read var value < fifo
$ eval export $var=$value

##EXPORT ALTERNATIVES
If you need to catch let's say one var you can also do it like shellvar=$(awk...). Whatever awk prints will be stored in $() pipe buffer.
For more vars, you could also use a temp file , printing inside va="value"\n and then you could source the file in shell scripts.

----------------------------------------------------------------------------------------------------------------
AWK:REPLACE Switch position of comma separated fields:
echo "textA,textB,textC,dateD" |awk -F, '{A=$3; $3=$2; $2=A; print}' OFS=,
textA,textC,textB,dateD
OFS affects only the output display separator. If omited space (default OFS) will be used.

print all the lines between word1 and word2 : awk '/Tatty Error/,/suck/' a.txt

Print up to EOF after a matched string: awk '/matched string/,0' a.txt

----------------------------------------------------------------------------------------------------------------
AWK:PRINT WITH MULTIPLE DELIMITERS

##BASIC TEST
Input: --- 22:16050075:A:G 16050075 A G
$ awk -F"[ :]" 'BEGIN{OFS="\t";print "$1","$2","$3","$4","$5","$6","$7","$8"}{print $1,$2,$3,$4,$5,$6,$7,$8}' file
$1      $2      $3              $4      $5      $6              $7       $8
---     22      16050075        A       G       16050075        A        G

#mind the diffrenece with -F"[:]"
$1        $2         $3  $4     
--- 22    16050075   A   G 16050075 A G 

##EXAMPLES
$ awk -F"name=|ear=|xml=|/>" '{print $2} {print $4}' a.txt >b.txt
Input: <app name="UAT/ECC/Global/MES/1206/MRP-S23"   ear="UAT/ECC/Global/MES/1206/MRP-S23.ear" xml="UAT/ECC/Glal/ME/120/MRP-  S23.xml"/>
Output: 
UAT/ECC/Global/MES/1206/MRP-S23   
UAT/ECC/Glal/ME/120/MRP-  S23.xml
Test: awk -F"name=|ear=|xml=|/>" '{print "Field1="$1} {print "Field2="$2} {print "Field3="$3} {print "Field4="$4}' a.txt
Mind that separate {} create a newline to out file.

##ANOTHER EXAMPLE OF USING AS FIELD SEPERATOR (F) anything (a char, a word, two delimiters, etc).
Compared to cut : with cut you allowed to use only one delimiter (-d), or to define a chars range using -c (i.e -c1-10 : seperate file in character 1-10 , whatever this char is).
echo "This is something new for us" |cut -c1-12 --> This is some # You can not combine -c with -f or with another -c, but you can print a range -c1-10, or particular chars using -c1,10,12

echo "value1,string1;string2;string3;string4" |awk -F"[;,]" '{print $2}' -->string1
echo "value1,string1;string2;string3;string4" |awk -F"[;,]" 'NR==1{for(i=2;i<=NF;i++)print $1","$i}'
-->value1,string1
-->value1,string2
-->value1,string3
-->value1,string4

In case of file , separated with new lines you need to apply this a bit different version: 
awk -F"[;,]" 'NR==1{print;next}{for(i=2;i<=NF;i++)print $1","$i}' file

----------------------------------------------------------------------------------------------------------------
AWK:SEARCH / GREP Search for a pattern with not known occurencies:
awk '{{for(i=1;i<=NF;i++)if($i == "name:") printf $(i+1)" "$(i+2)" "} print ""; }' yourfile

This is usefull if we dont know how many "name:" entries exist per line
If we know that each line has i.e 3 entries then this also works: awk -F"name:" '{print $2 $3 $4}'
If a line has less than 3 no problem. Var $3 and/or $4 will be empty. 
If line has more than 3 the -F solution will miss the rest entries.

Also check this out: awk '{for(i=3;i<=NF;++i)print $i}'
awk '{for(i=5;i<=NF;i++) {printf $i " "} ; printf "\n"}' awkTest2.txt

----------------------------------------------------------------------------------------------------------------
AWK:PRINT Produce a sed script to replace values to a file with entries from another file
http://unix.stackexchange.com/questions/340246/how-to-replace-a-string-in-file-a-by-searching-string-map-in-file-b#340247

Consider a user map containing multiple lines with "userid username" (seperated by space)
Consider a text file (letter.txt) contaiining paragraphs with reference to the users as userid.
We want to replace all userids in letter file with their realnames present in name mapping file.
Tricky solution: Transform map file (each line) to the format 's/userid/username/g' and then call sed -f <transformed mapfile> <text file that needs replacements>
The awk part: $ awk '{ printf("s/<@%s>/%s/g\n", $1, $2) }' user_map.txt >script.sed
The sed part: $ sed -f script.sed letter.txt 

*BASH Way: var="$(cat file.txt)";while read -r id name;do var="${var//@$id/$name}";done<mapfile.txt;echo "$var"

*SED Way : while read -r id name;do sed -i "s/\@$id/$name/g" textfile.txt;done<mapfile.txt

*SED Bug : File is opened and "seded" multiple times (but either the Kusulananda solution does sed multiple times, correct? - No. Does one sed with multiple replace patterns)
On the other hand, bash way opens the file once and , makes replacements in memory ($var) and when finished just echo the $var.
Bash solution doesnot require any external tools; it is just bash parameter expansion feature.

----------------------------------------------------------------------------------------------------------------
AWK:REPLACE Insert dash in string
String: #  1  2016-05-31-1003-57S._BKSR_003_CM6
awk '{print substr($3,0,13)"-"substr($3,14,2)}' file.txt

Output : 2016-05-31-10-03

Alternatives:
$ cut --output-delimiter='-' -c7-19,20-21 file.txt
$ while IFS= read -r line;do line="${line:6:13}-${line:14:2}";echo $line;done<file.txt
$ sed 's/..$/-\0/g' <(cut -d- -f1-4 <(cut -d" " -f5- file.txt)) #use >newfile at the end to send the results to a new file
Mind the \0 usage of sed (refers to data on hold , kind of history/previous value)

----------------------------------------------------------------------------------------------------------------
AWK:REPLACE  Replace values upon criteria matching:
Input:
  a b c
A 5 2 0
B 0 5 4
C 4 3 4
D 2 0 2

Output expected (replace all fields with a value to 1)
  a b c
A 1 1 0
B 0 1 1
C 1 1 1
D 1 0 1

AWK script : 
BEGIN { OFS = FS = "\t" }NR != 1 {for (i = 2; i <= NF; ++i) {if ($i != "0") {$i = "1";}}}{ print }

Runit : awk -f script.awk datafile

----------------------------------------------------------------------------------------------------------------
AWK:PRINT IF CONDITION - Column Checker
Input:
A B B A
A A A
B A B A
B B
A A
A B

Output : 
If all columns are the same then print just once the common characted (i.e if AAA then print A)
If columns are different then print "multi"

Code:
awk '{ for (i = 2; i <= NF; i++) { if ($i != $1) { print "multi"; next } }; print $1 }'

----------------------------------------------------------------------------------------------------------------
AWK:PRINT Print line with matched pattern (simulates grep):
awk '/word/' file.txt
GNU awk also accepts variables in the form of awk "/$variable/" file.txt
Using '!/pattern/' you invert the match (similar to grep -v)
----------------------------------------------------------------------------------------------------------------
AWK:PRINT Print a line if columns are equal to 5 (ps: columns separated by "|")
awk -F \| 'NF==5' data3

----------------------------------------------------------------------------------------------------------------
AWK:COUNT LINES
awk '{ $0 = NR "\t" $0 } 1' file.txt | tail #Here obviously we assign to $0 the value NR=Number of Record + tab + $0. The 1 at end prints
GV: awk '{ print NR "\t" $0 }' file1 #This also applies numbering 

Counting Alternatives:
grep -n ^ file.txt | sed 's/:/\t/' | tail
sed = file.txt | sed 'N;s/\n/\t/' | tail
pr -n -t -l 1 file.txt | tail
nl /boot/config-4.9.0-1-amd64 |tail

#------------#-------------##------------#-------------##------------#-------------##------------#-------------#
AWK:PRINT / GREP WITH MULTIPLE PATTERNS - Simulate agrep (grep with AND) for patterns read from a file:
http://unix.stackexchange.com/questions/341076/how-to-find-all-files-containing-various-strings-from-a-long-list-of-string-comb
Basic Command: awk '/pattern1/ && /pattern2/' file   #Performs the default action print $0

while IFS= read -r line;do awk "$line" *.txt;done<./tmp/d1.txt
Above will print in screen the matched lines of all txt files.
If we need to print only the filename and not the matched contents then awk must be used like this:
awk "$line""{print FILENAME}" *.txt

In order this to work , search terms in file d1.txt should be in format /term1/ && /term2/ && /term3/ etc.
PS: Single quotes found not necessary in GNU AWK

In the example given in link , pattern file contains lines like this:
"surveillance data" "surveillance technology" "cctv camera"

Which are converted to awk format like this:
$ sed 's/" "/\/ \&\& \//g; s/^"\|"$/\//g' ./tmp/d1.txt
> /surveillance data/ && /surveillance technology/ && /cctv camera/ #mind the absence of single quotes

Alternativelly we could use :

(a) agrep by formatting the data in the form of 
pattern1;pattern2;pattern3 
and then using: while IFS= read -r line;do agrep "$line" *.txt;done<./tmp/d1.txt

(b) classic grep with xargs -r : grep -l pattern1 * |xargs -r grep -l pattern2 |xargs -r grep -l pattern3
The -l instructs grep to print filename . Usefull info for the next grep.
If we just want to multi grep with and on the data then even this should work:
grep pattern1 file |grep pattern2 |grep pattern3
The final output would include all three patterns. 

#------------#-------------##------------#-------------##------------#-------------##------------#-------------#
AWK:GREP AND MULTIPLE PATTERNS CASE B  
awk '/Label 3/ { print $3 } /Label 5/ { print $3 }'  <==>  grep -E 'Label 3 \|Label 5' |cut -d' ' -f3
Mind the print function between patterns.
AWK can directly print to a file using { print $3 >>"F3.csv" }

Combine multi grep pattern and cut -d' ' -f3 for multiple lines
awk '/Label 3/ { print $3 >>"F3.csv" } /Label 5/ { print $3>>"F5.csv" }'
OR
awk '/Label 3|Label 5/ { print $3 >> "F"$2".csv"}' #Field 2 is used as name for the output file.

Consider a file with lines 
Label 3 70
Label 4 95
Label 5 100

#------------#-------------##------------#-------------##------------#-------------##------------#-------------#

AWK:GREP AND MULTIPLE PATTERNS CASE C  
Consider small patterns file with two patterns per line.
Consider big data file with lines in where the patterns can be in different order
awk '/pattern1/ && /pattern2/'  works fine, either directly or by applying awk -f <(sed ..... patterns) datafile

What about a full awk implementation to handle the two patterns in an AND way?
##1. awk 'BEGIN { getline < "patterns"; split($0, patterns) } { for (i in patterns) if ($0 !~ patterns[i]) next; print }' file1 
#Above works only for 1st line of patterns (by geirha)

##2. awk 'NR==FNR{patts[$1]=$2;next}{for (i in patts) if (($0 ~ i) && ($0 ~ patts[i])) print}' patterns file1  
Restricted to work for only two patterns per line but works ok (regex match - no word match)

##3.awk 'NR==FNR{data[$0]=$0;next}{for (i in data) if ((data[i] ~ $1) && (data[i] ~ $2)) print data[i]}' file1 patterns  
#works even ok with pitfall that big data file is loaded in memory instead of patterns file.

##4.awk 'NR==FNR{patts[$0]="\\<" $1 "\\>.*\\<" $2 "\\>|\\<" $2 "\\>.*\\<" $1 "\\>";next} \
    {for (i in patts) {if ($0 ~ patts[i]) print}}' patterns file1   
#Just one more alternative using regex AND and word matching. AND can be emulated with pattern1.*pattern2 | pattern2.*pattern1
Patterns are transformed from "833 7777" to "\<833\>.*\<7777\> | \<7777\>.*\<833\>"

##5. awk 'NR==FNR{patts[$0]="\\<" $1 "\\>.*\\<" $2 "\\>|\\<" $2 "\\>.*\\<" $1 "\\>";next} \
{for (i in patts) {if ($0 ~ patts[i]) !found[i]?found[i]=$0:found[i]=found[i] ORS $0}} \
END{for (k in found) {print found[k];print "-----"}}' patterns file1   
#The final solution : word matching of pattterns with logical AND operation and printing in the end with matching groups and a separator string. Test it online : http://www.tutorialspoint.com/execute_bash_online.php?PID=0Bw_CjBb95KQMd2JiTTJndl9TLU0

##6. Extending to work with a 2d array in gawk , by geirha
printf 'p1 p2\np3\n' > patterns
awk 'NR==FNR{for(i=1;i<=NF;++i)patts[FNR][$i];n=FNR;next}END{for(j=1;j<=n;++j){for(p in patts[j]) print j, p}}' patterns
array checks can be made like : <geirha> for (i = 1; i <= NF; ++i) if ($i in patts[j]) ...

Complete code using 2D arrays with two or more patterns check on the same record in any order:
http://www.tutorialspoint.com/execute_bash_online.php?PID=0Bw_CjBb95KQMaTBHTlY3LXRaTFE

awk 'NR==FNR{for (i=1;i<=NF;i++) patts[FNR][$i]=1;f1=FNR;next}    #all patterns are held in a 2D array in format patts[1][patt1] patts[1][patt2], etc
#Reading of the data file
 {for (k=1;k<=f1;k++) #iterating through patts array [FNR] indices
  {  
    {gotit=0;for (r=1;r<=NF;r++) if ($r in patts[k]) gotit++}    #Iterating through all fields of curret data file record
    {if (gotit==2) found[k]?found[k]=found[k] ORS $0:found[k]=$0}    #If two indices of patts[k] match fields of $0. Can be changed to >2, >3, etc  
  }
}
END{for (l in found) {print found[l];print "-----"}}' patterns file1   


#------------#-------------##------------#-------------##------------#-------------##------------#-------------#
AWK:PRINT print a line before the pattern match and also the line containing the pattern 'Solaris': 
$ awk '/Solaris/{print x;print;next}{x=$0;}' file

Every line read is stored in the variable x. Hence, when the pattern matches, x contains the previous line. 
And hence, by printing the $0 and x, the current line $0 and the previous line x is printed. 

#------------#-------------##------------#-------------##------------#-------------##------------#-------------#
AWK:PRINT previous line, the pattern matching line and next line
$ awk '/Solaris/{print x;print;getline;print;next}{x=$0;}' file
x: is a var, containing the previous line , becasure of the assignment in the end x=$0
print: just prints the current line (stored at $0)
getline : stores next line to $0.
The last print prints again $0 which now is the next line due to previous getline  

#------------#-------------##------------#-------------##------------#-------------##------------#-------------#
AWK:REPLACE - DELETE STRING
awk '{ gsub(",","",$3); print $3 }' /tmp/data.txt
Replaces , with null in field $3 and then prints $3
If $3 is not provided , then gsub operation is performed in $0=Whole Record = Whole line
Alternative :$3=""

#------------#-------------##------------#-------------##------------#-------------##------------#-------------#
AWK:PRINT IF-THEN
https://www.yumpu.com/en/document/view/25827537/sed-and-awk-101-hacks#
awk -F'"' '{if ($0!="") print $1"\""$2" || "$2"\""$3;}' e.txt
awk '{if ($3 =="" || $4 == "" || $5 == "")print "Some score for the student",$1,"is missing";'}' student-marks
awk '$4<900 || $5<=5' file.txt #Prints the records in which $4 is less than 900 and $5 is -le 5
#Can combined with {print $2} to print a particular field instead of the whole line.
awk '$3==$4' file #prints records/lines only if $3 field = $5 field
awk '$2=="Tennis" #prints lines where #2 is tennis
awk '$2 ~ "Tennis" #prints lines where #2 contains tennis
#------------#-------------##------------#-------------##------------#-------------##------------#-------------#
AWK:IF-THEN-ELSE
http://www.thegeekstuff.com/2010/02/awk-conditional-statements
1. awk '{if ($3 >=35 && $4 >= 35 && $5 >= 35) print $0,"=>","Pass";else print $0,"=>","Fail";}'

#------------#-------------##------------#-------------##------------#-------------##------------#-------------#
AWK:IF-THEN-ELSE AND REPLACE

awk -F'"' '{if ($0=="") print $0;else {a=$2;gsub("lbk_addcolumn","ColumnAdd",$2);gsub("lbk_dropcolumn","ColumnDrop",$2);print $1"\""$2" || "a"\""$3;}}' e.txt

# Alternative - more easy to read:
# $ a=$(awk -F'"' '{if (($0=="")) print $0;else print $1"\""$2" || "$2"\""$3}' file.txt)
# $ sed 's/lbk_addcolumn/ColumnAdd/;s/lbk_dropcolumn/ColumnDrop/' <<<"$a
# Or Even with process substitution: 
# $sed 's/lbk_addcolumn/ColumnAdd/;s/lbk_dropcolumn/ColumnDrop/' <(awk -F'"' '{if (($0=="")) print $0;else print $1"\""$2" || "$2"\""$3}' file.txt)

This prints blank lines straight away (unprocessed) and performs all operation if lines are not blank.
You need to apply {} in if-then-else if you need more than one commands to be executed as a group. 
If you dont enclose commands in{} only the first command (up to first ;) is executed and then if condition check stops. 
Rest commands will be executed as a continuation of the code after if-then-else

For Input like this :     
x;x;x;x;x;x;cmd="lbk_addcolumn TABLE_NAME_1 COLUMN_X";x;x;x;x

x;x;x;x;x;x;cmd="lbk_dropcolumn TABLE_NAME_2 COLUMN_Y";x;x;x;x

Gives Output like this:    
x;x;x;x;x;x;cmd="ColumnAdd TABLE_NAME_1 COLUMN_X || lbk_addcolumn TABLE_NAME_1 COLUMN_X";x;x;x;x

x;x;x;x;x;x;cmd="ColumnDrop TABLE_NAME_2 COLUMN_Y || lbk_dropcolumn TABLE_NAME_2 COLUMN_Y";x;x;x;x
(Ps: mind that blanc line is preserved - not processed. )

#-----------------------------------------------------------------------------------------------------------------------
AWK:PRINT last and first line
awk 'NR==1 {first = $0} END {print; print first}' file
Just save the first line to a variable (first in this example). print in the END, by default will print the last $0=last line

#-----------------------------------------------------------------------------------------------------------------------
AWK:SEARCH LIKE GREP WITH GAWK AND VARIABLE
$ i="Mytestserver02"
$ awk /"$i"/'{print $2}' d3.txt # returns the second field , space separated
19
Or even awk /"$i"/ file will bring the matched line (all fields)
Although, experts claim that allowing expansion of a bash var in awk is a bad technique.

This source gives some alternatives: http://cfajohnson.com/shell/cus-faq-2.html#Q24
#-----------------------------------------------------------------------------------------------------------------------
AWK:PRINT lines with unique fields (uniq emulation without sorting)
awk -vFS=";" '!unique[$2]++' c.txt
Will print all lines that have a unique field $2 (; separated)
#-----------------------------------------------------------------------------------------------------------------------
AWK:REPLACEMENT Replace fields in a file using replacement keys in a separate file
http://stackoverflow.com/questions/42084340/compare-a-txt-and-csv-file-and-need-to-replace-with-matching-name-in-csv-file/42087905?noredirect=1#comment71347363_42087905
File 1(txt): [fields:WinSpc:defect]
File 2 (csv - holding keys): WinSpc,projects.winspc  # Field , replacement data
Target : [fields:winspc:defect] (replace uses only part of the new value)
awk solution:
awk 'FNR==NR{split($2,list,"."); replacement[$1]=list[2]; next} \
   {for (i in replacement){ if (match($0,i)) {gsub(i,replacement[i],$0); break} }}1 ' \
      FS="," file2.csv file1.txt
      
My Solution with bash:
$ readarray -t a < <(grep -e "\[fields:" a.txt |cut -d: -f2)
$ for ((i=0;i<${#a[@]};i++));do a[i]=s/${a[i]}/$(grep -e "${a[i]}" b.txt |cut -d, -f2 |cut -d. -f2)/g\;;done
$ sed -f <(echo "${a[@]}") a.txt

#-----------------------------------------------------------------------------------------------------------------------
AWK:ARRAY CREATION:
https://www.gnu.org/software/gawk/manual/html_node/Reference-to-Elements.html#Reference-to-Elements

## THESE NEXT 2 entries are not one-line scripts, but the technique is so handy that it merits inclusion here.

## CREATE AN ARRAY NAMED "month", indexed by numbers, so that month[1] is 'Jan', month[2] is 'Feb', month[3] is 'Mar' and so on.
split("Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec", month, " ")

## CREATE AN ARRAY NAMED "mdigit", indexed by strings, so that mdigit["Jan"] is 1, mdigit["Feb"] is 2, etc. Requires "month" array
for (i=1; i<=12; i++) mdigit[month[i]] = i

##CHECK IF A VAR BELONGS IN ARRAY WITH IN OPERATOR
$ awk -F'|' 'NR==FNR{check[$0];next} $2 in check' file2 file1
Mind that IN operator actually looks in array indexes and not values.

##ARRAY TESTING
awk 'BEGIN{a["one"]=1;a["two"]=2;a["two"]++;a["three"]="tria";!a["three"]++;a["four"]++;!a["three"]++;a["three"]++; \
for (i in a) print i,a[i]; if (2 in a) {print "2 found"} else {print "2 not found"}; \
if ("two" in a) print "two found";print a["one"],a["two"],a["three"],a["four"];print !a["one"],!a["two"],!a["three"],!a["four"]}'

three 3
two 3
four 1
one 1
2 not found
two found
1 3 3 1
0 0 0 0

Mind that the declaration a["three"]="tria" has been overwritten by a["three"]++ which assigns a new numeric value

##ARRAY CHALENGE : Identify how 'a[$0]++ or '!a[$0]++' works 
'a[$0]++' prints only duplicate lines (up to two-not more) anywhere in file (no sorting required)
'!a[$0]++' prints only unique lines, anywhere in file (no sorting required)
There is a kind of explanation here: https://unix.stackexchange.com/questions/159695/how-does-awk-a0-work/159697

It is actually a condition check together with array creation, both combined in one command.
But the condition check is not identical to '$0 in a' - if it was only 'a[$0]' without ++ should work - but does not.

.B Let's examine the easy case : a[$0]++
This prints duplicate lines - Lines that are found exactly two times. If a line is repeated for more than 2 times will be printed.

Basics:
'{print $0}' is the default action and will be executed if omitted. This is why awk '1' file will print all lines
Simple test conditions (i.e 'NR==FNR{do this}') do not require the full "if (expr) {}" syntax.
a[$0] is a direct array creation of array a with index $0 = line1 , line2, etc from file - value is nothing.
a[$0]++ does array element creation also but also applies a numeric value (1 for the first time) to the just created array element
The second time that awk will read $0 which was found before, the reference a[$0]++ will add +1 to the existed a[$0] , and it's value will change from 1 to 2.

On the same time, the expression a[$0]++ is evaluated as a condition. Actually the value of a[$0] before the ++ is checked. 
If the value of a[$0] is 1 (meaning that one line has been found two times), 1 means true and awk will perform it's default action (print $0)
But if the value of a[$0] is 0 (line found only once) then 0 means false = no printing.

Actually awk evaluates as true any value different than zero or null. This can be checked by applying a[$0]-- which achieve the same results.

What is important is that condition check takes place before increment (post increment method).
If it was ++a[$0] = pre increment, the condition check would happen after ++ , a[$0] would have value 1 even the first time found.


Mind that if a line appears three times , then a[$0] will return two . Two is also evaluated as true and thus even this third line will be printed.

Actually whatever is not 0 or null OR whatever is set will be handled as true by awk and will be printed, and this applies to strings or even to negative numbers.

.B Examining the '!a[$0]++' case - print unique lines without sorting

Using negation (!) in front of a[$0]++ you don't affect the array creation but you affect condition check result and thus the print $0.

The negation will revert the result of values stored in each a[$0] element. 
In the very first reading, a[$0] is null=unset and returns 0=false.  
With negation ! this will be reverted to 1 and will be printed.Then (pody increment) the ++ operator will add 1 to this element

On the second time that a[$0] will be found , condition checking will see a value of 1 (set from the first time).
Any value different than 0 (1,2,3,-1,-2,-3,whatever) will return true and by ! will be reverted to zero thus will not be printed.
As a result , only true unique lines of the file will be printed, excluding all repeating lines (no matter how many times they do repeat)

The success of this method is based on the following facts:
check of a[$0] value happens before assignment due to post increment a[$0]++ 
AWK will print an array element if has any valid value = not zero , not null , not unset
Negation mark will revert above behavior. All nulls/zeros will become 1 (and printed), and all non zero numbers will become 0 (not printed)

.B Important: 
Using just awk 'a[$0]' file4 is not printing duplicate lines , is not printing unique lines or all lines. Prints nothing.
This happens because without ++ the element a[$0] has a null value (and not 0 nor 1,2,etc) and nulls are evaluated as false thus no printing
On the other hand, using awk '!a[$0]' file4 prints all the lines - no filtering since the initiall null value is reverted to 1 by ! =all printed.
Due to the absence of ++, the value of a[$0] remains null=false

Mind these tests:
# awk '{print a[$0]}' file4     ---> #Prints nothing - null for each line (a block of null lines is printed)

# awk 'a[$0]' file4         ---> Prints nothing. a[$0] will have null value at the first time and this new value will be preserved since there is no assignment like ++.

# awk 'a[$0];{if ($0 in a) print}' file4 or 'a[$0];$0 in a'    ---> Prints all the lines of file4. 
This is because the if check is after the array assignment and thus always $0 will be found inside array a

# awk '{if ($0 in a) print};a[$0]' file4 or '$0 in a;a[$0]'    ---> Prints correctly repeated/duplicate lines = similar to 'a[$0]++'  
Works because condition check ($0 in a)is done before array assignment.

# awk '!($0 in a);a[$0]' file4 ---> Prints correctly only unique lines for the same reason (check before assignment)
Mind that '!$0 in a;a[$0]' does NOT work- requires parenthesis


# awk 'BEGIN{print !1,!2,!3,!4}' ---> 0 0 0 0                    # Direct negation of any number gives 0
# awk 'BEGIN{print a,george}'    --->                             # Prints nothing - null. a and george are handled as newly declared variables with out a value
# awk 'BEGIN{print !a,!george}'  ---> 1 1                        # The null value of vars a and george is reverted and prints 1 1

# awk '{print a[$0]++ ,$0 ,a[$0]}' file4
0 line1 1                        # 1st Col prints the value of a[$0]=a["line1"]=0 (not assigned yet). 
0 line2 1                        # At the end - 3rd column - it prints the value of a[$0] after has been assigned by ++ = 1
0 line3 1                        # If the first argument was ++a[$0] then 1st and 3rd column would have been identical (tested)
0 line4 1                        # Obviously the trick to work is based on the value of 1st column=check value before initial assignment
0 line5 1                        #  
1 line2 2                        # When we reach this line , since has been found before, we find a value of 1 before the new ++
1 line3 2                        # Same for this line
2 line2 3                        # In this line we found already a value of 2 for a["line2"] since has been increased ++ two times so far

# awk 'a[$0]++' file4
line2                            # a[$0] = 1 = print 
line3                            # a[$0] = 1 = print 
line2                            # a[$0] = 2 = print
Remark: Due to this behavior the use of '$0 in a;a[$0]' is better  to exclude repeated lines since the test is based in condition if $0 belongs to a and not in the value of a[$0]

# awk '{print a[$0], !a[$0]++ ,$0 ,a[$0]}' file4
  1 line1 1                        # 1st:a[$0]=null , 2nd: !a[$0]++ = null reverted to 1 and THEN add one , 4th =1 final value of a["line1"] after ++
  1 line2 1                        # All lines found with null value at 1st col are reverted by ! to 1 (2nd col) and thus get printed
  1 line3 1
  1 line4 1
  1 line5 1
1 0 line2 2                        # here the second column reports zero because 1st col found with a value from previous assignments by ++ (col 1 = 1)
1 0 line3 2                        # if it was never assigned a value (like awk '!a[$0]') then here would be also 1 @ 2nd col.
2 0 line2 3                        # Since the value of these three last lines is reverted to zero by ! at time accessed by awk they don't get printed

# awk '!a[$0]' file4    ---> all lines printed         # Prints ALL the lines - no filtering, because at time of check a[$0] has null=false reverted to 1=true by ! and thus printed
# awk '{print !a[$0]}' file4    ---> all 1        # Print 1 for every line - unique and repeated lines. 
                            # No value assignment is ever made , thus even the second time that we found line2 , 
                            # has value null which is reverted to 1 and this is why we see all 1 and all lines are printed in previous example


As a result awk '!a[$0]' file4 is based on the return value of 2nd col above and prints lines 1-2-3-4-5 . 
last three lines (2-3-2) had already a value when awk reached them (had been found before) the negation turns this value to 0 = no print

You can instead of a[$0] assign a different field like a[$2] if you want to print duplicate lines in terms of dupplicates in field 2.

To understand how awk boolean evaluation works in above codes, you can also apply !a[$0]-- that will have the same behavior.
Only first time a[$0] will be zero = > reverted => true = print. All the next times will be -1,-2 but since it is not zero is evaluated as true / valid by awk = no print due to negation mark


##MORE TRICKY USAGES
.B awk '!a[$0]++' <==> '(a[$0]++ == 0)'  # Bothe emulate uniq
Prints also unique lines. In this alt syntax, it seems that parenthesis is evaluated arithmetically.
For every line that a[$0]++ is evaluated as null=zero then print it because the whole parenthesis will return true.

.B awk '{seen[$0]++}END{for (i in seen) print seen[i],i}' file4  <===> uniq -c (all lines with No of occurences found)

.B awk '(a[$0]++ == 1)'  #Direct equivalent to uniq -d
Uniq -d prints only repeating lines = lines found more than one time (either 2 times or more).
Above AWK with arithmetic Comparison can achieve this without sorting requirement. 
awk will print "line1" ONE Time as soon as this line has been found again (and thus a[$0]==1) . 
First time a[$0]++ will be evaluated as null=0 and will not print. 
Second time a[$0]++ will be evaluated as 1 and will print. (then will be increased by 1 more - post increment).
Third and more times, a[$0]++ will have a value greater than 1 and will not print.

This is different to awk 'a[$0]++' since in this awk, the a[$0] is evaluated with true or false and not arithmetically.
With 'a[$0]++' we have a print for every line that is not null/0 . So if line1 exists three times will be printed two times.

.B uniq -u emulation - Prints line found only once time = true unique lines
$ { seq 1 3 && seq 2 4;} |sort 
1
2
2
3
3
4

$ { seq 1 3 && seq 2 4;} |sort |uniq -u
1
4

$ { seq 1 3 && seq 2 4;} |awk '{seen[$0]++}END{for (i in seen) printf (seen[i]==1)?i ORS:""}' #sort is not required
1
4


.B awk 'a[$0]++' <==> awk '(a[$0]++ > 0)'
In alt syntax with arithmetic evaluation the whole parenthesis will be true if a[$0} is greater than zero before increment. 
Zero will be only the first time found thus will return arithmetically false.

.B awk '(a[$0]++ > 1)' 
This will print all lines that they do repeat more than two times. First occurence a[$0] would return zero, second would be found 1, more than two will be found greater than 1 = parenthesis true=print

#-----------------------------------------------------------------------------------------------------------------------
##ARRAYS EXAMPLE - GREP SIMULATION
https://stackoverflow.com/questions/42239179/fastest-way-to-find-lines-of-a-text-file-from-another-larger-text-file-in-bash
(grep -F -f file1 file2 but restricted to field2 of file2)

##INIAN METHOD WITH CLASSIC CHECK OF FIELD IN HASH
.B awk 'FNR==NR{hash[$1]; next}$2 in hash' file1.txt FS='|' file2.txt

file1 has patterns only. file2 has three fields in format field1|field2|field3. 
Target is to print lines from file2 in which field 2 match the patterns of file1.

for the first file (FNR==NR) an array hash is created with index the field 1 of file1 (= the pattern).
If file1 has foo in a line , we would create hash["foo"]. This happens for all the lines / patterns of file1
When next finishes the file1, we go to file2. We check if field 2 ($2) bellongs to the array hash.
If this condition returns true, awk default action is performed (print $0 of file2)
This is equivalent to if ($2 in hash) {print $0}, but written in a smart compact way.

##ALTERNATIVE GREP METHOD BY GV
awk 'NR==FNR{a[$0]=1;next}a[$0]' patterns bigfile #replace $0 with $2
This will print all the lines of big file that a[$0] of big file has a regular -different than zero - value
If a[$0] of bigfile is null that means that a[$0] was not found in patterns = array element not created

##GREP -v method by GV
awk 'NR==FNR{a[$0]=1;next}!a[$0]' patterns bigfile #replace $0 with $2
All lines in bigfile not found in patterns file will return false which will be reverted to true & printed

.B GV Method Pitfalls:
As a matter of speed even in big files has been proved that the a[$2] vs ($2 in a) is the same speed.
Though my method with checking status of a[$2] has a major pitfall. 
If the $2 of bigfile is not valid in array a, then will return null/zero , will not be printed (that is desiarable).
But on the same time this array element WILL BE CREATED and this means that you consume huge amount of memory without reason.

On the other hand, the method ($2 in a) checks for existance of $2 in indeces of array a but is not creating array elements.

One workaround for a[$2] would be to delete the array element afterwards in order to free space. 
But if you delete a[$2] after print, the second time that same pattern is found in bigfile might not be printed (like grep -m1)

Unless is to be done something like this (not tested)
awk 'NR==FNR{a[$0]=1;next}a[$0]?print:delete a[$0]' patterns bigfile #If a[$0] returns true print $0, else delete a[$0]

##ALTERNATIVE CLASSIC GREP NOT RESTRICTED TO CHECKING FIELD 2:
awk 'FNR==NR{hash[$1]; next}{for (i in hash) if (index($0,i)) {print; break}}' file1.txt FS='|' file2.txt
Same logic as before (file1 loaded in hash array) but the whole line of file2 ($0) is compared to hash array indexes
This will allow matches even if pattern of file1 has been found at any field of file2 (1-2-3), not only in field 2 as before.
This is directly equivallent 'grep -F -f file1 file2'

Can be done also using match : 
awk 'FNR==NR{hash[$1]; next}{for (i in hash) if (match($0,i)) {print; break}}' file1.txt FS='|' file2.txt

GV Way: 
Based on the return status of a[$1] or a[$2] or a[$3] . If null/zero returns false = no print. If has a value = true = print

awk 'NR==FNR{a[$0]++;next}(a[$1] || a[$2] || a[$3])' patterns bigfile #considering that patterns file has only one field


##AWK:MORE ARRAYS EXAMPLE
[: http://unix.stackexchange.com/questions/344856/take-two-columns-in-a-tab-delimited-file-and-merge-into-one/344880?noredirect=1#comment610394_344880 :]
awk 'BEGIN{FS=OFS=" "}{z[NR]=$1FS$3; print $1,$2}END{for (i=1; i<=NR; i++){print z[i]}}' a.txt


##AWK:CHALLENGE : Modification of awk output with 1
http://awk.freeshell.org/AwkTips
In awk we follow the 'condition{action}' rule over and over. 
We can have just '{actions}' which presume a missing true condition, thus actions will be always executed.
On the contrary we can apply just '1' as condition = true which will perform default action = print $0
Real Example:
Suppose a file with contents line1-line2-line3

awk '{sub(/ine/,"foo")}' or awk 'sub(/ine/,"foo")'    --> Output will be lfoo1-lfoo2-lfoo3
awk '{sub(/www/,"foo")}' or awk 'sub(/www/,"foo")'    --> Output will be nothing! No substitution made.
awk '{sub(/www/,"foo")}1' or awk 'sub(/www/,"foo");1' --> Output will be all the lines, no matter if the subs succeed or failed.

awk '{sub(/1/,"foo")}1' or awk 'sub(/1/,"foo");1' --> Output will be all the lines; replaced (like line1 = linefoo) or not. 
.B Thus this command with the use of ;1 is directly equivalent to sed 's/pattern/replacement' 

##AWK:MULTIDIMENSIONAL 2D ARRAY
https://www.gnu.org/software/gawk/manual/html_node/Arrays-of-Arrays.html

$ cat file1.txt
denovo23    HNS.2_9729  HNS.2_20867
denovo28    HNS.6_14948 HNS.6_148211    HNS.11_327521
denovo62    HNS.7_468475    HNS.7_631780
denovo897   WNA.2_58410 WNA.1_175071
denovo621   WNA.2_20180 WNA.2_294219
denovo622   CES.1_24310 HNS.6_26786

$ awk '{a[$1][$2]=$3}END{for (i in a) {for (k in a[i]) print i,"-",k,":",a[i][k]}}' file1.txt
denovo28 - HNS.6_14948 : HNS.6_148211
denovo897 - WNA.2_58410 : WNA.1_175071
denovo23 - HNS.2_9729 : HNS.2_20867
denovo621 - WNA.2_20180 : WNA.2_294219
denovo622 - CES.1_24310 : HNS.6_26786
denovo62 - HNS.7_468475 : HNS.7_631780

$ awk '{a[$1][$2]=$3}END{for (i in a) print i}' file1.txt
denovo28
denovo897
denovo23
denovo621
denovo622
denovo62

$ awk '{a[$1][$2]=$3}END{for (i in a) print i,a[i]}' file1.txt
awk: cmd. line:1: (FILENAME=file1.txt FNR=6) fatal: attempt to use array `a["denovo28"]' in a scalar context



#-----------------------------------------------------------------------------------------------------------------------

AWK:PRINT/SPLIT A FILE BASED ON PATTERN
[: http://unix.stackexchange.com/questions/344890/to-split-files-under-linux-shell/344898?noredirect=1#344898 :]
[: http://www.tutorialspoint.com/execute_bash_online.php?PID=0Bw_CjBb95KQMVXZWWUtrMUx1Ums :]
Consider file with multiple sections like 
Start of test case:test1
a
b
c
Date is feb 12

Target : To print/split each section in it's own file.
awk '/Start of test case/{i++}{print >"a"i}' a.txt
Counter will be increased in the next found pattern "Start of test case". Each section will be sent to file a1, a2,a3,etc
With awk you don't need >>. It seems that the file remains open - or awk buffers the data and send them at one push to file.
For testing : awk '/Start of test case/{i++}{print i,$0}' a.txt


##SPLIT A FILE BY PARAGRAPHS / CHAPTERS
For a file like :
> Rosa Luxemburg
aaaaaaaaa
aaaaaaaaaa
> Charles Darwin
bbbbb
bbbbbbb
bb
> Charles Chaplin
cccc
ccccc
c
> George Vasiliou
dd
ddddd
dd
dddd

We can print only paragraphs/chapter starting with 'Charles' using: awk '/^>/{p=0}/^> Charles/{p=1}p' file9
This will set p=0 for every line starting with > (paragraph/chapter separator) unless Chapter begins with "Charles" --> p=1
Value of p is preserved during the whole awk script. That means that if next line is not starting with >, p will keep having the previous value.

Alternative by Ed:
As advised by Ed, this can be done using just
awk /^>/{p=/Charles/}p

This is insanely idiomatic. We have var assignment based on regex /../
Actually this means that p will become 1/true only if Charles exist in $0
This is identical to awk '/Charles/' file , but assigned to a variable p.

awk '/Charles/' will print all records containing Charles. 
This means that expression /Charles/ returns true = 1 and performs default action.
So {p=/Charles/} will assign 1/true to var p , for all records containing /Charles/.
If the record starts with > (paragraph separator) but does not contain Charles, expression /Charles/ will return false=0 thus p will be 0.
If record/line does not start with > then {p=/Charles/} will not be performed at all and p will keep it's previous value (0 or 1 respectively) 
Remember that null or zero values are considered as false by awk, and thus default action is not performed.
Value of 1 (or any value actually other than null or zero) is considered as true by awk and default action is performed.


#-----------------------------------------------------------------------------------------------------------------------
AWK:COMBINE CSV FILES - TAB SEPARATED (emulates JOIN)
http://unix.stackexchange.com/questions/345051/concatenate-csv-with-some-shared-columns
$ awk -F"\t" '{a[1]="";{for (i=1;i<=NF;i++) if (i==6 ||i==7) continue;else \
if ($i!="") a[1]=a[1]FS$i;else a[1]=a[1]FS"NaN";print a[1]}}' <(paste b.txt c.txt)
==EQUALS TO ==
$ join --nocheck-order -eNaN -13 -22 -t$'\t' -o 1.1 1.2 1.3 1.4 1.5 2.3 2.4 b.txt c.txt

Output:                        
A   B   C   D   E   F   G    
1   2   3   4   5   6   7    
NaN 1   2   NaN 1   2   1    
#-----------------------------------------------------------------------------------------------------------------------
AWK:UNDERLINE HEADERS by Gilles
http://unix.stackexchange.com/questions/349790/how-to-insert-a-new-line-after-the-first-line-in-a-file-that-is-similar-to-it/349885?noredirect=1#comment619305_349885

##ADD DASHES BELLOW EACH FIELD OF 1st line (not space nor tab) 
awk '{print} NR==1 {gsub(/[^\t ]/, "-"); print}'    
Incredibly smart: 
The initial {print} prints whatever record comes in awk <==> print $0.
If NR==1 = 1st line, global replacement of all chars that is not a tab nor a space with a dash. Then print
You don't need to calculate any lenght etc. You just replace the whole $0 with dashes , excluding tabs and spaces (delimiters)
When the second line comes in, initial {print} will print $0, but the replacement gsub will be skipped since NR >1

##VARIATIONS
awk '{print} NR==1 {gsub(/[^ \t]/, "-"); while (sub(/- -/, "---")) {}; print}'  #Tricky:WIll insert a long dashed line bellow 1st line.
awk '{print} NR==1 {gsub(/[^\t]/, "-"); print}'            #Add a dash bellow every non tab char of line1.

#-----------------------------------------------------------------------------------------------------------------------
AWK:ONELINERS
http://www.pement.org/awk/awk1line.txt
    http://www.catonmat.net/series/awk-one-liners-explained
http://www.pement.org/awk/
http://awk.freeshell.org/AwkTips


Tip: print is the default action of awk, so even if ommited, print will be performed.
Tip: In condition checks it is usuall that /string/ refers to patterns . For fixed checks do not ude /../ but simmply =="..."

sort -u file | awk 'NR==FNR{seen[$1]++;next}seen[$1]==1' - file     
# Use of awk combined with pipe/stdin and a file *actually the same file. Also possible with awk '...' file file (doube pass)
# http://unix.stackexchange.com/questions/348252/print-if-for-the-same-1st-field-theres-a-single-value-of-2nd-field-on-all-lines

awk '1; NR%3==0 {print "----"}' file            
# Prints some dashes every three lines. Actually is a division modulo NR(lines) / 3 = 0. Modulo will be zero for NR 3,6,9,etc (multipliers of 3) and non-zero in all other numbers.

awk 'FNR==1 && $1==8 && $2>500{print FILENAME}' *.csv     
# Prints file names of all the files satisfying all three criteria (Field 1 to be 8 and field 2 to be more than 500 at first line of each file)

awk 'BEGIN{FS="";OFS="-"}$1=$1'             #Input chars will be transformed to dash separated. 
                                            #By setting FS to null "" awk treats each char as a field. 
                                            #$1=$1 forces record reconstruction acc to new different OFS="-"

awk '{print $1}' file                       #Print first field for each record in file
awk '{print FNR "\t" $0}'                   #Print line numbers in each file = cat -n
awk '/regex/'                               #print only lines which match regular expression (emulates "grep")
awk 'BEGIN {IGNORECASE=1};/regexp/'         #case insensitive grep
awk '!/regex/'                              #print only lines which do NOT match regex (emulates "grep -v")
awk '$2 == "foo"' file                      #Print any line where field 2 is equal to "foo" in file
awk '$2 != "foo"' file                      #Print lines where field 2 is NOT equal to "foo" in file
awk '$1 ~ /regex/' file                     #Print line if field 1 matches regex in file
awk '$1 !~ /regex/' file                    #Print line if field 1 does NOT match regex in file
awk 'if ($0 ~ /edu/ && $0 ~ /li/) print'    #Two Patterns - Boolean AND
awk ‘$0 ~ /foo/ && ($2 == bar++)’           # AND Operation. bar is not ++ unless ‘foo’ is found on record. 
/foo|bar|baz/  { buzzwords++ }END{print buzzwords, "buzzwords seen" }    #regex pattern with or + counting + printing of counts
awk '$7  ~ /^[a-f]/'                        #print line if field #7 matches regex
awk '$7 !~ /^[a-f]/'                        #print line if field #7 does NOT match regex
awk 'NR!=1{print $1}' file                  #Print first field for each record in file excluding the first record
awk 'END{print NR}'                         #count lines (emulates "wc -l").Actually prints NR after end of file. 
awk '$1 > $2 {print $1,$2,$1-$2}'           #prints Field1 , Field2 and difference $1-$2 for all lines that $1>$2

awk '/foo/{n++}; END {print n+0}' file      #Print total number of lines that contain foo
awk '{total=total+NF};END{print total}'     #Print total number of fields in all lines

awk '/regex/{getline;print}' file           #Print line immediately after regex, but not line containing regex in file
awk 'length > 32' file                      #Print lines with more than 32 characters in file

awk 'NR==12' file                                       #Print line number 12 of file
awk '{s=0; for (i=1; i<=NF; i++) s=s+$i; print s}'      #print the sums of the fields of every line
awk '{for (i=1; i<=NF; i++) s=s+$i}; END{print s}'      #print the sums of the fields of ALL lines

awk 'BEGIN{ORS="\n\n"};1'               #double space a file - also awk '1;{print ""}'
awk '1;{print "\n"}'                    #triple space a file

awk '{print FNR "\t" $0}' files*            #precede each line by its line number FOR THAT FILE (left alignment).
awk '{print NR "\t" $0}' files*             #precede each line by its line number FOR ALL FILES TOGETHER, with tab.
awk '{printf("%5d : %s\n", NR,$0)}'         #number each line of a file (number on left, right-aligned)
x == y ? a[i++] : b[i++]                    #Ternary if operation ; mind the trick to increment array indexes
awk 'NF{$0=++a " :" $0};1'                  #number each line of file, but only print numbers if line is not blank

awk '/Beth/{n++}; END {print n+0}' file                         #print the total number of lines that contain "Beth"
awk '$1 > max {max=$1; maxline=$0}; END{ print max, maxline}'   #print the largest first field and it's line
awk '{ print NF ":" $0 } '                              #print the number of fields in each line, followed by the line
awk '{ print $NF }'                                     #print the last field of each line
awk '{ field = $NF }; END{ print field }'               #print the last field of the last line
awk ’length($0) > 80’ data                              #print lines with length more than 80
awk ’NF > 0’ data                                       #print any line with at least one record=no blank line
awk 'NF > 4'                                            #print every line with more than 4 fields
awk '$NF > 4'                                           #print every line where the value of the last field is > 4
awk -v RS="\0" '{print NR,$0}'                          #Use null char \x0 as record separator. 
                                                        #Whole file is treated as one "stream" with "\n" chars inside 

awk 'BEGIN{while (a++<513) s=s " "; print s}'           #create a string / generate 513 spaces
awk 'BEGIN { str=sprintf("%40s", " ");gsub(" ","-",str);print str }'    #Print 40 spaces and then convert to 40dashes 
gawk --re-interval 'BEGIN{while(a++<49)s=s " "};{sub(/^.{6}/,"&" s)};1' #insert 49 spaces after column #6 
awk '{print} NR==1 {gsub(/[^\t ]/, "-"); print}'    #Tricky by Gilles: Add dashes bellow each field of 1st line (not space nor tab) 
awk '{print} NR==1 {gsub(/[^ \t]/, "-"); while (sub(/- -/, "---")) {}; print}'  #Tricky:WIll insert a long dashed line bellow 1st line.
awk '{print} NR==1 {gsub(/[^\t]/, "-"); print}'            #Add a dash bellow every non tab char of line1.


awk '{ print $NR }'                             #Will print $1 for 1st line, $2 for second line, etc
awk 'NR < 11'                                   #print first 10 lines of file (emulates behavior of "head")
awk 'NR>1{exit};1'                              #print first line of file and then exits(emulates "head -1")
awk '{y=x "\n" $0; x=$0};END{print y}'          #print last 2 lines (emulates "tail -2"). Alt: y=x ORS $0
awk 'END {print}'                               #emulates "tail -1"
awk 'END {print NR}'                            #print the number of lines of a file (emulates wc -l <file)
awk '$5 == "abc123"'                            #print any line where field #5 is equal to "abc123"
awk '$5 != "abc123"' OR awk '!($5 == "abc123")' #print only those lines where field #5 is NOT equal to "abc123"
awk '/regex/{print x};{x=$0}'                   #print line before a regex - exluding line containing the regex
awk '/regex/{getline;print}'                    #print line after a regex, excluding line containing the regex
awk '/AAA/ && /BBB/ && /CCC/'                   #grep for AAA and BBB and CCC in any order on the same line. 
awk '/AAA.*BBB.*CCC/'                           #grep for AAA and BBB and CCC (in that order)
awk '/regex/,0' OR awk '/regex/,EOF'            #print range of file from regex till EOF (end of file)
awk 'NR==8,NR==12'                              #print section of file based on line numbers (lines 8-12, inclusive)
awk 'NR==52 {print;exit}'                       #print line number 52 - efficient on large files. Alternative awk 'NR==52'
awk '/patt1/,/patt2/'                           #print a range/section between two regexps (inclusive-case sensitive)
awk '/patt1/,/patt2/{if ($0 ~ /patt3/) print}'  #print line with patt3 in range patt1-patt2 (inclusive)
awk '/patt1/{f=1;next}/patt2/{f=0}f && /patt3/' #print line with patt3 in range patt1-patt2 (patt1-patt2 exluded)

awk '$1 ~ /foo/ { print $0 }' or '$1 ~ /foo/'   #print if field1 matches pattern foo

awk NF                              #delete ALL blank lines from a file (same as "grep '.' ") - alternative awk '/./'
awk 'NF--'                          #removes last field and prints the line. Reconstructs record without last field 
awk 'NF-=3'                         #removes last three fields. Gives fatal error if record has less than 3 fields
awk 'a !~ $0; {a=$0}'               #remove duplicate, consecutive lines (emulates classic "uniq")
awk '!a[$0]++'                      #remove duplicate, nonconsecutive lines - even unsorted!! = Advance uniq 
awk '!($0 in a){a[$0];print}'       #alternative to above

gawk -v BINMODE="w" '1' infile >outfile     #IN DOS ENVIRONMENT# convert DOS newlines (\r\n=CR & LF) to Unix format. Alternative: tr -d \r <infile >outfile

awk '{sub(/^[ \t]+/, "")};1'                #delete whitespace (spaces, tabs) in front of each line - left align file
awk '{sub(/[ \t]+$/, "")};1'                #delete trailing whitespace (spaces, tabs) from end of each line

awk 'BEGIN{system("a=5 && echo system a=$a")}'    #Call a shell by awk . variable will not be available in main shell.
                                                  #Alternative: export to file in form a="5" and source the file.

awk '{print (NF? ++a " :" :"") $0}'                             # number each line of file, but only print numbers if line is not blank
awk '{for (i=1; i<=NF; i++) if ($i < 0) $i = -$i; print }'      # replace each field with its absolute value
awk '{for (i=1; i<=NF; i++) $i = ($i < 0) ? -$i : $i; print }'  # replacing each field with its absolute value
awk '{ total = total + NF }; END {print total}' file        # print the total number of fields ("words") in all lines
awk '{sub(/\r$/,"")};1'                        #IN UNIX ENVIRONMENT: convert DOS newlines (CR/LF) to Unix format. Assumes EACH line ends with Ctrl-M
awk '{sub(/$/,"\r")};1'                        # IN UNIX ENVIRONMENT: convert Unix newlines (LF) to DOS format
awk 1                                # IN DOS ENVIRONMENT: convert Unix newlines (LF) to DOS format

awk '{gsub(/^[ \t]+|[ \t]+$/,"")};1'          #delete BOTH leading and trailing whitespace from each line
awk '{$1=$1};1'                               #also removes extra space between fields

awk '{sub(/^/, "     ")};1'                     #insert 5 blank spaces at beginning of each line (make page offset)
awk '{printf "%79s\n", $0}' file*                 #align all text flush right on a 79-column width
awk '{l=length();s=int((79-l)/2); printf "%"(s+l)"s\n",$0}' file*     #center all text on a 79-character width

awk '{sub(/foo/,"bar")}; 1'                               #replace "foo" with "bar" only 1st instance
gawk '{$0=gensub(/foo/,"bar",4)}; 1'                      #replace "foo" with "bar" only 4th instance
awk '{gsub(/foo/,"bar")}; 1'                              #replace "foo" with "bar" ALL instances in a line

awk '/baz/{gsub(/foo/, "bar")}; 1'                     #substitute "foo" with "bar" ONLY for lines which contain "baz"
awk '!/baz/{gsub(/foo/, "bar")}; 1'                    #substitute "foo" with "bar" EXCEPT for lines which contain "baz"
awk '{gsub(/scarlet|ruby|puce/, "red")}; 1'                 #change "scarlet" or "ruby" or "puce" to "red"
awk '{a[i++]=$0} END {for (j=i-1; j>=0;) print a[j--] }' file*         #reverse order of lines (emulates "tac")
awk '/\\$/ {sub(/\\$/,""); getline t; print $0 t; next}; 1' file*     #if a line ends with a backslash, append the next line to it (fails if there are multiple lines ending with backslash...)
awk -F ":" '{print $1 | "sort" }' /etc/passwd           #print and sort the login names of all users
awk '{print $2, $1}' file                               #print the first 2 fields, in opposite order, of every line
awk '{temp = $1; $1 = $2; $2 = temp}' file              #switch the first 2 fields of every line
awk '{ $2 = ""; print }'                                #print every line, deleting the second field of that line
awk '{for (i=NF; i>0; i--) printf("%s ",$i);print ""}'  #print in reverse order the fields of every line
awk 'ORS=NR%5?",":"\n"' file                            #concatenate every 5 lines of input, using a comma separator
awk '$3 != "None" && $4 != "None" {print $2}'           #Print if two fields match the same pattern
awk 'length < 64'                                       # print only lines of less than 65 characters

echo This++++this+++is+not++done | awk '{gsub(/\++/," ");}1' ==> Removes all +. Actually replaces two ++ with a space
echo This++++this+++is+not++done | awk -F'++' '{$1=$1}1' ==> Same as above (http://stackoverflow.com/questions/14432209/substitute-a-regex-pattern-using-awk). This works because OFS is not assigned = default = space. The $1=$1 is reconstructing the output record with the use of OFS.

awk 'BEGIN{OFS=FS="|"}{$0=$3OFS$2OFS$1}{print $1}' file7     # This reassigns $0 = swaps original field1 with 3. After swap, awk prints field $1 of the new $0 ==> $3 of original file

awk '!a[$0]++' file1 file2 file3                #This prints unique lines among all files even if UNSORTED!! http://unix.stackexchange.com/questions/159695/how-does-awk-a0-work

awk 'a[$0]++' file1 file2 file3                    #This prints all non unique lines among all files even if UNSORTED!!

awk '(a[$0]++ == 1)'                          #Direct equivalent to uniq -d - Arithmetic Evaluation - not boolean

awk 'NR==FNR{a[$2];next}$2 in a' file1 file2         #grep -f operation - exact match of $2 / file1 with $2/file2. Prints matches. 

awk 'NR==FNR{a[$2]=1;next}a[$2]' file1 file2        #Same as above but even more tricky. This works justs by boolean checking if a[$2] when reading the second file has a value or not (set by file1). Null value=No Value = False=no print.


awk 'NR==FNR{a[$2];next}!($2 in a)' file1 file2                    #like above but prints unmatched results = grep -v -f. 
awk '{if (NR==FNR){a[$2];next};if (!($2 in a)) print}' file1 file2     #Long Equivalent of the above short oneliner.

awk 'BEGIN{OFS=FS="|"}{if (FILENAME=="file8") $0=$1OFS$1OFS$1}!a[$2]++' file8 file7           # This reassings $0 of file8 in order $2 to match $2 of file 7 and then removes duplicates based on $2.

awk 'BEGIN{OFS=FS="|"}{if (FILENAME=="file8") {$0=$1OFS$1OFS$1} else {b=$0}}{if (a[$2]++) print b}' file8 file7 # Prints duplicates of file 8 (patterns) and file7 (data). 
Bug is that comparison is made on field2 (in both files) so if data file7 has same $2 with different $1 and $3 will be exluded also.

awk 'BEGIN { cnt = 10; cnt += 10; print "Counter =", cnt }'       #cnt is increased by 10,prints 20. Also available: -= ,*= (multiply),/= (divide),%= (modulo), ^= or**=(raise)

awk '{ $6 = ($5 + $4 + $3 + $2) ; print $6 }' inventory-shipped   # Assign contents to a field, even if the field is out of normal range

echo 1 | awk '{ printf("%02d\n", $1) }'         # Prints numbers with leading zero. If number is >10 then no leading zero is added.

.B Concatenate / Join Lines

$ cat file1
Now is the time for
all good men
The quick brown fox
jumped over

$ awk '{printf "%s%s", $0, (/^[[:upper:]]/?OFS:ORS)}' file1
Now is the time for all good men
The quick brown fox jumped over
-------------------------------------------------------------------------------------------------------------------------------------

AWK:READ FROM 2 FILES
##BASICS
Consider the following two files:

# cat file1
f1d1 f1d2 f1d3 f1d4
f1d5 f1d6 f1d7 f1d8
f1d9 f1d10 f1d11 f1d12

# cat file2
f2d1 f2d2 f2d3 f2d4
f2d5 f2d6 f2d7 f2d8
f2d9 f2d10 f2d11 f2d12

The main idea is that the files are read in series by awk. So the common workaround is to make a check if NR == FNR.
NR is the line counter continiously increasing along all files. FNR is per file counter = resets on every new file.
The condition NR==FNR is true only for the first file, because on second file the NR keeps increasing and differs from the resetted FNR:

awk '{print "NR:",NR,"FNR:",FNR,"fname:",FILENAME,"Field1:",$1}' file1 file2
NR: 1 FNR: 1 fname: file1 Field1: f1d1
NR: 2 FNR: 2 fname: file1 Field1: f1d5
NR: 3 FNR: 3 fname: file1 Field1: f1d9
NR: 4 FNR: 1 fname: file2 Field1: f2d1
NR: 5 FNR: 2 fname: file2 Field1: f2d5
NR: 6 FNR: 3 fname: file2 Field1: f2d9

This is why this usage is frequent: awk 'NR==FNR {file1[$1]=$0; next} {$1=file[$1]; print}' file1 file2

##COMBINE FILES WITH NO COMMON KEYS:
You just need a way to store the records/lines from first file to memory, in order to be printed later during processing of file2..

awk 'NR==FNR {c++;a[c]=$1;next}{d++;print $1,a[d]}' file2 file1
f1d1 f2d1 #Combined first fields of line 1 of both files
f1d5 f2d5
f1d9 f2d9

You can assign to array a whatever field you need from first file (file 2 in example above)
This is different from the getline var <file method which allways will send in var the whole line = $0
The next forces awk to go on the next record and the second group of commands {print..} is not performed. 

When all the records of the first file are finished by the next, then we enter at the next group of commands{d++;print...} for the second file.
Obviously the next command is valid for the file in process.When it finishes, the script goes on to second group of commands.

Keeping more field from first in the read file:
# awk 'FNR==NR{c++;a[c]=$2 FS $3;next}{d++;print $0, a[d]}' file2 file1
# here we have combined fields in each printed line from both files
f1d1 f1d2 f1d3 f1d4 f2d2 f2d3 
f1d5 f1d6 f1d7 f1d8 f2d6 f2d7
f1d9 f1d10 f1d11 f1d12 f2d10 f2d11

Smart Way: Instead of double counters you can just use
awk 'FNR==NR{a[FNR]=$2 FS $3;next}{print $0, a[FNR]}' file2 file1

a[FNR] will be a[1] for field record 1 of first file2, a[2] for FNR2 , etc

The trick is that when first file2 finishes and second file in the row (file1) is processed , FNR resets back to 1 and thus the calls to a[FNR] are valid.
This will break if file1 and file2 have a different number of rows.
Break means that an invalid call to a[] will print nothing - no error is given (i.e try {print $0,a[500]})

##COMBINE FILES WITH A COMMON KEY
This is the same , but in a smarter way you store first file in array with key that equals to the common entry:
Join of bellow files in common field (5 of file 1 - 1 of file 2)
# cat file1
f1d1 f1d2 f1d3 f1d4 f2d1
f1d5 f1d6 f1d7 f1d8 f2d5
f1d9 f1d10 f1d11 f1d12 f2d9
# cat file2
f2d1 f2d2 f2d3 f2d4
f2d5 f2d6 f2d7 f2d8
f2d9 f2d10 f2d11 f2d12

# awk 'FNR==NR{a[$1]=$2 FS $3;next}{ print $0, a[$5]}' file2 file1
f1d1 f1d2 f1d3 f1d4 f2d1 f2d2 f2d3
f1d5 f1d6 f1d7 f1d8 f2d5 f2d6 f2d7
f1d9 f1d10 f1d11 f1d12 f2d9 f2d10 f2d11

Now the calls to array are valid, since the elements are stored in a[$1] of file2 = a[f2d1] (for first line)
So when the second file1 is read, the call to a[$5] = a[f2d1] = the stored values of first file2 are printed.

----------------------------------------------------------------------------------------------------------------------------------
AWK:REGEX MATCH AND REPLACE on XML Files

##XML REPLACE NO1
http://stackoverflow.com/questions/42307534/get-attributes-of-a-parent-tag-in-xml/42322481#42322481
Consider a file with various xml entries like:
<tile x="765" y="491" z="7">
    <item id="2114"/>
</tile>

Below Code gets item id and brings the x-y-z values for all fields. tile x-y-z are not always the previous row.

term="2114";awk -v term=$term '{a[NR]=$0; if ($0 ~ term) {i=NR;while (a[i] !~ "<tile x=.+ y=.+ z=.+") i--; print gensub(/(<tile) (.+)(>)/,"\\2","g",a[i]),":",gensub(/([ ]*)(<item )(.+)(\/>)/,"\\3","g",$0)}} ' file5
>x="765" y="491" z="7" : id="2114"
The logic : If you find the search term we look for, then go back to the stored data and when you find a line with x-y-z data print it together with the item id

Mind the regex here:
gensub(/([ ]*)(<item )(.+)(\/>)/,"\\3","g",$0)
First part=/([ ]*)(<item )(.+)(\/>)/
This is divided in four groups:
([ ]*) #white space before item
(<item ) #literal string match
(.+) #any char following previous <item
(\/>) #closing of the line />

By using parenthesis you can group your regex; You can later select which part of regex you need to call (we call 3rd group = id="2114")

##XML REPLACE NO2 - https://goo.gl/DL2hBl
$ cat file
<word key="ACTIVE" group="Service application" value="testvalue1"/>
<word key="ACTIVE" group="Service application" value="testvalue2"/>
<word key="ACTIVE" group="Service application" value="testvalue3"/>
<word key="ACTIVE" group="Service application" value="testvalue4"/>
<word key="ACTIVE" group="Service application" value=""/> #mind the empty value

$ awk '{print gensub(/(value=")(.+)(".+)/,"\\2","g",$NF)}' file
testvalue1
testvalue2
testvalue3
testvalue4
            #Mind the missing field due to missing value on last line of file

$ awk 'FNR==NR{a[FNR]=$0;next}{print gensub(/(value=")(.*)(".+)/,"\\1"a[FNR]"\\3","g",$NF)}' file2 file ##file2 has 4 lines = newvalue1..4
value="newvalue1"/>
value="newvalue2"/>
value="newvalue3"/>
value="newvalue4"/>
value="newvalue5"/>

$ awk 'FNR==NR{a[FNR]=$0;next}{$NF=gensub(/(value=")(.*)(".+)/,"\\1"a[FNR]"\\3","g",$NF);print}' file2 file #Working with groups
# OR without groups
$ awk 'FNR==NR{a[FNR]=$0;next}{$NF=gensub(/value=".*"\/>/,"value=\""a[FNR]"\"\/>","g",$NF);print}' file2 file #No groups
<word key="ACTIVE" group="Service application" value="newvalue1"/>
<word key="ACTIVE" group="Service application" value="newvalue2"/>
<word key="ACTIVE" group="Service application" value="newvalue3"/>
<word key="ACTIVE" group="Service application" value="newvalue4"/>
<word key="ACTIVE" group="Service application" value="newvalue5"/> #Mind that even the empty value is caught and replaced.

This code assumes (acc to OP) that new values are loaded in a new file2 with so many lines as original xml file.
Since we modify the NF valuer, this code will work only if "value" is the last field of each line = awk $NF

-------------------------------------------------------------------------------------------------------------------
AWK:IDIOMATIC CODE EXAMPLES
http://backreference.org/2010/02/10/idiomatic-awk/

##ASSIGNMENT AND EVALUATION OF THE ASSIGNMENT RESULT - ALL AT THE SAME TIME
https://www.gnu.org/software/gawk/manual/html_node/Conditional-Exp.html#Conditional-Exp

One of the best idiomatic usage is this one, demonstrating assignment and evaluation on the same time:
$ printf '%s\n' line{1..10} |awk ' ORS = NR%5 ? FS : RS '
line1 line2 line3 line4 line5
line6 line7 line8 line9 line10

Output Record Separator = Every Fifth Line NR%5 (warning - this is an assignment , not a condition check) 

So if NR%5 is zero (ie, we are at line 5, 10, 15, etc.), ORS gets the value of RS (which by default contains "\n"); otherwise ORS gets the value of FS (by default, a space). 
If RS and FS have their standard values, it can be rewritten like this: awk 'ORS=NR%5?" ":"\n"'

If NR%5 is zero , then ORS=NR%5 ==> ORS=0 (assignment)  ==> on the same time, assignment is evaluated as false ==> : section is printed (RS)
if NR%5 <> zero , then ORS=NR%5 ==> ORS<>0 (assignment) ==> on the same time, assignment is evaluated as true  ==> ? section is printed (FS)
Remember that for awk, whatever is not null or zero is evaluated as "true", no matter if real value is 10 or -10.

.B Check and Assign max value
awk '{max=($2>max)?$2:max};END{print max}' 2016203.csv         #Prints the max value of $2 inside csv file

.B Convert Rows to Columns
awk '{S=S?S OFS " " $0 "" :" " $0 " "} END{print S}' OFS=, <(echo -e '1\n2\n3')  #Checks if S is set and assgins S value accordingly
Equals to : "assignment of S"=(evaluation of S)?case if true evaluation:case if false evaluation
Equals to : if (S) { S=S OFS s1 $0 s1 } else { S = s1 $0 s1 } #if (S) is assigned then ..true part... else ...false part....
The very first time this runs , S in unassigned thus the "false" code block is executed

##INITIAL VALUES

$ cat matrix.txt
a1;a2;a3;a4;a5
b1;b2;b3;b4;b5
c1;c2;c3;c4;c5
d1;d2;d3;d4;d5

$ awk -F\; '{for(i=1;i<=NF;i++)r[i]=r[i] sep $i;sep=FS}END{for(i=1;i<=NF;i++)print r[i]}' matrix.txt
a1;b1;c1;d1
a2;b2;c2;d2
a3;b3;c3;d3
a4;b4;c4;d4
a5;b5;c5;d5

For the very first time that for loop is executed, sep has null value and so has r[i]. 
As a result, the very first time this one 
r[i]=r[i] sep $i equals to r[i]=$i
The second time (second line) r[i] and sep have a value set previously - on the first record.

##SELF ASSIGNMENT - CHANGING OUTPUT FORMAT 
This uses the $1=$1 trick, which fools awk to reconstruct the $0 using the OFS separator:
$ echo 'foo;bar' | awk -v FS=';' -v OFS=',' '/foo/'               ---> foo;bar (OFS ignored)
$ echo 'foo;bar' | awk -v FS=';' -v OFS=',' '/foo/{$1=$1}1'       ---> foo,bar ($0 rebuilt using the OFS)

Combining the simultaneous assignment and evaluation, also this works fine:
# echo 'foo;bar' | awk -v FS=';' -v OFS=',' '$1=$1'               ---> foo,bar

##AWK vs SED
http://www.pement.org/awk/awk_sed.txt
sed "s/from/to/3"       awk '{$0=gensub("from","to",3); print}'
sed "s/reg[ex]/ZZ/4"    awk '{$0=gensub(/reg[ex]/, "ZZ", 4); print}'
sed "s/.*=/equ/5"       awk -v var=".*=" '{$0=gensub(var,"equ",5);print}'

##DIFFERENT / CHANGE RS FOR THE SECOND FILE (by Ed Morton)
##RS TRICKS
You can define a global RS in the very beginning using awk -v RS="-", but you also can declare RS inside BEGIN{...}.
But you can also define RS in the end of awk command, just before file to be parsed.
like awk '1' RS="-" file1.txt

The tricky part is that you can define different RS for diffent input files.
For example awk '1' file1 RS=">" file2 , will use default RS for parsing file1 and will use ">" as a separator for file2.
Quite usefull when you want to isolate blocks of input, separated by a kind of header.

$ cat file1
key1,field1,key2,field2

$ cat file2
key4;field4;key5;field5

$ awk '{print FNR,$0}' RS=',' file1 RS=';' file2
1 key1
2 field1
3 key2
4 field2

1 key4      #second file: FNR resets, RS redifined to ;
2 field4
3 key5
4 field5

-------------------------------------------------------------------------------------------------------------------
AWK:USER DEFINED FUNCTIONS

##USER FUNCTIONS
$ awk 'function t(num1,num2) { dd=2*ss;return num1*num2; };BEGIN{ss=111;print t(3,9);print dd}'
27			#value of t
222			#value of dd,defined in function, printed outside the function

.B Remarks:
1. Local vars defined inside a function (like var dd) are available in the whole program (outside the function)
2. Vars defined in program outside of the function (like var ss) are available in the function. 
   This is why dd is calculated correctly as 2*ss => 2*111
3. Return of a function can be even string , i.e return "ok" and will work nicely

##GREP EMULATION MULTIPLE FILES
http://stackoverflow.com/questions/42415826/how-to-get-two-files-having-max-difference-among-a-series-of-files/42419795#42419795
bash while loop and awk 'NR==FNR{a[$2];next}!($2 in a)' file1 file2 per pair of files = 30 awks

SubRoutines Testing

awk -v file1="20161202.csv" -v file2="20161203.csv" 'BEGIN{while (getline var <file1) {split(var,ff1,OFS);a[ff1[2]]}; \
while (getline var2 <file2) {split(var2,ff2,OFS);if (!(ff2[2] in a)) print var2}}'
123456 50000 some value
123457 70000 some value
#This is indeed the different rows between file1 - file2 given as fixed strings here

awk -v file="20161202.csv" 'BEGIN{FS="";split(file,fn,FS);print fn[1],"-",fn[2],"-",fn[3]}'         #Splits filename of file by chars -output= 2-0-1). You can restrore FS later if you wish by using FS=" "

awk -v file="20161202.csv" 'BEGIN{FS="";split(file,fn,FS);year=fn[1]fn[2]fn[3]fn[4];month=fn[5]fn[6];day=fn[7]fn[8]; \
print year"--"month"--"day}'                 #prints 2016--12--02

awk -v file="20161202.csv" 'BEGIN{split(file,fn,"");year=fn[1]fn[2]fn[3]fn[4]; \
month=fn[5]fn[6];day=fn[7]fn[8];file1=sprintf("%s%s%02d%s",year,month,day+1,".csv");print file1}'    #Prints 20161203.csv = next day file
Replacing print with ;print file1,(getline < file1 < 0)?"not exists":"exists"}'   prints new day filename and if exists or not (!)

awk -F: '{if(system("[ ! -d " $6 " ]") == 0) {print $1 " " $3 " " $7}}' /etc/passwd     # Checks with system call if filename ($6) is directory

awk -v f="20121202" 'BEGIN{match(f,/(....)(..)(..)/,arr);print arr[1],arr[2],arr[3],arr[4]}'

awk -v file1="20161201.csv" 'function sepname(file) {match(file,/(....)(..)(..)/,arr);print arr[1];return (arr[2])}BEGIN{print file1,sepname(file1),arr[3]}'
2016
20161201.csv 12 01

awk -v file1="20161201.csv" 'function incfile(file,days) {match(file,/(....)(..)(..)/,fn);newfile=sprintf("%s%s%02d%s",fn[1],fn[2],fn[3]+days,".csv");return (newfile)};BEGIN{print incfile(file1,3)}'
20161204.csv

------------------------------------------------------------------------------------------------------------------------

AWK:CASE USAGE AND SWITCH (GNU AWK ONLY)
http://stackoverflow.com/questions/43779638/bash-script-how-to-loop-through-rows-in-a-csv-file/43780998#43780503
https://www.gnu.org/software/gawk/manual/html_node/Switch-Statement.html

This shit compares numbers ($1) of current line with number $1 of previous line. 
If current < previous then devide them. Result is something like 0.966, etc.
After dividing, round the number and according to rounding print the correspoinding message.
The only problem is that printf rounding will round 0.99999 to 1.0 and not to 0.9 as requested.
I solved this situation in my answer at SO using substr($1/p,1,3) ==> returns first three chars of 0.99999 = 0.9

PS: In order to use gnu awk case, you need to first use switch(criteria term).

awk '
$1<p { 
    s=sprintf("%.1f",$1/p)
    switch(s) {
    case "0.9":            # if comparing to values ranged [0.9-1.0[ use /0.9/
        print "A"          # ... in which case (no pun) you don't need sprintf
    case "0.8":
        print "B"
    case "0.7":
        print "c"
    default:
        print "D"
    }
    exit
}
{ p=$1 }' file

This is the if-then-else alternative for non-gnu awk (i.e macosx) that do not support switch and case.
awk '$1<p { 
    s=sprintf("%.1f",$1/p)
    if(s==0.9) 
        print "A"
    else if(s==0.8) 
        print "B"
    else if(s==0.7) 
        print "c"
    else
        print "D"
    exit
}
{ p=$1 }' file

------------------------------------------------------------------------------------------------------------------------
AWK:USE EXTERNAL SHELL / SCRIPT COMMAND INSIDE AWK
.B GAWK Documentation: https://www.gnu.org/software/gawk/manual/gawk.html#Getline_002fVariable_002fPipe
.I https://unix.stackexchange.com/questions/72935/using-bash-shell-function-inside-awk
.I https://unix.stackexchange.com/questions/399135/converting-epoch-time-from-multiple-lines-in-a-log-file
provided by αғsнιη
Tags: #external, #system, #getline, #logfile, #date 

.B Example
echo -e "1648589451\n1245589441" |awk '{cmd="date -d@" $1;cmd |getline m;print $1,"-->",m}'

- cmd="date -d@" $1 ==> uses the input first field $1 to built the "cmd" (using the space as concatenation method of awk way to join strings)
- cmd |getline m    ==> first cmd runs as a shell command (acc to gawk manual) and the output then it is sended (pipe) to getline.
getline reads the data received by "cmd" output (one record per time) and saves those data to variable m
- print $1,"-->",m  ==> prints first field of input record and the value of var m (output of cmd command)
Result:
1648589451 --> Wed 30 Mar 2022 12:30:51 AM EEST
1245589441 --> Sun 21 Jun 2009 04:04:01 PM EEST

.B A more idiomatic way to use this method to "transform" dates stored in a log file (first field)
tail -20 /var/log/squid/access.log |awk '{cmd="date -d@"$1;cmd |getline $1; close(cmd)}1'
- cmd="date -d@"$1;   => built the shell command 'date' using $1=first field of input file as an operator to -d switch of 'date'
- cmd |getline $1;    => run the shell command "cmd" , send (pipe) cmd results to getline and finally getline stores the cmd results BACK to $1 (magnificent)
- close(cmd)}         => close the shell
- 1'                  => just print the whole record ($0)of input file with $1 modified by getline!

.B Alternative way 1
tail -2 /var/log/squid/access.log |awk '{$1=system("date -d@" $1)}1'
This also works ok , but system returns a new line at the end of the results that is messing up the output.

.B Altrnative way 2
tail -2 /var/log/squid/access.log |awk '{$1=strftime("%c",$1)}1'
This also works great.

------------------------------------------------------------------------------------------------------------------------
AWK:INPUT METHODS INSTEAD OF FILE (awk '{}' file) AND INSTEAD OF PIPE (command | awk)
Actually this is bash related and not awk related. 
Different shells may have different ways to inject data into awk.

awk '{...}' <<<"one two three"
awk '{....}' <(command) # i.e <(seq 100)

.B Working Examples (2022) with bash & gawk:
awk '{print $1}' <(echo -e "oneA oneB\ntwoA twoB")

awk '{print $1}' <<<"one
two"

awk '{print $2}' <(cat <<EOF
oneA oneB
twoA twoB
EOF
)

awk '{print $2}' <<EOF
oneA oneB
twoA twoB
EOF
