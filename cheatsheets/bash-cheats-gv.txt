BASH:CHEATS BY GV

BASH:BASICS
http://mywiki.wooledge.org/BashFAQ
http://mywiki.wooledge.org/BashSheet

http://tiswww.case.edu/php/chet/bash/bashref.html#SEC31 - Search for "replace"
BASH CHEAT SHEET : https://github.com/pkrumins/bash-redirections-cheat-sheet/blob/master/bash-redirections-cheat-sheet.pdf
BASH HACKERS EXAMPLES / PARAMETER EXPANSION , ETC: http://wiki.bash-hackers.org/syntax/pe
ADVANCED BASH SCRIPTING : ftp://ftp.monash.edu.au/pub/linux/docs/LDP/abs/html/abs-guide.html#PIPEREF
IO REDIRECTION: http://tldp.org/LDP/abs/html/io-redirection.html
https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html
http://ss64.com/bash/expr.html
https://debian-administration.org/article/150/Easily_renaming_multiple_files
http://www.ccs.neu.edu/research/gpc/MSim/vona/terminal/VT100_Escape_Codes.html
Also very good info available at local 'man bash' and particularly the section SHELL BUILTIN COMMANDS (find it with / in man page view)

Check man bash for :
mapfile [-d delim] [-n count] [-O origin] [-s count] [-t] [-u fd] [-C callback] [-c quantum] [array]
read [-ers] [-a aname] [-d delim] [-i text] [-n nchars] [-N nchars] [-p prompt] [-t timeout] [-u fd] [name ...]
printf
printf '%s: %s\n' "$user" "$shell"
------------------------------------------------------------------------------------------------------------------------
BASH:CHEAT SHEET TXT 2 MANPAGE FORMATTING
##Read this cheat file with man pages:
http://technicalprose.blogspot.gr/2011/06/how-to-write-unix-man-page.html
groff programming: http://web.cecs.pdx.edu/~trent/gnu/groff/groff.html#IDX123
man pages making: https://liw.fi/manpages/
https://linux.die.net/man/1/help2man
http://man7.org/linux/man-pages/man7/man-pages.7.html
man formatting: man 7 man & man 7 man-pages

##Working Command:
man --nj <(h=".TH man 1 2017 1.0 cheats page";sed "1i $h" cheatsheets/utils*gv.txt |sed 's/^UTILS:/.SH UTILS:/g; s/^$/\.LP/g; s/^##/\.SS /g; s/\\/\\e/g;G' |sed 's/^$/\.br/g')
You can also combine with --nh 
PS: man options --nj = not auto justified , --nh = not auto break words with hyphen on line changes.

Or in a function (i.e sticked at bash_aliases)
.B function mancheat { 
.B man --nj --nh <(h=".TH man 1 2017 1.0 $1-cheats";sed "s/^${1^^}:/.SH ${1^^}:/g; s/^$/\.LP/g; s/^##/\.SS /g;G" cheatsheets/${1,,}*gv.txt |sed 's/^$/\.br/g; s/\\/\\e/g;' |sed "1i $h");
.B }

##man and groff/troff require special handling.
man ignores normal line feeds at end of lines ($); empty lines (^$) are recognized and displayed
Line feeds in man pages can be done by inserting .br between two lines.
More .br in series of lines are ignored by man and got intepreted as a single line feed - not multiple new lines.
Man pages should start with a .TH line
Man sections / header start with .SH. 
Subsection start with .SS. Alternativelly you can use .B to make this line bold. .B follows text identation - .SS has it's own idents.
The backslash \ works as escape in groff, so you need to escape the backslash with \e (or \\ can also work)
The example tr -d '\n' will become tr -d '\en' with \e escaping, or will become tr -d '\\n' with \\ escaping.

To play with man formatting and see immediate results on screen try:
man <(echo -e ".TH man 1 2017 1.0 testing \n\n.SH Test\n\n\n\n.BR\n\.LP\n\.ce\nbefore end\n.BR \n\The end")

For sed explanation see the sed cheatsheet.

------------------------------------------------------------------------------------------------------------------------
BASH:LOCALE
http://unix.stackexchange.com/questions/87745/what-does-lc-all-c-do/87763#87763
For various operations you can temporary apply a C locale which speeds things up in case of grep, etc.
To see the results of various locales use this test:
$ LC_ALL=C sort <<< $'a\nb\nA\nB'
You can try with en_US, with UTF, with POSIX , etc
See the difference:

$ LC_ALL=en_US sort <<< $'a\nb\nA\nB'  --> a A b B   #printed in separated lines - concatanated here for space saving
$ LC_ALL=C sort <<< $'a\nb\nA\nB'      --> A B a b   #printed in separated lines - concatanated here for space saving
------------------------------------------------------------------------------------------------------------------------
BASH:TERMINAL COLORS
https://github.com/aureliojargas/txt2regex/blob/master/txt2regex.sh
Also See the shellcolors.sh file

The main idea is to define some vars like:
#		cN=$(echo -ne "\033[m")      # normal - also 30m works ok.
#		cP=$(echo -ne "\033[31m") #red color
you can then print in color like echo "$cP This is a red text $cN"
or even trickier you can define your vars like $red and $normal and work it like this:
red=$(echo -ne "\033[31m"); normal=$(echo -ne "\033[m");echo $red hi there $normal --> works ok , prints hi there in red color and returns to normal. If you ommit the normal , terminal remains in red!
Or even better do them functions ! function red { echo -ne "\033[32m";echo $@;echo -ne "\033[m";};red hi there

------------------------------------------------------------------------------------------------------------------------
BASH:READ - BASICS
##Reading Files (mapfile, readarray, read -r , etc)
http://unix.stackexchange.com/questions/339992/how-to-read-different-lines-of-a-file-to-different-variables/339996#339996
http://wiki.bash-hackers.org/commands/builtin/read
http://wiki.bash-hackers.org/commands/builtin/mapfile
http://unix.stackexchange.com/questions/209123/understand-ifs-read-r-line

Check man bash for :
readarray /  mapfile [-d delim] [-n count] [-O origin] [-s count] [-t] [-u fd] [-C callback] [-c quantum] [array] #man bash line 2981
read [-ers] [-a aname] [-d delim] [-i text] [-n nchars] [-N nchars] [-p prompt] [-t timeout] [-u fd] [name ...] #man bash line 3056

Important: When you need to read a file with while read -r and separate it according to a custom separator (i.e :) adjust accordingly the IFS=: and not -d: option of read.

------------------------------------------------------------------------------------------------------------------------
##READ FILE TO ARRAY
http://stackoverflow.com/questions/11393817/bash-read-lines-in-file-into-an-array/11395181#11395181
mapfile -t -O1 var <input.txt --> each line of file goes into array var, withou loop. You just need to refer to line1 as var[1]
mapfile -t -O1 var < <(sed/grep/awk/cat/etc 'expression' file) to feed mapfile with the results of a command
PS1: mapfile has also an alias called readarray
PS2: readarray can also be used like <<<"$var"

This also works: IFS=$'\n' read -d '' -r -a lines < file

------------------------------------------------------------------------------------------------------------------------
##READ LINES FROM FILE NULL SEPARATED (and not new lines)
http://stackoverflow.com/questions/8677546/bash-for-in-looping-on-null-delimited-string-variable
http://stackoverflow.com/questions/42381149/bash-function-with-an-array-input-and-output

Example of NULL Separated file : /proc/1/environ to variables script
You can view this file with cat /proc/1/environ |tr '\0' '\n' #all null will become new lines


You can convert any file to null separated (for testing) by cat file |tr '\n' '\0'
Or for testing you can feed read with find . -type f -print0

To split the files based on null you need something like this:
$ while IFS= read -r -d '' line ; do vars+=("$line");done <file #Mind the gap in -d option of read.
declare -p vars

Also , with readarray - without loop (bash 4.4 and above since readarray before bash 4.4 does not support -d option): 
IFS= readarray -t -d '' vars2 <file3;declare -p vars2
PS: I tried to apply as a delimiter -d$'\0' or -d '\x00' but not working. Has to be -d '' - maybe -d $'\0' could also work (spaced)

This also works for find files : readarray -t -d '' var2 < <(find . -print0)

READ with null values does not work without loop- it gets only the first element: 
IFS= read -d '' -r -a lines < <(find . -name '*.sh' -print0) #I also tried with IFS=$'\0', IFS='', IFS=$'' , no IFS , but no luck

Workaround: IFS=$'\n' read -d '' -r -a arr1 < <(tr '\0' '\n' </proc/1/environ); declare -p arr1
[0]="SHLVL=1" [1]="HOME=/" [2]="init=/sbin/init" [3]="TERM=linux" [4]="drop_caps=" 
[5]="BOOT_IMAGE=/boot/vmlinuz-4.9.0-1-amd64" [6]="PATH=/sbin:/usr/sbin:/bin:/usr/bin" [7]="PWD=/" [8]="rootmnt=/root")
Workaround will break if values include newlines = useless use of -print0 

To correctly READ null values separated data need to be combined with a loop to work:
while IFS= read -r -d '' file;do
echo "file found=$file" #this can be whatever , i.e an array assignment filearray+=("$file")
done < <(find . -print0)

For files this also works ok:
find . -print0 |while IFS= read -r -d '' file;do echo "file found=$file";done

## PRINTING A NULL SEPARATED VARIABLE
For a variable, according to http://stackoverflow.com/questions/8677546/bash-for-in-looping-on-null-delimited-string-variable
while IFS= read -r -d '' line ; do
    echo "line=$line"
done <<<"$var"
In realiry this method works for null separated files (done<file) but never worked with a VAR
Test it with var=$(find . -print0). It did not work even with $(printf '%s\0' "$var")

The only way for this to work with a variable is to transform the variable to a format that will preserve the null (base64) , since bash removes the null byte from vars.

You need to do something like var=$(find . -print0 |base64) and on the "done" part of the while you need to apply 
done < <(echo "$var" |base64 -d) - See the ARGS section for details

------------------------------------------------------------------------------------------------------------------------
## WHILE READ METHODS 
while read -r var1 var2 ;do ....;done<file.txt :will assign var1 to first field of line 1 and var2 to second field.
If the file has more than two fields, then var1 will get the first text and var2 will get all the rest text (all of them)

Withoud defining IFS, the default IFS is used. Even unsetting the IFS default values will be used.
This is why sometimes is preferable to write IFS= (equals to IFS=null) instead of unset IFS
 
You can define IFS=' ' for space delimiter, or ':' for semicol delimiter or IFS=$'\n' for new line, etc
To read whole lines use just IFS= (empty = whole line is returned) - But in this case the mapfile tool is better.

Skip Lines Trick inside loop : [[ $line = \#* ]] && continue #skips lines starting with # = comments

------------------------------------------------------------------------------------------------------------------------
## READ LINES AND SPLIT TO SPACE SEPARATED FIELDS
you can combine also multi read (line read / field read) like this:
while IFS= read -r line;do IFS=' ' read -r -a v1 <<<"$line";done<c.txt
OR 
while IFS= read -r line;do readarray -d' ' -t v1 <<<"$line";done<c.txt

It will assign filelines to line and then with different IFS will split line to space separated fields.

Can be also done with ONE loop like this:
while read -r -a vars;do #whatever;done<file #you might also use IFS=' ' , but even with default IFS works fine.
For a line of "arg1_1 arg2_1 arg3_1" , vars array will look like this:
declare -a vars=([0]="arg1_1" [1]="arg2_1" [2]="arg3_1")

Mind that vars array will be redifined/overwritten in the next line of the file. 

To have an array that fiels from all the lines will be appending serially to this array instead of overwritting at every new line:
while IFS= read -r lines;do readarray -t -d' ' -O"${#vars[@]}" vars <<<"$lines";done<c.txt;declare -p vars
Or you can use the read -r -a method and transfer fields to another array that will not be overwritten by the next line.

------------------------------------------------------------------------------------------------------------------------
## READ A SINGLE STRING/VARIABLE WITH read -r:
http://mywiki.wooledge.org/BashFAQ/001
read -r first last junk <<< 'Bob Smith 123 Main Street Elk Grove Iowa 123-555-6789'
read -r first last junk <<< "$a" 
# $first will contain "Bob", last will contain "Smith", junk holds everything else that follows
------------------------------------------------------------------------------------------------------------------------

## READ FROM FILE AND SPLIT FIELD BY DOUBLE QUOTES 
Line Example : "aaa bbb" "xxx yyy"  # Next line can be of the same pattern plus a new line separator at end of prev line
Target : separate fields of line like $1="aaa bbb" $2="xxx yyy" in order to be sent to a function or command

This is tricky because delimiter is not new line but neither space.
Using newline as delim will get the whole line : $1="\"aaa bbb\" \"xxx yyy\""
Using space as delim will get $1="\"aaa" $2="bbb\"" etc

This guy here builds an array to take advantage of the existed double quotes:
http://stackoverflow.com/questions/42111441/how-to-pass-command-line-arguments-with-spaces-through-a-variable-in-bash#42111441
while IFS= read -r line;do
  eval args=\("$line"\) 			#or eval args+=\("$line"\)  or declare -a args+=\(....\)
  command_or_function "${args[@]}" 	#or outside the loop if args+=
done<file.txt

By calling function inside while loop it results that function will be called as many times as file lines
Every time function is called will have args starting from [0] up to number of fields i.e [1]
In reality the command eval args=\("$line"\) (or declare -a args=), expands to (as seen using set -x)
++ args=("aaa bbb" "xxx yyy")
Which is two different array entries! Genious. PS: Brackets are quoted to avoid being handled as sub shell by bash
On the next loop the array is redifined (due to absence of +=) and array elements get the fields of the next line.
Using += fields of next line will be added to the existed array

Examples:
$ cat e.txt
"aaa bbb" "xxx yyy"
"some more" "data here"

$ unset args;while IFS= read -r line;do declare -a args=\("$line"\);declare -p args;done<e.txt
declare -a args=([0]="aaa bbb" [1]="xxx yyy")
declare -a args=([0]="some more" [1]="data here")

$ unset args;while IFS= read -r line;do declare -a args+=\("$line"\);done<e.txt;declare -p args
declare -a args=([0]="aaa bbb" [1]="xxx yyy" [2]="some more" [3]="data here")

$ function test { for a in "$@";do echo "$a";done;};test "${args[@]}"
#or function test { while [[ "$#" -gt 0 ]];do echo "$1";shift;done;};test "${args[@]}"

aaa bbb
xxx yyy
some more
data here

More Tips:
To call an external command (not a function) it seems that `xargs command <file` is enough (??strange!!)

In case you want to parse whole lines from file as an argument to a function you can simply use : /some/command "$(<file.txt)"
------------------------------------------------------------------------------------------------------------------------
##READ IN PAIR OF TWO LINES
while read -r prereq && read -r target; do
   printf '%s: %s\n' "$target" "$prereq"
done < file.txt
PS: Mind that two lines are read as a pair. Line 1 and Line 2 , and then Line 3-Line4.
You miss the Line 2- Line3 combination

Can be also used for filenames:
while read -r f1 && read -r f2;do echo "f1=$f1, f2=$f2";done < <(find . -name 'file*')
f1=./file1, f2=./file2
f1=./file3, f2=./file4
------------------------------------------------------------------------------------------------------------------------

##READ FIRST TWO LINES ONLY 
{ IFS= read -r line1 && IFS= read -r line2; } < input.txt
{ line1=$(line) && line2=$(line); } < input.txt

#consider that alpha produces "a b c\n" and "d" in separate line. 
#calling beta $(alpha) each char was an arg (not desirable)
#calling beta "$(alpha) arg1 ($1) was getting "a b c\nd" and arg2 was empty (not desirable)

IFS=$'\n' read -d '' a b < <(echo $'a b c\nd') #mind that -d should be set to null in order IFS to work correctly (Anubhava)
declare -p a b --> prints "declare -- a="a b c"" & "declare -- b="d"" 

All time classic : readarray -t args < <(alpha) && beta "${args[@]}" #will by default sepearate args by \n

Best trick :   alpha | xargs -d "\n" beta 

------------------------------------------------------------------------------------------------------------------------
##READ WITH LINE BUILTIN
$ line <file will print the first line of file
can also be assigned to a var: a=$(line <file)

------------------------------------------------------------------------------------------------------------------------
##READ:LOOP WITH FILE DESCRIPTOR
exec 9< "$file"
while IFS= read -r line <&9; do
  cat > ignoredfile
  printf '%s\n' "$line"
done
exec 9<&-
------------------------------------------------------------------------------------------------------------------------
##READ - SPLIT STRING TO CHARS
$ while read -rn1 char;do printf '%s\n' "$char";done <<<"1.Nf3 c5 2.e4 Nc6"

Trick:
$ while read -rn1 char;do sumA+=$(printf '%d+' "'$char'");done <<<"1.Nf3 c5 2.e4 Nc6"
echo "$sumA"  ---> 49+46+78+102+51+39+99+53+39+50+46+101+52+39+78+99+54+39+
------------------------------------------------------------------------------------------------------------------------
##MORE READ EXAMPLES (DELIMITERS, WORD SPLIT, ARRAYS)
Read supports -a option for array. 
read builtin default separator is new line. If read founds a new line then exits.
Word Splitting can be done by adjusting IFS

Example:
IFS="[;]" read -a array <<<$'one;two;three;no more'; declare -p array --> [0]="one" [1]="two" [2]="three" [3]="no more"
This works in bash 4.4 and also 4.3 so probably in all versions

If the input contains new line then read is terminated on this new line due to internal default delimiter 
IFS="[;]" read -a array <<<$'one;two;\nthree;no more'; declare -p array  --> [0]="one" [1]="two"

For array declaration among multiple lines , you can either apply one of the "read two lines" techniques, or a while loop 
OR you can make this trick and avoid the use of while loop with Bash 4.4 and above:
$ echo "$a"
one;two;
three;no more;
$ IFS=[$'\n'";"] read -d '' -a arr1 <<<"$a"; declare -p arr1   #Luckilly enough IFS accepts regex [newline or ;] in bash 4.4
declare -a arr1=([0]="one" [1]="two" [2]="three" [3]="no more")
This method works even in bash 4.3
I had the feeling that does not work because command declare was joined with && and not ; - ALWAYS USE ;

In Bash 4.4 and 4.3 you can also force splitting only by new lines:
$ IFS=$'\n' read -d '' -a arr1 <<<"$a"; declare -p arr1   ---> [0]="one;two;" [1]="three;no more;"

And you can expand this newlines splitting to array technique in files without the need to use a while loop
$ cat file11
Ubuntu, 120, 143, 154
Yggdrasil, 144, 170-171
Yood, Charles, 6
We Were Young, Bob, 178-179
Zawinski, Jamie, 204

$ IFS=$'\n' read -d '' -a arr1 <file11; declare -p arr1   
#Above Works even in bash 4.3! Strange to see that - i was expecting to break in -d '' option of read
#Without -d '' onyle the first line of the file is read
[0]="Ubuntu, 120, 143, 154" 
[1]="Yggdrasil, 144, 170-171" 
[2]="Yood, Charles, 6" 
[3]="We Were Young, Bob, 178-179"       #mind that space in line was succesfully handled
[4]="Zawinski, Jamie, 204"

Also this workd fine in bash 4.4 and 4.3 : f=$(cat file11); IFS=$'\n' read -d '' -a arr1 <<<"$f"; declare -p arr1 #same as above

The success of above read operations is based on setting -d '' = set internal read delimiter to null/nothing. 
Thus splitting is made purely based on IFS values

Trick: By disabling read -d and also by diasbling IFS you can get the whole file in one array position = one line
IFS= read -d '' -a arr1 <file11; declare -p arr1 
declare -a arr1=([0]=$'Ubuntu, 120, 143, 154\nYggdrasil, 144, 170-171\nYood, Charles, 6\nWe Were Young, Bob, 178-179\nZawinski, Jamie, 204\n')

You can also make use of this technique to read null separated values without while read loop:


About word splitting, setting temporary value to IFS affects the word splitting significantly:
IFS= read -a array <<<$'one;two;three;no more'; declare -p array --> [0]="one;two;three;no more"   #No split at space
read -a array <<<$'one;two;three;no more'; declare -p array --> [0]="one;two;three;no" [1]="more"  #Split at space


##Trick with IFS and values in one line - one variable:

Instead of creating an array with values and manipulate array items like:
IFS="[, ]" read -r -a list <<<"Yeseva+One, Yrsa"  #Convert values to array
for k in "${list[@]}";do  #actions ;done

You can  do:
variable="Yeseva+One, Yrsa"
IFS="[, ]" for i in $variable;do #actions;done

##READ - SPLIT STRINGS WITH WHILE INSTEAD OF ARRAY
When you don't want to use array but you want to loop over separated values of a string with while , the use of -r can be misleading

a=$'a|b|c\nd|e|f'
b="a|b|c"
while IFS="[|]" read -r var;do echo "$var";done <<<"a|b|c"    ---> Prints a|b|c - no splitting
while read -d '|' -r var;do echo "$var";done <<<"a|b|c"       ---> Prints a and b in separate line - c is missing
while read -d '|' -r var;do echo "$var";done <<<"$b|" ---> Prints corrctly a-b-c in different lines. mind the '|' after <<<$b
while read -d '|' -r var;do echo "$var";done <<<"$a|" --> prints correctly a-b-c-d-e-f in sep lines. Might require IFS=$'\n' if IFS is not correctly set

while IFS=[$'\n'"|"] read var;do echo "$var";done <<<"$a|" ---> This prints a|b|c in one line and d|e|f| in second line. Opposite to the usage of read -a , the IFS has no effect here.

##CONFLICT
while IFS=[$'\n'"|"] read -d '' var;do echo "$var";done <<<"$a|"   --> Prints nothing!
while IFS=[$'\n'] read -d '|' var;do echo "$var";done <<<"${a}|"   --> Works Correctly!

Even when we use arrays with read -a , the first method also works fine:
IFS=[$'\n'"|"] read -d '' -a var <<<"$a"; declare -p var --> declare -a var='([0]="a" [1]="b" [2]="c" [3]="d" [4]="e" [5]="f")'

PS: Sometimes the use of -r yelds only to the first value [0]="a" and rest values were ignored.

------------------------------------------------------------------------------------------------------------------------


BASH:PIPES
Pipes are usefull to send data from one command to the other (i.e cat a.txt |less)
What needs to be said is that pipes create a subshell.
Thus multiple pipes create multiple subshells = performance penalty.
More over variables of parent can not be modified by childs (=subshells)

Example:
s=1000; ps -ly |while read a b c d rest;do s=$(($s+c));echo "c=$c - S=$s";done;echo "final s=$s"
s values will be sent to pipe (child), and value of c will be added. Outside the pipe, final s will be reported 1000, because child can not change parents.
c=PID - S=1000
c=1578 - S=2578
c=1614 - S=4192
c=1998 - S=6190
c=1999 - S=8189
final s=1000

Workaround to avoid pipe / subshell:
s=1000; while read a b c d rest;do s=$(($s+c));echo "c=$c - S=$s";done < <(ps -ly);echo "final s=$s"
In this case (process substitution) the final s will have the correct/final value.
------------------------------------------------------------------------------------------------------------------------
BASH:PIPES Exit Code Checking (http://mywiki.wooledge.org/BashFAQ/002)
Check Exit code of commands inside pipe: ${PIPESTATUS[@]}
"${PIPESTATUS[0]}" refers to first (read from left) command
"${PIPESTATUS[1]}" refers to second (read from left) command, etc
"${PIPESTATUS[@]}" prints them all 

Bash 3.0 added a pipefail option as well, which can be used if you simply want to take action upon failure of the grep: 
set -o pipefail
if ! grep foo somelogfile | head -5; then
    printf "uh oh\n"
fi
------------------------------------------------------------------------------------------------------------------------
BASH:RUN COMMAND AS DIFFERENT USER : 
gksu -u gv command. Usefull if you are in root terminal and want to execute i.e google-chrome-stable
Run Google Chrome as Root : http://unix.stackexchange.com/questions/175967/how-to-run-google-chrome-as-root-in-linux/332128#332128
Easy trick : su or gksu -u gv google-chrome-stable - works fine either by root terminal or by root login

Notice the difference:
# su - gv -c "echo $(whoami)" --> root
# su - gv -c 'echo $(whoami)' --> gv

Double quotes are expanded first 
------------------------------------------------------------------------------------------------------------------------
BASH:CONDITION CHECKS 
Space is required around braces AND around operators : [ "$a" != "$b" ]  and not [ "$a"!="$b" ] 
See also man bash and man test
One line if check : This is based to the operation of && which executes the next command only if previous command exit with 0 = succesfull exit = pseudocode as TRUE (if it maybe the only time that something with value zero is translated to true!)
For else conditions or for performing actions under false conditions you can use ! operator (not) in front of expression which will reverse exit code.
[ "$USER" = "root" ] && echo "hello root" -> hello root # displays nothing if user is not root
[ "$USER" = "root" ]  || [ "$LOGNAME" = "root" ] && echo "hello root" --> hello root #
[ "$USER" = "root" ]  || [ "$LOGNAME" = "root" ];echo $? -> 0 #zero = all ok = true
[ "$USER" = "root" ]  || [ "$LOGNAME" = "rot" ];echo $? -> 0 #zero = all ok = true due to the OR operator || (for and you should use &&)
[ "$USER" = "rot" ]  || [ "$LOGNAME" = "rot" ];echo $? --> 1 #one = not ok = false since i'm logged in as root and not rot
! [ "$USER" = "rot" ]  || ! [ "$LOGNAME" = "rot" ];echo $? --> 0  #expression ok = true means that it is true that i'm not user rot or logname rot (true since i'm logged in as root)
Tip: When you gksu terminal from normal user account then $USER and $LOGNAME are set to root.
[[ -z $1 ]] && echo "Pass me a directory to cat files" && return #if is not set , if it is blank, empty
[[ ! -z $1 ]]  equals to [[ -n $1 ]] # -n operator makes the opposite job of -z. Returns true / success / exit code 0 if it is set 
[[ ! -d $1 ]] && echo "Argument passed is not a directory - please send a dir" && return

[[ ! $diskstatus =~ ^(ready|online)$ ]] && echo "Not OK: $diskstatus" # Regex match of two values.

$ echo "/home/gv/Desktop" |sed 's![^/]$!&/!' --> /home/gv/Desktop/ #Check if a slash '/' is present in the end- add it if missing
$ d="/home/gv/Desktop"; [[ "${d: -1}" != "/" ]] && d="${d}/" #bash alternative: if last char is not a dash, add a dash

##CONDITION CHECK on command output , based on the return code of command without the need to compare $?.
if command; then
    printf "it succeeded\n" #executed when command returns 0
else
    printf "it failed\n" #executed when command returns <>0 (i.e 1 or other code - carefull with diff)
fi

.I Real Example:
if grep -F "A=" b.txt ;then echo "found";else echo "Not Found";fi
parameterA=0 #Output of grep
found #Output of if

To suppress the grep output use grep -q or use >/dev/null

Mind that IS NOT necessary to enclose the grep in $(...) - it works directly.
Although this will also work if [[  $(grep "pattern" file) ]];then dothis;else dotheother;fi

##CONDITION CHECK -  CASE WITH NUMBERS RANGE
Using case with numbers:

This case makes human logic but not computer logic, since case compares $num to regex pattern
case $num in
  [0-6] )               echo "You're close...but too low" ;;
  [8-14] )              echo "You're close...but too high" ;;
  [15-100] )            echo "You're nowhere near my favorite number...sorry, try again" ;;
  7 )                   echo "YOU GUESSED MY FAVORITE NUMBER!" ;;
  * )                   echo "You didn't pick a number between 1 and 100!" ;;
esac

Can be done with classic if-elif-else combined with -gt , -lt, -eq, etc oprators.
Alternativelly we can use bash with regex that will match the desired combinations like this:

case $num in
  ([0-6])                 echo "You're close...but too low" ;;
  ([8-9]|1[0-4])          echo "You're close...but too high" ;;
  (1[5-9]|[2-9][0-9]|100) echo "You're nowhere near my favorite number...sorry, try again" ;;
  7 )                     echo "YOU GUESSED MY FAVORITE NUMBER!" ;;
  * )                     echo "You didn't pick a number between 1 and 100!" ;;
esac

Pattern []0-6] is a valid regex pattern representing chars from 0 up to 6.
Pattern [8-9]|1[0-4] will epxand to chars 8-9 OR 10-11-12-13-14 (as chars)
Pattern (1[5-9]|[2-9][0-9]|100) will expand to 15-19 OR 20-99 OR 100 = range 15-100
------------------------------------------------------------------------------------------------------------------------
BASH:FIND 
Read here why you never parse output of ls: http://mywiki.wooledge.org/BashGuide/Practices#Quoting (chap 5)
POSIX Find Manual :http://pubs.opengroup.org/onlinepubs/9699919799/utilities/find.html
##Use find as ls alternative
find /home/gv -maxdepth 1 -type d -> list only directories
find /home/gv -maxdepth 1 -type f -> lists only files
find /home/gv -maxdepth 1 -> lists both
output of find can be piped to wc -l , xargs, and other commands.

Some progs (like whatis) can not accept pipes directly ; In this case you have to use xargs (pipe to xargs which will call the prog)
PS: By the way, you don't need find to call whatis. You can call whatis directly with wildmark : whatis /usr/bin/*

##Read File Names as a pair
while read -r f1 && read -r f2;do echo "f1=$f1, f2=$f2";done < <(find . -name 'file*')
f1=./file1, f2=./file2
f1=./file3, f2=./file4

##UNUSUAL FILE NAMES 
Consider files with spaces in their name (i.e a a (01).txx)

In order you want to get the basename of this file the following command fails:
for file in "$(find . -name "*.txx")";do basename "$file";done
But if you just type find . -name "*.txx" files will be listed correctly.
Also this command works ok: for file in "$(find . -name "*.txx")";do echo "$file";done
Mind the double quotes outside $(find...)

To correctly handle file names with spaces you need to combine find with  while read.
This works ok: find . -name "*.txx" |while read -r line;do basename "$line";done

Also this works ok , and it is more simple to use: for file in *.txx;do basename "$file";done

The problem here is that this method doesnt go inside subdirs, while the find method does.
http://stackoverflow.com/questions/4638874/how-to-loop-through-a-directory-recursively-to-find-files-with-certain-extension
http://www.commandlinefu.com/commands/view/14209/repeat-any-string-or-char-n-times-without-spaces-between
http://wiki.bash-hackers.org/syntax/expansion/brace
http://stackoverflow.com/questions/2372719/using-sed-to-mass-rename-files
linux   /boot/vmlinuz-4.0.0-1-amd64 root=UUID=5e285652 ro  quiet text

##FIND OPERATORS (AND , OR,NOT)
exclude files (i.e manifest.txt
find /path/to/ -name '*.txt' -and -not -name 'manifest.txt'

Usage: Multiple file names / types:
find /tmp -name '*.pdf' -or -name '*.doc' | xargs rm #mind the -or operator. 
Tip: We can avoid the use of find by enabling bash extended glob options (see shopt starglob and extglob chapter inhere) 

##FIND CUSTOM PRINTING
find /dir1 -type f -printf "%f\n" #prints only file name, without directory in front.
find . -printf "depth="%d/"sym perm="%M/"perm="%m/"size="%s/"user="%u/"group="%g/"name="%p/"type="%Y\\n

##FIND -EXEC
find can accept an -exec option in two formats:
-exec command {} \; #In this method command is invocated in each result of find
-exec command {} +  #In this method command is invocated after buffering an amount of find results.

.B Multiple exec options
find dirname ... -exec somecommand {} \; -exec someothercommand {} \; #by wildcard

.B Example with grep
[: http://unix.stackexchange.com/questions/85789/grep-in-couple-thousands-files :]
find . -type f -exec grep PATTERN {} +   # packs as many files as it can per command invocation, 
find . -type f -exec grep PATTERN {} \;  #runs grep in every find result = much slower

find / -type f -print0 | xargs -r0 -P2 grep -Hi 'the brown dog'

.B Example: Rename extensionless files
Mind the handling of the find results with {} - becomes $0 if you call bash
find . -type f  ! -name "*.*" -exec mv -v {} {}.txt \;
find . -type f ! -name "*.*" -exec bash -c 'mv "$0" "$0".mp4' {} \;


##Loop in files with find using null as file names separator (the correct way to loop)
By Stephane Chazelas : http://unix.stackexchange.com/questions/321697/why-is-looping-over-finds-output-bad-practice

As a general idea, to loop on files the glob method is better.
for f in dir/* ; do something with $f;done

If you need to do it with find, the only accurate approach is to use find -print0 that will separate file names with \0 = null byte
This will ensure that results will be correct even if filenames contain spaces, strange chars , newlines, whatever because on unix file names are allowed to contain everything except / (assigned to paths) and except null chars.

.B Method 1 - To perform actions on each file - null separated
while IFS= read -r -d '' fn;do var+=("$fn");done < <(find . -name '*.sh' -print0)

.B Method 2 - Quick store of filenames in array without loop
readarray -t -d '' var2 < <(find . -name '*.sh' -print0) #-d option only in bash >= 4.4
Unfortunatelly the read -r -d '' -a array < <(find . -print0) does not work directly because read gets one line only.Requires while.

.B Pipe find results to other commands:
find . -print0 | xargs -r0 something with
find . -print0 |while IFS= read -r -d '' file;do echo "file found=$file";done #instead of echo you could built an array or whatever

.B The globbing method to create an array with files
shopt -s nullglob dotglob #to have files with names beginning with a dot and avoid glob expanding to * if no files match
files=(*)  ## or fies=(*.sh), etc

##Combine basename and null separation to process files
$ while read -r -d '' file;do echo "ffmpeg -i $file -q 1 ${file}_%d.jpeg";done < <(find . -name "a*.txt" -printf %f\\0)
printf %f prints only filename without directory (=basename) and \\0 applies null after filename (separator)

## Iterate through various folders simulating find with a for loop in a list:
for i in a b c d e ;do
  (cd $i/; for i in Test_*_hit.txt; do cut -f1,2 $i > ${i%.txt}2.txt; done)
done
------------------------------------------------------------------------------------------------------------------------

BASH:PROCESSES ()TOP - HTOP - PS - KILL)
Search for a process using top . Top seems to catch all processes:
top -p $(echo $(pgrep time) |sed 's/ /,/g')
pgrep search for processes matching pattern even partially. pidof could be used if exact process name is known.
Defaut output of pgrep is to seperate processes found with new lines. By echo \n is removed and a space is used.
If you replaace that space with a comma, then can be fed to top -p which accepts multiple pids (comma seperated)

##PROCESSES LIST - KILL
ps all and ps aux
list all of tty1 : ps -t tty1
Isolate pids: ps -t tty1 |cut -d" " -f1
Remove new line chars: ps -t tty1 |echo $(cut -d" " -f1)
Kill all those processes at once: kill -9 $(ps -t tty1 |echo $(cut -d" " -f1)) # kill requires pids to be seperated by spaces, not new lines.
Best Solution : kill -9 $(echo $(ps -t tty1 --no-headers -o pid))

##EXPORT TOP DATA TO VARIABLE OR TO OTHER PROGRAMM
Top instead of continuous running can forced to run only once providing static results using -bn1 = batch mode , -n1=one iteration
Most of top options can be used with this batch mode , i.e -u gv = reports jobs/tasks for user gv
top -bn1 -u gv |grep 'whatever' or awk or sed or > file , etc

------------------------------------------------------------------------------------------------------------------------
BASH:PARAMETERS EXPANSION
http://wiki.bash-hackers.org/syntax/pe#use_a_default_value

a="this is some TEXT"; echo ${a: -10} 	-> some TEXT
a="this is some TEXT"; echo ${a: 10} 		-> me TEXT
a="this is some TEXT"; echo ${a: 5:7} 	-> is some
a="this is some TEXT"; echo ${a: 1:-1}    -> his is some TEX  #remove first and last char - OR - get from char 1 up to lst char-1

a="/home/gv/Desktop/PythonTests/a<>rte.zip";echo $(basename ${a/<>/_}) 	->a_rte.zip
a="/home/gv/Desktop/PythonTests/a<>rte.zip";echo ${a#/} 					-> removes only the first / 
a="/home/gv/Desktop/PythonTests/azip<>rte.zip";echo ${a%.zip} 			->removes the last .zip (but not middle zip) 
basename c.jpg .jpg -> c
basename c.jpg pg -> c.j #basename can be used as a tricky tool to remove chars from the end of ANY string but requires exact match
a="logfiletxt";basename $a txt -> logfile
a="logfile.txt";echo ${a/.txt} -> logfile #similar to basename but match starts from 1st char to the last. 1st occurence to be removed.
a="logfile.txt";echo ${a/fil} -> loge.txt #remove fil - exact match
a="logfilefilo.txt";echo ${a/lo} -> gfilefilo.txt #only first occurence of exact pattern removed
a="logfilelofi.txt";echo ${a/lo} -> gfilelofi.txt #only first occurence of exact pattern removed
a="logfilelofi.txt";echo ${a//lo} -> gfilefi.txt #all occurences of exact pattern removed.
a="logfilelofi.txt";echo ${a/lf/_} -> logfilelofi.txt #no replacement made since lf is not present in $a (exact match)
a="logfilelofi.txt";echo ${a//[lf]/_} -> _og_i_e_o_i.txt #all occurences of l and f - not exact match due to [] regex synthax
a="logfilelofi.txt";echo ${a/*l/_} -> _ofi.txt #from start up and including last l
a="logfilelofi.txt";echo ${a/*g/_} -> _filelofi.txt #from start up & including last g
a="logfilelofi.txt";echo ${a/*l/} -> ofi.txt #from start up to last l (if no replace string is specified then delete)
a="logfilelofi.txt";echo ${a/#/_} -> _logfilelofi.txt #replace first char with underscore
a="logfilelofi.txt";echo ${a/%/_} -> logfilelofi.txt_ #replace last char with underscore
a="logfilelofi.txt";echo ${a/.txt/_} -> logfilelofi_ #replace .txt with underscore
a="logfilelofi.txt";echo ${a/.txt} -> logfilelofi #delete .txt
MYSTRING=xxxxxxxxxx;echo ${MYSTRING/#x/y}  # RESULT: yxxxxxxxxx # Here the sign # is like an anchor to beginning
MYSTRING=xxxxxxxxxx;echo ${MYSTRING/%x/y}  # RESULT: xxxxxxxxxy # Here symbol % is an anchor to the end of string
a="logfilelofi.mp3";echo ${a/.[a-z0-9A-Z]*/} -> logfilelofi #delete any extension with a dot and any of a-z,0-9 and A-Z range
CLIP=$'http://abc\".x\'y`.com';cleanclip=$(echo ${CLIP//[\'\`\"]});echo $cleanclip ->http://abc.xy.com #mind the special var declaration of CLIP.
for i in *.JPG; do mv "$i" "${i/.JPG}".jpg; done -> finds files with JPG extension and renames them to .jpg
a="/home/gv/Desktop/PythonTests/a?<>rt*eew?.zip";echo $(basename ${a//[\/<>:\\|*\'\"?]/_}) 	-> _home_gv_Desktop_PythonTests_a___rt_eew_.zip
bash manual: ${parameter/pattern/string} . If pattern begins with ‘/’, all matches of pattern are replaced with string. Normally only the first match is replaced. If pattern begins with ‘#’, it must match at the beginning of the expanded value of parameter. If pattern begins with ‘%’, it must match at the end of the expanded value of parameter.

a="somefile.txt";echo ${a%%.txt} -> somefile #delete from end exact match
a="somefile.txt";echo ${a%.txt} -->somefile
a="sometxtfile.txt";echo ${a%txt} -->sometxtfile. #delete from end only exact match. midle txt is not deleted.
a="sometxtfile.txt";echo ${a##txt} --> sometxtfile.txt #no valid -no effect 
a="sometxtfile.txt";echo ${a##some} --> txtfile.txt #delete pattern (xact match) from the beginning
a="sometxtfile.txt";echo ${a#some} --> txtfile.txt
a="sometxtfile.txt";echo ${a#txt} --> sometxtfile.txt #no effect . there is no "txt" in the beginning.
a="Be conservative in what you send";echo ${a#* } --> conservative in what you send ("Be" is deleted. Single # removes the first word from beginning)
a="Be conservative in what you send";echo ${a##* } --> send #All text deleted except "send" Double ## removes all words from beginning except last
a="this.is.a.file.gz";echo ${a##*.} -->gz #all text deleted except last part (DOT separated) or delete from begining until the last dot found
a="apt";echo ${a:0:1} --> a #prints the first character of a variable (from zero give me 1)
a="Be conservative in what you send";echo ${a% *} --> Be conservative in what you #first word from end deleted. 
a="Be conservative in what you.send";echo ${a% *} --> Be conservative in what #works only for space separated words (IFS makes some effect in the resulted text)
a="Be conservative in what you send";echo ${a%% *} --> Be #all words from the end deleted (space separated)

a="some text here";echo ${a@Q} ->'some text here' #printing with single quotes
a="some text here";echo ${a@A} -> a='some text here' #operators available Q-E-P-A-a
a[0]="some text";a[1]="more text";echo ${a[@]} -> some text more text
a[0]="some text";a[1]="more text";echo ${a[@]@A} ->declare -a a=([0]="some text" [1]="more text")
a[0]="some text";a[1]="more text";echo ${a[@]@Q} ->'some text' 'more text'
a[0]="some text";a[1]="more text";a[2]="much more text";echo ${!a[@]} -> 0 1 2 #index of elements . This can be used in for i in ${a![@]} - i will be 0 , 1, 2 
a[0]="some text";a[1]="more text";a[2]="much more text";echo ${#a[@]} -> 3 #Total number of elements
a[0]="some text";a[1]="more text";a[2]="much more text";echo ${a[-1]} -> much more text. Use of -1 in index prints the last array element.
a="This is some Text";echo "${a^^}" --> THIS IS SOME TEXT #All chars converted to uppercase
array=(This is some Text);echo "${array[@]^^}" --> THIS IS SOME TEXT #All chars converted to uppercase
array=(This is some Text);echo "${array[@],}" --> this is some text
array=(This is some Text);echo "${array[@],,}" --> this is some text #all chars in lower case
array=(This is some Text);echo "${array[@]^}" --> This Is Some Text
array=(This is a text);echo "${array[@]%is}" --> Th a text ("is" is deleted from all elements of array : array=([0]="This" [1]="is" [2]="a" [3]="text"))
http://wiki.bash-hackers.org/syntax/pe : "As for most parameter expansion features, working on arrays will handle each expanded element, for individual expansion and also for mass expansion."
array=(This is a text);echo "${array[@]/t/d}" ⇒ This is a dext #first found t replaced with d. Capital T is intact.
array=(This is a text);echo "${array[@]//t/d}" ⇒ This is a dexd #all t replaced with d
array=(This is a text);echo "${array[@]/[tT]/d}" -> dhis is a dext #First found small and first found capital T replaced using regex

a="logfilelofi.mp3";av="anotherfile";echo ${!a@} -> a av #lists all active/stored parameters starting with letter a
echo ${!BASH*} -> BASH BASH_ARGC BASH_ARGV BASH_COMMAND BASH_LINENO BASH_SOURCE BASH_SUBSHELL BASH_VERSINFO BASH_VERSION
#mv path/you/do/not/want/to/type/twice/oldname !#$:h/newname #!$ returns the argument of last command /history
#Similarry to !$ there is alsos !! which prints last commad (full) and last result
path/you/do/not/want/to/type/twice/oldname !#$:h/newname -> path/you/do/not/want/to/type/twice/oldname path/you/do/not/want/to/type/twice/newname

expr 40 - 3 ->37 #expr is available in GNU Bash. 
expr substr "the is a kind of test" 5 10 -> is a kind  
a="the is a kind of test";echo ${a: 5:10} -> s a kind o
export -p -> gives infor about global vars : declare -x USER="root" , declare -x XDG_CURRENT_DESKTOP="XFCE"
IFS=:;a[0]="some text";a[1]="more text";echo "${a[*]}" -> some text:more text #the use of * instead of @ seperates array elements by IFS 

Print / Refer to array elements in a different way using parameters expansion / string manipulation
array=(0 1 2 3 4 5 6 7 8 9 0 a b c d e f g h)
echo ${array[@]:7} -> 7 8 9 0 a b c d e f g h
echo ${array[@]:7:2} -> 7 8
echo ${array[@]: -7:2} -> b c
echo ${array[@]: -7:-2} ->bash: -2: substring expression < 0
echo ${array[@]:0} -> 0 1 2 3 4 5 6 7 8 9 0 a b c d e f g h  #equivalent to echo ${array[@]}
echo ${array[@]:0:2} -> 0 1 #extract part of array / sub-array
echo ${array[@]:2:1} -> 2   #Start from position 0 and print 1 . eqivalent to echo ${array[2]} 
MYARR=(a b c d e f g);echo ${MYARR[@]:2:3}  -->c d e            # Extract a sub-array
MYARR=(a b c d e f g);echo ${MYARR[@]/d/FOO} --> a b c FOO e f g  # Replace elements that match pattern (d) with word FOO)

##Printing arrays with BASH (declare -p)
MYARR=(a b c d e f g);declare -p MYARR  #Print array in the smart way ;-) Works even with associative arrays.
#Output --> declare -a MYARR=([0]="a" [1]="b" [2]="c" [3]="d" [4]="e" [5]="f" [6]="g")

declare -p array |sed 's/declare -a array=(//g' |tr ' ' '\n' |sed 's/)$//g'
if you just declare -p array then output is like this:
a=( 1 2 3);declare -p a --> declare -a a=([0]="1" [1]="2" [2]="3")
So the first sed gets rid of the 'declare -a a=(' part.
tr replaces spaces (between array elements) with new line
last sed deletes the last ) in the array
result : 
root@debi64:/home/gv/Desktop/PythonTests# a=( 1 2 3);declare -p a |sed 's/declare -a a=(//g' |tr ' ' '\n' |sed 's/)$//g'
[0]="1"
[1]="2"
[2]="3"
You can then further select id of an array directly (see manon script)
You can also have a function for this : function pa { declare -p $1 |sed s/"declare -a $1=("//g |tr ' ' '\n' |sed 's/)$//g';};pa a
or even assign it to an alias:
alias printarray='function _pa (){ if [ -z $1 ];then echo "please provide a var";else declare -p $1 |sed "s/declare -a $1=(//g; s/)$//g; s/\" \[/\n\[/g";fi; };_pa'
for some reason tr ' ' '\n' raises errors in alias.... We switched to last sed replacing  [ with \n[ 

------------------------------------------------------------------------------------------------------------------------
BASH:WORD SPLITTING
http://mywiki.wooledge.org/WordSplitting
Mainly affects args in scripts / functions , affects array elements splitting and also affects the behavior of read command.

The shell's parser performs several operations on your commands before finally executing them. Understanding how your original command will be transformed by the shell is of paramount importance in writing robust scripts. From the bash man page:

The order of expansions is: brace expansion, tilde expansion, parameter, variable and arithmetic expansion and command substitution (done in a left-to-right fashion), word splitting, and pathname expansion

To understand word splitting consider a simple bash script called i.e args , including:
#!/bin/sh
printf "%d args:" $# && printf " <%s>" "$@" && echo

Tests:
args hello world "how are you?"                  --> 3 args: <hello> <world> <how are you?>
var="This is a variable" && args $var            --> 4 args: <This> <is> <a> <variable>
log=/var/log/qmail/current IFS=/ && args $log    --> 5 args: <> <var> <log> <qmail> <current>

Thing become weird in case of filenames, i.e args $(ls -l). As a general idea we do not want to let word splitting occur when filenames
are involved. (See http://mywiki.wooledge.org/BashPitfalls for a discussion of this particular issue.)

Double quoting an expansion suppresses word splitting, except in the special cases of "$@" and "${array[@]}"
var="This is a variable"; args "$var"                     --> 1 args: <This is a variable>
array=(testing, testing, "1 2 3"); args "${array[@]}"     --> 3 args: <testing,> <testing,> <1 2 3>

"$@" causes each positional parameter to be expanded to a separate word; its array equivalent likewise causes each element of the array to be expanded to a separate word.
There are very complicated rules involving whitespace characters in IFS. Quoting the man page again:

If IFS is unset, or its value is exactly <space><tab><newline>, the default, then any sequence of IFS characters serves to delimit words. 
If IFS has a value other than the default, then sequences of the whitespace characters space and tab are ignored at the beginning and end of the word, as long as the whitespace character is in the value of IFS (an IFS whitespace character). 
Any character in IFS that is not IFS whitespace, along with any adjacent IFS whitespace characters, delimits a field. 
A sequence of IFS whitespace characters is also treated as a delimiter. If the value of IFS is null, no word splitting occurs.

If IFS contains non-whitespace characters, then empty words can be generated:
$ getent passwd sshd                     --> sshd:x:100:65534::/var/run/sshd:/usr/sbin/nologin
$ IFS=:; args $(getent passwd sshd)      --> 7 args: <sshd> <x> <100> <65534> <> </var/run/sshd> </usr/sbin/nologin>

If above expansion result to a single word with * , then a further filename expansion is performed by bash (globbing).
PS: Pathname expansion can be disabled with set -f; though this can lead to surprising and confusing code.

When using the read command, word splitting is performed on the input, but only when multiple variable names are given, or when read -a is used (to populate an array). 
Quoting is irrelevant here, though this behavior can be disabled by removing whitespace from IFS

Tip: Word splitting is not performed on expansions in assignments. Thus, one does not need to quote anything in a command like these:
foo=$bar OR bar=$(a command) OR logfile=$logdir/foo-$(date +%Y%m%d) OR PATH=/usr/local/bin:$PATH ./myscript
In either case, quoting anyway will not break anything. This is why experts advise "always quote" = "always prevent word splitting".
Experienced coders can selectively remove quoting to achieve the per case required results making advantage of word splitting.

##Trick with IFS and values in one line - one variable:

Instead of creating an array with values and manipulate array items like:
IFS="[, ]" read -r -a list <<<"Yeseva+One, Yrsa"  #Convert values to array
for k in "${list[@]}";do  #actions ;done

You can  do:
variable="Yeseva+One, Yrsa"
IFS="[, ]" for i in $variable;do #actions;done

------------------------------------------------------------------------------------------------------------------------
BASH:IFS Tricks
bash hackers word split:http://wiki.bash-hackers.org/syntax/expansion/wordsplit
The IFS variable holds the characters that Bash sees as word boundaries in this step. The default contains the characters
<space>
<tab>
<newline>
These characters are also assumed when IFS is unset. 
When IFS is empty (nullstring), no word splitting is performed at all.

The correct way to assign values to IFS is the bash way $'...' . 
This way is also used when you need to store special chars in any variable , i.e a=$'this is some\ntest' . 
You can not do above operation (embed newline) without using $' ' . If you try like a="this is a \ntest" the \n will be literal.
Ugly alternative for variables : a=$(echo -e "this is \n a test")

#### Special IFS settings used for string parsing. ####
Whitespace == :Space:Tab:Line Feed:Carriage Return:
WSP_IFS=$'\x20'$'\x09'$'\x0A'$'\x0D'
No Whitespace == Line Feed:Carriage Return
NO_WSP=$'\x0A'$'\x0D'

later, you can just set IFS=${WSP_IFS}

You can temporary set IFS inside a command like while IFS=: read -r lines
Set IFS= to read whole lines , separated by \n
Or set IFS=$'\n'
Set IFS to any char to perform special field split
For file with lines in format field1:field2:field3 using IFS= you will get the whole line, using IFS=: you can split each field.

##Trick with IFS and values in one line - one variable:

Instead of creating an array with values and manipulate array items like:
IFS="[, ]" read -r -a list <<<"Yeseva+One, Yrsa"  #Convert values to array
for k in "${list[@]}";do  #actions ;done

You can  do:
variable="Yeseva+One, Yrsa"
IFS="[, ]" for i in $variable;do #actions;done

------------------------------------------------------------------------------------------------------------------------

BASH:ARGS - POSITIONAL PARAMETERS 
http://wiki.bash-hackers.org/scripting/posparams#range_of_positional_parameters

##Iterate through args
function test { for a in "$@";do echo "$a";done;};test "${args[@]}"
function test { while [[ "$#" -gt 0 ]];do echo "$1";shift;done;};test "${args[@]}"

START at the last positional parameter: echo "${@: -1}" or -1:1 to get one char from end.

##ARGS to Array
function test { argn=${#@};for ((i=$argn;i>0;i--)); do args[$i]=${@: -$i:1};done;};test a b c;declare -p args
Output --> declare -a args=([1]="c" [2]="b" [3]="a")


##ARGS Seperatation depending on first char (i.e dash -)
${1:0:1} will return the first char. Then you can compare (if) with == "-"

You can assign all args in an array using "-" as delimiter
Function test { local args=$@;declare -a params;readarray -d"-" -t -O1 params <<< "$args";declare -p params;}

Thus you can send an argument like "one two", which will be normally considered as two args ($1=one , $2=two)
But in case of readarray will be considered as $1 since there is not dash - to separate the args.

Another Option is to use case with loop to iterate through args separated by dash -:
for arg in $@;do
case arg in
-*) arg starts with dash , then do this;;
*) not dash , so do the other thing;;
esac

Also see the BASH:READ Section for sending args through a file in various cases.

##USE OF SET FOR CUSTOM USAGE
Build a custom command with args by set - make use of positional parameters
http://unix.stackexchange.com/questions/338852/array-as-value-for-tar-exclude/338854#comment599032_338854

You can use the $@ as a kind of variable to make commands.
By applying set ls , command ls is stored in $@.
Using echo "$@" will see ls.
Using just $@ will EXECUTE ls.

You can also append more commands, switches, etc in $@ using set "$@" -d
Now the command hold in $@ is ls -d. Can be seen with echo, can be executed just like $@
You can add as many parameters as you wish.

Also , you can refer to this $@ var with "${@:2:1}" -> will print the second argument of $@

Example by StackExchange (Gilles):
set tar -zcvf "$FILE" 
set "$@" --exclude='/home/user/test1'
set "$@" --exclude='/home/user/test2'
set "$@" --exclude='/home/user/test3'
"$@" "$SOURCE"

More Checks:
    $ set ls && echo "${@}" --> ls
    $ set "$@" "*.sh" && echo "${@}" --> ls *.sh
    $ $@ --> prints (ls) all .sh files
    $ set "$@" "*.txt" && echo "${@}" --> ls *.sh *.txt
    $ $@ --> Outputs all .sh and .txt files
    $ echo "${@:2:1}" --> *.sh
    $ echo "${@:2}" --> *.sh *.txt
    $ echo "${@}" --> ls *.sh *.txt
    $ shift && echo "${@}" --> *.sh *.txt
    $ shift && echo "${@}" --> *.txt

.B Use set to strip domain name from http address
dom="http://unix.stackexchange.com/questions/ask"
FS=/; set -- $dom; echo "$3" :--->unix.stackexchange.com
Alternativelly can be done with cut -d/ -f3, or with regex groups (perl) or with tricky sed (i.e sed "y|/|\x00|;s/.*\x00\x00//;s/\x00.*//")

##ARGS null separated
You can manipulate args that may contain any character using null byte as a separator.
arg1="some more";arg2="text here";args=$(echo -e "$arg1\0$arg2\0" |base64) 
#it is important to have null even at the end to separate the last item
#we need to convert to base64 since bash tends to remove the null byte from variables. With base64 we preserve it.
while IFS= read -r -d '' f;do echo "f= $f";done< <(echo "$args" |base64 -d) #base64 variable is decoded

.B Function test with null separated args
function test { while IFS= read -r -d '' f;do echo "f= $f";done< <(echo "$@" |base64 -d);}
test "$args" #works correctly. One arg is sent containing more args null separated.

.B Function Test - Different multi separated args
function test2 { for arg in $@;do echo "test2-arg=" $(echo "$arg" |base64 -d);done;}
arg11=$(echo -e "$arg1\0" |base64); arg22=$(echo -e "$arg2\0" |base64)
test2 $arg11 $arg22
#args are understood and printed correctly in output

.B Tip
For bash 4.4 and above you could also use mapfile/readarray that support -d option (delimiter):
mapfile -t -d '' arr < <(echo "$args" |base64 -d)
mapfile -t -d '\0' b < <(sort_array_function "${a[@]}") 
#see also http://stackoverflow.com/questions/42381149/bash-function-with-an-array-input-and-output

Another way to print null separated args/vars/arrays is to use printf :
  printf '%s\0' "${sorted[@]}"

------------------------------------------------------------------------------------------------------------------------
BASH:PARAMETERS PRACTICAL_USE_OF_BASH_PARAMETERS_EXPANSION
Check these one-liners: http://www.catonmat.net/blog/another-ten-one-liners-from-commandlinefu-explained/
Scroll to the end of page for more one-liners.
http://wiki.bash-hackers.org/syntax/pe

Command substitution : Use contents of file as parameter: $(<file) #emulates cat
Command $(cat file) can be replaced by the equivalent but faster $(< file).  : echo "$(<file.txt) -- similar to cat file.txt
if [[ " ${array[@]} " =~ " ${value} " ]]; then whatever fi #if array contains value
if [[ ! " ${array[@]} " =~ " ${value} " ]]; then whatever fi
#Get name without extension -> ${FILENAME%.*} ⇒ bash_hackers.txt
#Get extension -> ${FILENAME##*.} ⇒ bash_hackers.txt
#Get extension : find $PWD -type f -exec bash -c 'echo "${0##*.}"' {} \; -> Lists all extensions found.
#Get directory name -> ${PATHNAME%/*} ⇒ /home/bash/bash_hackers.txt
#Get filename -> ${PATHNAME##*/} ⇒ /home/bash/bash_hackers.txt

Remove first and last char with bash expansion: 
a=$(echo "\"some@some.com\"");echo "Original a=$a - Modified a= ${a:1:-1} - First and Last char removed"
--> Original a="some@some.com" - Modified a= some@some.com - First and Last char removed

FOO="http://unix.stackexchange.com/questions/ask"
tmp="${FOO#*//}" 	#Removes everything from begininng up to first found pattern '//'
echo "${tmp%%/*}" : unix.stackexchange.com		#remove everyting from end up to last/longest (%%) pattern '/' . One % = shortest/first

a='2.5.2.pl';echo "${a%.*}-${a##*.}" ----> 2.5.2-pl      #Replace last dot with dash:

##Lowercase - UpperCase
http://stackoverflow.com/questions/2264428/converting-string-to-lower-case-in-bash-shell-scripting
$ string="A FEW WORDS"
$ echo "${string,}"  --> a FEW WORDS
$ echo "${string,,}" --> a few words
$ echo "${string,,[AEIUO]}" --> a FeW WoRDS #Only vowels
$ string="A Few Words" && declare -l string && string=$string; echo "$string" --> a few words

$ string="a few words" && echo "${string^}"  ---> A few words
$ echo "${string^^}"                         ---> A FEW WORDS
$ echo "${string^^[aeiou]}"                  ---> A fEw wOrds
$ string="A Few Words" && declare -u string && string=$string; echo "$string" ---> A FEW WORDS

$ string="A Few Words" && echo "${string~~}"      --> a fEW wORDS   #Toggle
$ string="A FEW WORDS" && echo "${string~}"       --> a fEW wORDS   #Toggle
$ string="a few words" && echo "${string~}"       --> A Few Words   #Toggle
$ string="a few words" && $ declare -c string && string=$string && echo "$string"       ---> A few words #Capitalize
$ string="a few words" && string=($string) && string="${string[@]^}" && echo "$string"  ---> A Few Words
$ declare -c string && string=(a few words) && echo "${string[@]}"                      ---> A Few Words

Alternatives:
echo "$a" | tr '[:upper:]' '[:lower:]'
echo "$a" | awk '{print tolower($0)}'
echo "$a" | perl -ne 'print lc'

------------------------------------------------------------------------------------------------------------------------
BASH:TIPS AND TRICKS
##Resources: 
http://www.catonmat.net/blog/another-ten-one-liners-from-commandlinefu-explained/
http://www.catonmat.net/blog/top-ten-one-liners-from-commandlinefu-explained/
http://wiki.bash-hackers.org/snipplets/start

##Count occurences of a char within a string (var="text,text,text,text")
GREP: reps=$(grep -o "," <<< "$var" | wc -l)
AWK : awk -F"," '{print NF-1}' <<<"$var" #or {print NF?NF-1:0}
BASH : reps="${var//[^,]}" && echo "${#reps}"  #removes everything except comma

##Print a dashed line in full terminal width
printf '%*s\n' "${COLUMNS:-$(tput cols)}" '' | tr ' ' -
OR printf '%.0s-' {1..20}; echo  #will print just 20 dashes
OR eval printf %.0s- '{1..'"${COLUMNS:-$(tput cols)}"\}; echo

##Identify the width of the terminal
tput cols

##Using subshells: 
$ (cd /tmp && ls) This will call a subshell to perform the commands and will exit. Thus your real shell will not cd to /tmp

##Reverse any word
echo "nixcraft" | rev

##Rename using for and bash parameter expansion
for f in 0[12]/I00[12]0001 ; do mv "$f" "${f}.dcm" ; done # This will go in two folders (01 and 02) and read two files inside each folder (I0010001 and I0020001) and add dcm extension to each of them.

##Remove new line char from strings and replace it with space using trim (tr)
echo -e "hello\nyou asshole" |tr "\n" " " ->hello you asshole #If you remove the tr you will see the text to be printed in two different lines. If you apply -d "\n" new lines will be deleted.
With sed it supposed to be sed -e 's/[\n]//g' but is not working. Texts keeps priting in terminal in two lines.

##Use dnstools to read a wikipedia page in terminal:
dig +short txt hacker.wp.dg.cx # searches wikipedia for term hacker.
I have an alias for that. Alternative : host -t txt hacker.wp.dg.cx

##Quick move and copy 
cp filename{,.bak} #using brace expansion
mv /path/to/file{,_old} #brace expansion

##Trace root with ping together: 
$ mtr google.com

##Find the last command that begins with "whatever," but avoid running it : 
$ !whatever:p

##Change to the previous working directory
$ cd - (insted of cd $OLDPWD)

##Serve the current directory at http://localhost:8000/
$ python -m SimpleHTTPServer 8000

##Run the last command as root : 
$ sudo !! (also simple !! just repeats last command)

##Capture video of a linux desktop : 
$ ffmpeg -f x11grab -s wxga -r 25 -i :0.0 -sameq /tmp/out.mpg

##Read the first line from a file and put it in a variable : 
$ read -r line < file OR IFS= read -r line < file

##Read a random line from a file and put it in a variable : 
$ read -r random_line < <(shuf file)

##Extract filename /dirname from the path : 
filename=${path##*/} 
dirname : dirname=${path%/*}

##declare upper/lower case variables 
uppercase: declare -u foo 
lowercase: declare -l
declare -u b;eval {a,b}="george"; echo "$a --- $b" --> b will print GEORGE due to declare -u in the beginning.

PS: Alternative for lower/upper case is ${a^^} and ${a,,}

##Assign same value to multiple commands using bash parameter expansion: 
eval {a,b,c}="some text" # Variables a , b and c will get value some text. Without eval is not operating.

##Identify files/ commands : 
type command (try i.e type grep and type eval)
file <file> #will print info about the file , like if it is ASCII

##List all kernel modules that are loaded (i.e lsmod)
cat /lib/modules/$(uname -r)/modules.dep
find /lib/modules/$(uname -r) -type f -name \*.ko

##Working with ASCII values of a string / file 
$ echo 'this is \"something\"' |od -t x1c                                                                                               0000000  74  68  69  73  20  69  73  20  5c  22  73  6f  6d  65  74  68
          t   h   i   s       i   s       \   "   s   o   m   e   t   h
0000020  69  6e  67  5c  22  0a
          i   n   g   \   "  \n
od -t x : Prints hex values
od -t d : Prints dec values
od -t o : Prints octal values
Tip1: You can echo a hex ascii value with echo -e "\x5c" => will print a backslash .
Tip2: You can assign a hex ascii in a variable using var=$'\x5c' - 
Tip3: You can also use hex values in sed and other tools to avoid escaping : sed 's/\x22/\x5c\x22/g' --> Will escape all double quotes.

--------------------------------------------------------------------------------------------------------------------------------------
BASH:PRINTF
http://wiki.bash-hackers.org/commands/builtin/printf
http://www.computerhope.com/unix/uprintf.htm

##Print the progress of a recursive command in one / the same line (by Duffy):
touch ff1 ff2 ff3
rm -fv ff1 ff2 ff3 |while read -r line;do 
ws=$(( COLUMNS - ${#line} ));printf '\r%s%*s\r%s' "$line" "$ws" " " "$line";sleep 2;done;echo

##PRINTF: Looping over hexadecimal values (hex loop)
Range 00-09 and then 0A-0B-0C-0D-0E-0F. Repeats for all numbers.
The most easy way is to use printf since natively supports hex printing

for ((i=0;i<128;i++));do                     #Decimal Numbers in Looping but Hex Output !
h1=$(printf "%#x\n" $i) && echo "$h1"        #prints 0x7a
h2=$(printf "%x\n" $i) && echo "$h2"         #prints just 7a. This can combined in bash /awk/sed/perl with \x in front
h3=$(printf "\\\x%x\n" "20") && echo "$h3"   #prints \x14 - ready to be used in bash/sed/perl/awk as hex code (i.e echo -e "$h3")
done

Testing:
$ h=$(printf "\\\x%x" "60") && echo "$h" && echo -e "$h"   #First echo prints \x3c, Second echo prints <


##PRINTF: CONVERT CHARS TO ASCII VALUE
$ printf '%d\n' "'a" or "'a'" -> prints 97 . For some reason the letter must be enclosed by double quotes and with single quotes!. 
$ var="a"; printf '%d\n' "'$var'" --> also prints 97

##CHARACTERS ASCII Sum:
while read -rn1 char;do sumA+=$(printf '%d+' "'$char'");done <<<"1.Nf3 c5 2.e4 Nc6"
echo "$sumA"              ---->> 49+46+78+102+51+39+99+53+39+50+46+101+52+39+78+99+54+39+ #mind the trailing +
$ bc <<<"${sumA:0:-1}"    ---->> 1114   #with 0:-1 the trailing + is removed.

--------------------------------------------------------------------------------------------------------------------------------------

BASH:EVAL AND INDIRECT EXPANSION
http://tldp.org/LDP/abs/html/ivr.html
http://mywiki.wooledge.org/BashFAQ/048 - Eval command and security issues
http://mywiki.wooledge.org/BashFAQ/006#eval

1) foo=10 && x=foo
2) y='$'$x
3) echo $y --> $foo
4) eval y='$'$x   #or eval y="\$$x"
5) echo $y --> 10 # or even 
6) eval echo '$'$x ->10 # also works - alt: eval echo "\$$x"

Simpler with indirect expansion:
$ foo=10;x=foo;echo ${!x} 	--> 10
$ foo=10;x=foo;echo ${x} 	--> foo

Also try to use ${a,b}="some" - bash will complain that a="some" command not recognized. If you use eval then goes ok.

##Indirect Expansion Examples

$ function test { for (( i=1; i<="$#"; i++ )); do   echo "${i}";done; }; test -abcd    ---- > Prints 1 since i is 1 due to for loop
$ function test { for (( i=1; i<="$#"; i++ )); do   echo "${!i}";done; }; test -abcd   ---- > Prints -abcd since $i becomes $1 

Indirect expansion is also found to be written in the format eval 'echo "${'"$i"'}"'  <-> $1 when i expands to 1

http://stackoverflow.com/questions/42047532/bash-for-loop-to-set-a-variable-its-value-and-evaluate-it/42047814?noredirect=1#comment71311357_42047814

$ for i in {1..2};do eval my${i}var="./path${i}_tofile";eval echo "$""my${i}var";done
./path1_tofile
./path2_tofile

Tip: the first eval (eval my${i}var) can be avoided using declare my${i}var
Remark: OP tried to print the values using echo "$my${i}var" , which never worked (variable within a variable - bash panic!)

For echo part, instead of using eval , the same can be achieved with Indirect Expansion
(see also http://wiki.bash-hackers.org/syntax/pe#indirection)
# for i in {1..4};do declare my${i}var="./path${i}_tofile";tmpvar=my${i}var;echo ${!tmpvar};done

Mind the difference If you don't use the indirect expansion :
# for i in {1..2};do declare my${i}var="./path${i}_tofile";tmpvar=my${i}var;echo ${tmpvar};done
my1var
my2var

## Indirect Expansion vs Eval
Eval is possible to execute potential command substitutions. Indirect expansion will not. 
Test with this: 
tmpvar='foo`execute nasty command`'; echo "${!tmpvar}"; eval echo "${tmpvar}" -- 
the indirect expansion is essentially "variable not found, substitute the empty string", while eval executes the nasty command.

Some more tests:
a="ls -l";foo=a;echo "${!foo}"       ---> Prints ls -l
a="ls -l";foo=a;eval "${!foo}"       ---> Executes ls -l
a="ls *.sh";foo=a;"${!foo}"          ---> bash: ls *.sh: command not found
a="ls";foo=a;"${!foo}"               ---> Executes simple ls
a="ls -l";foo=a;${!foo}              ---> Executes ls -l
a="ls *.sh";foo=a;${foo}             ---> a command not found -same with dquotes
a="ls *.sh";foo=a;eval echo ${foo}   ---> Prints a - same with double quotes
a="ls";foo=a;eval ${foo}             ---> bash: a: command not found - same with dquotes
a="ls";foo=a;eval \$"${foo}"         ---> executes ls
a="ls";foo=a; echo \$"${foo}"        ---> prints $a

a="ls";foo=$a;${foo}                 ---> executes ls either with single or double quotes
a="ls";foo=$a;echo ${foo}            ---> prints ls

Also:
name=$'My name is \x3b\x6c\x73'; eval "user=$name" --> will execute ls (hexcode = ;ls --> my name is; ls


##Indirect Expansion in arrays
http://unix.stackexchange.com/questions/20171/indirect-return-of-all-elements-in-an-array

One of the most often cases is the need to use eval or indirect expansion in arrays.
Bash does not allow IE to be applied in $!arr[@]} because same operator is used in order to get array indices

A real life usage is to have i.e two different arrays and ask from user (or get from script) which array to print:
$ arr1=("one" "two")
$ arr2=("alpha" "omega")
$ declare -p arr1 arr2
declare -a arr1=([0]="one" [1]="two")
declare -a arr2=([0]="alpha" [1]="omega")
$ user="1"   #user selection - could be read -p "select array" user
$ tmp="arr$user[@]" && declare -p tmp
declare -- tmp="arr1[@]"
$ newarr=("${!tmp}") && declare -p newarr
declare -a newarr=([0]="one" [1]="two")

a=(one two three) && b="a[@]" && echo "${!b}" && newArr=( "${!b}" ) && declare -p newArr

---------------------------------------------------------------------------------------------------------------------------------------
BASH:FILE DESCRIPITORS AND REDIRECTIONS
##Sources
Check this FAQ http://mywiki.wooledge.org/BashFAQ/002 for a nice table explaining redirections with file descriptos
Redirections explained with graphics: http://www.catonmat.net/blog/bash-one-liners-explained-part-three/
http://www.tldp.org/LDP/abs/html/io-redirection.html
http://stackoverflow.com/questions/4102475/bash-redirection-with-file-descriptor-or-filename-in-variable
http://unix.stackexchange.com/questions/13724/file-descriptors-shell-scripting
http://www.tldp.org/LDP/abs/html/ioredirintro.html

##BASICS: 
fd0=stdin , fd1=stdout , fd2=stderr - Equals to 0>/dev/tty, 1>/dev/tty , 2>/dev/tty
This redirection format >/dev/null ()or >wherever) EQUALS TO 1>/dev/null (or 1>wherever) == redirection of stdout

default descriptor for < redirection is 0, STDIN_FILENO
default descriptor for > redirection is 1, STDOUT_FILEN

You can close a file descriptor using &-
If you exec >&- this equals to 1>&- = close stdout
If you exec <&- this equals to 0<&- = close stdin. This in a terminal closes the terminal (simulates control D = EOT)
Mind that if you specify a fd , then you can close it either with > or <. Thus exec 0>&- also works like exec <&- (both close terminal)


IF you try : test=$(java -version);echo $test then you will receive output of java -version in your terminal but var test will be empty.
But if you try test=$(java -version 2>&1);echo $test works ok.
java app prints its version to stderr and not to stdout.
By default you can not assign in vars output of commands that send their output other than stdout &1 (i.e &2 =stderr) 
With the 2>&1 you redirect stderr to stdout and thus you can store that output in a variable.

<GV Remark>
Although we said that at initial stage all fds point to screen this means all fds go to /dev/tty and not to >1
seems that 1&,2& are a kind of "internal variable" and /dev/stdout is a kind of temp file - sym link
In a pseudocode format : uservar=$(command) EQUALS to command 1>uservar (instead of 1>/dev/tty=screen)
</GV Remark>

Redirect stderr to file and stdout + stderr to screen :
exec 3>&1 && foo 2>&1 >&3 | tee stderr.txt (equals to 2>&1 1>&3)

##Tricky redirection from bins missing man pages:
man -w binaryfile 2>&1 >/dev/null (-w prints man page location)
In case of a normal bin file (i.e grep) then nothing is printed. In case of a bin that do not have a man page (i.e getweb) then
the error message is printed.
mind also that a=$(man -w getweb >/dev/null) will also print the error message, even if $a is NOT echoed and also $a will be blank.
the redirection >/dev/null is in reality equal to 1>/dev/null, meaning redirecting stdout (&1) to dev/null
mind also that examples:
man -w grepp 2>/dev/null -> although the package / bin is wrong = no man page , nothing is printed on screen since stderr is forwarded to /dev/null
man -w grep 1>/dev/null -> equivalent to man -w grep >/dev/null
man -w grep 2>/dev/null -> since there is a man page for grep, the location is printed on screen since this redirection affects only &2 = stderr
man -w grepp &>/dev/null -> this syntax forwards both stdout and stderr and as result nothing is printed either for grepp (no man page) or grep (valid man page)
#
With annotate-output shell script of devscripts you can run any command and it's output will be marked by O or E depending on where it's printed (0 for stdout, E for stderr). It is also provide Info (I) about exit code
Main usage of file descriptors is when you need to split your code like this;
exec >data-file #equivalent to 1>data-file = redirect stdout to a data-file
exec 3>log-file
echo "first line of data"  #though you don't specify fd , it is redirected to data_file due to the very first exec
echo "this is a log line" >&3 #this goes to fd3 = log file
if something_bad_happens; then echo error message >&2; fi #this goes to fd2 (not specially defined in this example)
exec &>-  # close the data output file
echo "output file closed" >&3
But again you dont gain anything with fds. You can send output directly to >anyfile in case of echo
On the other hand , by assigning stdout to ata-file (just >data-file) you can capture messages from scripts/programms etc that would
normally go to stdout.
correspondingly you can exec 2>error-file and any programm that prints anything of &2 (stderr) will be sent to error-file.

-----------------------------------------------------------------------------------------------------------------------------------------
BASH:FILE DESCRIPTORS tricky usage from http://mywiki.wooledge.org/BashFAQ/002

##Basic redirections chain
The key point to understand redirections is that commands behavior is always the same : 
normal output goes to /dev/stdout , errors go to /dev/stderr and both of them go to /dev/tty

Works like :
command output 1>-> /dev/stdout (&1)--> /dev/tty[0] (in CLI)
command errors 2>-> /dev/stderr (&2)--> /dev/tty[0] (in both CLI and script format $())

Within a pipe - variable:
command output 1>-> /dev/stdout (&1)--> buffer/pipe (in script format $())

Actually with redirections you alter the last part of the chain.
You disconnect /dev/stdout for /dev/tty and you connect it somewhere else, i.e in a logfile

So in case of  $(cat 'any file' 3>&1 1>&2 2>&3) we have 
Initial state in script usage:
cat command succesfull -> normal output --> stdout -->fd1 --> command pipe
cat command error (file not found) -------> stderr -->fd2 -->tty0


Important: 
You  can force - redirect something directly to dev/tty (=real screen) instead of the /dev/stdout.
/dev/stdout serves like intermediate var to connnect tty with command fd &1
So the norma chain
command &1>--> /dev/stdout (fd1) --> /dev/tt0
changes to command &1>--> /dev/tty0 = attach &1 to /dev/tty instead of /dev/stdout

It it the /dev/stdout that is automatically attached to command internal buffer $( ) 

For example you can apply : cat file |tee >/dev/tty |awk '{print $2}'
This will force tee to print on screen and on the same time data will be available to reach awk
Equivalent function to tee:
function teee { a="$(</dev/stdin)";echo -e "pipe in\n$a\npipe out\n" >/dev/tty; echo "$a"; }
Usage cat file |teee |awk '{...}' Prints the output of cat and pipes the data to awk as well


##Manipulate stdout & stderr  
Well, then first you have to choose a non usefully place for stdout to go: 
(a)output=$(command 2>&1 >/dev/null)  # Save stderr, send stdout to /dev/null = nowhere
(b)output=$(command 2>&1 >/dev/tty)   # Save stderr, send stdout to the terminal.
(c)output=$(command 3>&2 2>&1 1>&3-)  # Save stderr, send stdout to script's stderr.

Analyzing the last (c) case:
Initial :If this run in a terminal without any redirections stdin, stdout and stderr are all initially connected to the terminal (tty).
state	: equals to 0>/dev/tty , 1>/dev/tty, 2>/dev/tty and 3 does not exist by default.

3>&2 	: 	FD 3 should point to what FD 2 points to at this very moment, meaning FD 3 will point to the script's stderr 
			(also called "save stderr in FD 3").

2>&1 	:	Next, FD 2 should point to what FD 1 currently points to, meaning FD 2 will point to stdout. 
			Right now, both FD 2 and FD 1 would be captured.

1>&3 	:	Next, FD 1 should point to what FD 3 currently points to, meaning FD 1 will point to the script's stderr. 
			FD 1 is no longer captured. We have "swapped" FD 1 and FD 2.

3>&- 	:	Finally, we close FD 3 as it is no longer necessary.

gv: This swaps stdout and stderr. In real life the succesfull results of the command go to &1 which is printed in screen instead of the command buffed (pipe). The error of the command goes to &2 which has been redirected to buffer/pipe.
Verificable example:
sh-4.2$ echo "echo printing: $(cat test 3>&2 2>&1 1>&3)" 
this is a test #this is printed directly on screen (tty0) by cat
echo printing: #this is the output of the command $(..) which is nothing since with above redirections the pipe will hold only stderr.

sh-4.2$ echo "echo printing: $(cat notexistingfile 3>&2 2>&1 1>&3)" 
echo printing: cat: notexistingfile: No such file or directory 
Nothing is printed on the screen. Error message is stored in the pipe / command buffer $(...)

Another redirection example . It is useless but can give some tips on how redirection works:
sh-4.2$ echo "echo printing: $(cat notexistingfile 1>&2 2>&1)"
cat: notexistingfile: No such file or directory #printed directly on the screen/tty0
echo printing: #command buffer holds nothing
sh-4.2$ echo "echo printing: $(cat test 1>&2 2>&1)"
this is a test #printed directly on tty0 / screen
echo printing: #command buffer / pipe holds nothing again

Explanation:
First Redirection :1>&2 detach fd 1 from pipe/buffer (normal operation for a command $()) and send it to fd2 = tty0 (default for fd2)
Second Redirection: 2>&1 detach fd2 from tty0 (default) and send it to wherever fd1 is attached = tty0 (from previous redir)
Result : both fd1 and fd2 are sent to tty0

Another useless example:
sh-4.2$ echo "echo printing: $(cat notexistingfile 2>&1 1>&2)"
echo printing: cat: notexistingfile: No such file or directory
sh-4.2$ echo "echo printing: $(cat test 2>&1 1>&2)"
echo printing: this is a test

Explanation:
First redirection 2>&1 : detach fd2 from default (tty0) and send it to wherever fd1 goes (command buffer/pipe when used with $())
Second redirection 1>&2 :detach fd1 from pipe/buffer (default in case of $()) and send it to wherever fd2 goes = command buffer = useless
Result: Nothing is printed on tty0 but both normaloutput (stdout) and error output (stderr) go to pipe/command buffer.

Another example of swapping stderr with stdout
sh-4.2$ echo "echo printing: $(cat notexistingfile 3>&1 1>&2 2>&3)"
echo printing: cat: notexistingfile: No such file or directory #stderr goes to command pipe/buffer

sh-4.2$ echo "echo printing: $(cat test 3>&1 1>&2 2>&3)"
this is a test #normal out (stdout) goes directly to screen 
echo printing: #command pipe/buffer is empty since no error raised - no data send to stderr.

Explained Redirections:
---------------------------STEP 1 (3>&1)----------------------------------------------------------------------------------
fd3-----|          #fd3 created and goes to wherever stdout-->fd1 goes = pipe for scripts format 
        |
fd1---> pipe 
fd2---> tty0
---------------------------STEP 2 (1>&2)----------------------------------------------------------------------------------
fd3 ------------|
                |
fd1 ----|      pipe
        |          #stdout --> fd1 detached from pipe and goes to wherever fd2 (stderr) goes = tty0 
fd2--->tty0

---------------------------STEP 3 (2>&3)----------------------------------------------------------------------------------

fd3 ------------|
                |
fd1 ----|      pipe
        |       |
       tty0     |
                |
fd2-------------|  #stderr-fd2 detached from tty0 and goes to wherever fd3 goes = pipe


Redirection Result Chain (swap fd1 with fd2 for scrip usage):
cat command normal output --> stdout -->fd1 --> tty0 	#step 2 above
cat command error --> stderr -->fd2 -->pipe		#step 3 (2>&3) and 1 (3>&1) above

And a final redirection that disregards normal output and get's only error to do something with them:
sh-4.2$ echo "echo printing: $(cat test 2>&1 1>/dev/null;echo $?)"
echo printing: 0 #normal output --> stdout-->fd1-->/dev/null
sh-4.2$ echo "echo printing: $(cat tesat 2>&1 1>/dev/null;echo $?)"
echo printing: cat: tesat: No such file or directory 1 #error output ->stderr ->fd2 -> pipe/buffer
In both cases nothing printed on screen directly.

Mind the exit code of cat. Can be used to built if statements like if error without printing anything.
remember that the usage if $(command);then EQUALS TO if $? of last command = 0 then

AS IT IS OBVIOUS, REDIRECTION SEQUENCE IS IMPORTANT TO ACHIEVE YOUR RESULT. REDIRECTIONS ARE READ FROM LEFT TO RIGHT.

In all above redirections, the existance of fd3 is like a temp save of the location that fd1 goes.
This could be avoided if we were sure where exactly to redirect final destination.
If for example in all systems world wide the screen was tty0, then we could avoid the use of temp fd3 like this:
$(cat 'any file' 2>&1 1>/dev/tty0)
Meaning: detach fd2 from tty0(default) and send it to wherever fd1 goes (=pipe) and then detach fd1 from pipe and send it to /dev/tty0.

But this is not portable and this is why is not used at all. 
You create a tmp fd3 to go at fd1 in the very beginning , saving the pipe internal location/name , then you ask fd1 to go to wherever fd2 goes (screen , maybe tty0, maybe tty1, maybe something else) and finally you detach fd2 from screen and send it to fd1 previous location = fd3 current location , since you can not redirect fd to a place that no longer exists (like fd1 old place) 

## ANOTHER TRICKY REDIRECTION FOR SWAPPING STDERR WITH STDOUT (http://mywiki.wooledge.org/BashFAQ/002)
This also achieves the swap in a different way:
exec 3>&1			# Save the place that stdout (1) points to . This is the screen because we are in CLI mode.
output=$(command 2>&1 1>&3)	# Run command.  stderr is captured, and 
exec 3>&- 			#close fd3	

This does the swap job (stderr is captured at output variable pipe and script's output is send to screen) because:
exec 3>&1 OUTSIDE script is sent to wherever fd1 goes OUTSIDE script = SCREEN(tty0)
2>&1 inside script goes detach fd2 from tty0 and send it to wherever fd1 goes INSIDE script = pipe  
1>&3 inside script detaches fd1 from script's pipe and send it to fd3 which has been assigned OUTSIDE the script to go at screen.
Brilliant! 

## Redirecting output to multiple processes:
$ tr '\n' '\\' <file3 > >(sed 's#a#A#g') > >(sed "s#[\]#\0n#g")
Apples\norAnges\nbAnAnAs\ncArrots\n

##  GLOBAL SCRIPT REDIRECTION
http://mywiki.wooledge.org/BashFAQ/106
#!/usr/bin/env bash
# or whichever shebang you prefer
exec >log 2>&1 #equals to 1>log 2>&1 = 2>log
Now, all of the commands in the rest of your script will inherit log as their stdout and stderr. 

## Redirection explained for enginners
Imagine each device as a lamp and each fd as a wire. If lamp found to be not connected to a wire is destroyed.
Thus initial state is fd1 --> screen. To remove fd1 wire from screen you need to connect first a helper wire (exec 3&>1)
Once lamp/screen is connected with wire 3 you can remove wire 1 and attach it in another lamp (i.e file)
Inside a script redirection looks like that lamp1 (stdout) is enclosed in a box (subshell). 
When lamp 1 lits (stdout), it lits inside box, while lamp 2 (stderr) lamp still remains out of the box.
------------------------------------------------------------------------------------------------------------------------

BASH:WORKING WITH /dev
http://tldp.org/LDP/abs/html/devref1.html
http://stackoverflow.com/documentation/bash/399/redirection#t=201703221237056414693

$ cat </dev/tcp/time.nist.gov/13      ---> 53082 04-03-18 04:26:54 68 0 0 502.3 UTC(NIST) *

$ exec 5<>/dev/tcp/www.net.cn/80 && echo -e "GET / HTTP/1.0\n" >&5 && cat <&5

exec 3</dev/tcp/www.google.com/80
printf 'GET / HTTP/1.0\r\n\r\n' >&3
cat <&3

------------------------------------------------------------------------------------------------------------------------
BASH:FIFO NAMED PIPES

FIFOs actually work as named buffers. With all subshells/subprocesses can share info.
Maybe some commands do not accept input by refular files but from fifos and/or file descriptors. 

To create a FIFO pipe use "mkfifo mypipe1"
This actually creates a kind of FIFO file with name mypipe1 (can be seen with ls). 

Command "file mypipe1" will advise that this is a fifo.
Delete a fifo by rm mypipe1, as with any regular file.

You can echo something to this FIFO using echo "something" >mypipe1 . Mind that terminal prompt is trapped and terminal hangs until you retrieve 
the buffer data (from another terminal/shell/subshell) using cat <mypipe1 . Once finished terminal1 and terminal2 are released.

After cat , the fifo is empty - can be verified by trying to cat again.
Once you cat fifo in terminal2 and info has no data , terminal 2 remains trapped awaiting for data to come.
But once data comes in , will be printed and prompt will be freed.
Yad designed uses fifos in this example: https://sourceforge.net/p/yad-dialog/wiki/Frontend%20for%20find+grep%20commands/

Another example is the wikipedia netcat small proxy
mkfifo backpipe; nc -l 12345  0<backpipe | nc www.google.com 80 1>backpipe

This makes the fifo, and redirects local connections to port 12345 to google (default netcat operation)
but also redirects response back to browser!! (this is not netcat default operation)
You can verify if a fifo is present with if [[ ! -p "$pipe" ]];then mkfifo XXX;fi

There are various techniques to cheat the fifo "one-shot" and hanging behavior.

In terminal 1 if you run cat something >fifo1 & , this will release prompt1 and you can then echo many times to fifo1 witout terminal1 to be trapped again.

terminal 2 will print immediately whatever comes in fifo.

See: 
http://stackoverflow.com/questions/8410439/how-to-avoid-echo-closing-fifo-named-pipes-funny-behavior-of-unix-fifos
http://stackoverflow.com/documentation/bash/399/redirection#t=201703221519166519836

.B Example1:
$ ls -l /tmp > myPipe &
$ cat < myPipe or grep 'word' <mypipe or whatever.
As a general idea pipe is used just like a file.
This works on the same terminal because the use of & (background) is in reallity a subshell.

.B Example2:
$ { ls -l && cat file3; } >mypipe &
[2] 5582
$ cat <mypipe	#Prints correctly the file list AND file3 contents afterwards

.B Example3:
# ls -l >mypipe &
[2] 5589
# cat file3 >mypipe &
[3] 5590
# cat <mypipe
This also works ok. Mind that first are printed the contents of file3 (LIFO configuration)

.B Example4:
# var=$(cat <mypipe ) &
[2] 5599
# ls -l >mypipe
[2]+  Done                    var=$(cat <mypipe )
# echo "$var"    -----> prints NOTHING since the var has been assigned a value in a subshell 

.B Example5:
# { pipedata=$(<mypipe) && echo "$pipedata"; } &
[2] 5632
# ls >mypipe ---> Prints correctly the output of ls

.B Example6:
# pipedata=$(<mypipe)  #this hangs expecting the data
From another terminal ls -l >mypipe works and releases terminal 1. Also var pipedata has the correct value in terminal 1
Even in script , the script hangs.
If you do pipedata=$(<mypipe) & does not hang but data inside mypipe are not stored to pipedata - echoes nothing.

.B Example 7:
# export pipedata
# pipedata=$(<mypipe) &
[2] 5783
# ls -l *.sh >mypipe
[2]+  Done                    pipedata=$(<mypipe)
# echo "$pipedata"   ---> Prints Correctly!
Assigns pipe data to a variable / array in the same shell (same terminal/script) without hanging.
The variable is made available to both shell and subshell using export. 

 
------------------------------------------------------------------------------------------------------------------------

BASH:ASSOCIATIVE ARRAYS (declare -A array)
http://www.artificialworlds.net/blog/2012/10/17/bash-associative-array-examples/
http://www.artificialworlds.net/blog/2013/09/18/bash-arrays/  #Tips / Examples of Normal Arrays.

##Using Associative Arrays
Works like dictionaries of advanced programming languages.
You can assign whatever index you want (i.e array[file])
declare -A MYMAP=( [foo]=bar [baz]=quux [corge]=grault ); echo ${MYMAP[foo]};echo ${MYMAP[baz]} -> bar \n quux
K=baz; MYMAP[$K]=quux;echo ${MYMAP[$K]} -->quux   #also echo ${MYMAP[baz]} works 
declare -A MYMAP=( [foo a]=bar [baz b]=quux );echo "${!MYMAP[@]}" --> foo a baz b #prints only the keys
declare -A MYMAP=( [foo a]=bar [baz b]=quux );for K in "${!MYMAP[@]}"; do echo $K; done  #loop on keys only - mind double quotes.
--> foo a 
--> baz b
declare -A MYMAP=( [foo a]=bar [baz b]=quux );for V in "${MYMAP[@]}"; do echo $V; done #loop on values only
--> bar
--> quux
declare -A MYMAP=( [foo a]=bar [baz b]=quux );for K in "${!MYMAP[@]}"; do echo $K --- ${MYMAP[$K]}; done #loop on keys and values
--> foo a --- bar
--> baz b --- quux
declare -A MYMAP=( [foo a]=bar [baz b]=quux );echo ${#MYMAP[@]}  --> 2 # Number of keys in an associative array

##Number Indexing of Associative Array :
declare -A MYMAP=( [foo a]=bar [baz b]=quux );KEYS=("${!MYMAP[@]}");echo "${KEYS[0]} --- ${MYMAP[${KEYS[0]}]}" -> foo a --- bar   # KEYS=(${!MYMAP[@]}) = a normal array containing all the keys of the associative array. You can then refer to associative array with numerical index (0,1,2,etc)
declare -A MYMAP=( [foo a]=bar [baz b]=quux );for (( I=0; $I < ${#MYMAP[@]}; I+=1 )); do KEY=${KEYS[$I]};  echo $KEY --- ${MYMAP[$KEY]}; done
--> foo a --- bar
--> baz b --- quux

##Simulating 2d Arrays
declare -A arr
arr[0,0]=0
arr[0,1]=1
arr[1,0]=2
arr[1,1]=3
echo "${arr[0,0]}" # will print 0  
echo "${arr[0,1]}" # will print 1

##Associative Arrays over files / globbing
declare -A file_hash;for file in *; do file_hash+=([$file]=1);done
file_array=(*);declare -A file_hash=( $(echo ${file_array[@]} | sed 's/[^ ]*/[&]=&/g') )  #Not working on bash 4.4
declare -A filehash; eval $(printf 'filehash+=(["%s"]=1);' *.txt) #Works ok

------------------------------------------------------------------------------------------------------------------------
BASH:TIPS WHEREIS & WHATIS
whereis finds where is the executable of a programm (whereis sed). 
whatis shows one-line info of the program.

Trick : whatis /bin/* 2>&1 |grep -v "nothing appropriate" |grep "file" -> Scans the whole bin directory for all executables/commands 
excluding "nothing appropriate" that appears in execs without a single line description and matching file in description

Display a small message about installed bin files in usr/bin (and other folders)
find /usr/bin -type f -executable |xargs whatis -v 2>&1 |sed 's/ - /:/g' >whatis.log
mind xargs. Without xargs is not operating.

------------------------------------------------------------------------------------------------------------------------
BASH:HEREDOCS
Best explained : http://tldp.org/LDP/abs/html/here-docs.html
Basic format : cat <<EOF >file or >/dev/stdout or nothing=stdout

##Inside a script: 
when using here-doc format within a script, the input to cat comes from the script.
Example:
#! /bin/bash
l="line 3"
cat <<End-of-message
-------------------------------------
	This is line 1 of the message.
This is line 2 of the message.
This is $l of the message.
This is line 4 of the message.
This is the last line of the message.
-------------------------------------
End-of-message

when the script finishes above lines are printed in stdout.
If you apply cat <<-ENDOFMESSAGE (mind the dash) then white space is trimmed (except space)

##Tricky script usage:
You can use the here-doc format to comment big blocks of text or code for debugging.
format is :<<whatever ...... whatever
if instead of :<< you use cat << , everything bellow tags will be printed on screen or to >file if defined.

##Another trick usage inside script:
GetPersonalData ()
{
  read firstname
  read lastname
} # This certainly appears to be an interactive function, but . . .


Supply input to the VARIABLES of above function.
GetPersonalData <<RECORD001
Bozo
Bozeman
RECORD001
exit 0

Use a cat here-doc to insert a new line to the end of an existed file
cat <<EOF >>file.txt
This line will be appended to the end of file
EOF

Use cat to insert a line in the BEGINNING of the file:
cat <<EOF >file.txt
This line will go at the beginning
$(cat file.txt)
EOF

You can offcourse use tac instead of cat. But in tac lines of here-doc will be inserted from the last to the first.
This is what tac does = reverse of cat.

##Create Script from Script
Also see http://linuxcommand.org/wss0030.php

#!/bin/bash 
This is master script
Various code of master script
cat > /home/$USER/bin/SECOND_SCRIPT <<- EOT
#!/bin/bash
This is a secondary script generated by master script.
    # - This shall be the second script which automaticall gets placed elsewhere
    # - This shall not be executed when executing the main script
    # - Code within this script shall not appear within the terminal of the main script
	# Comment are also send to secondary script.	
    # Settings

    LOCALMUSIC="$HOME/Music"
    ALERT="/usr/share/sounds/pop.wav"
    PLAYER="mpv --vo null"
(more lines of code here)
EOT # Secondary script finished
Rest Code of Master Script Continues

##Source external code inside your script (instead of sourcing the whole script)
http://unix.stackexchange.com/questions/160256/can-you-source-a-here-document

source <(sed -n '/function justatest/,/\}/p' .bash_aliases) && justatest
The function justatest is sourced correctly.

More source examples:
source <(cat << EOF
A=42
EOF
)
echo $A --> prints 42

Alternative - Directly eval the code 
eval "$(sed -n '/function justatest/,/\}/p' .bash_aliases)" && justatest #worked fine

##Here Doc trick to assign strange chars to variable without escaping
$ IFS='' read -r -d '' var <<'EOF'
j!'^+%&/()=1!'^+%&/()c
EOF
$ echo $var --> Output = j!'^+%&/()=1!'^+%&/()c #This is hard to be assigned even with escaping.


##Combine array declaration with process substitution and here doc
readarray -t arr < <( del_comments <<EOF
a       # comm1
b       # comm2
# comm3
c
EOF
)

------------------------------------------------------------------------------------------------------------------------
BASH:OPTIONS  
Globbing ,bash filename expansion, bash options ans shopt options
Bash Debugging: http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_02_03.html#sect_02_03_02
The Set Builtin: https://www.gnu.org/software/bash/manual/html_node/The-Set-Builtin.html
The Shopt Builtin: https://www.gnu.org/software/bash/manual/html_node/The-Shopt-Builtin.html#The-Shopt-Builtin
Shell Expansion: http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_03_04.html
Globbing: http://www.tldp.org/LDP/abs/html/globbingref.html

Bash has by default enabled filename expansion.
This means that a simple echo a* will print all the files starting with a (if any).

This is why sometimes the command apt list a* prints results and sometimes not. 
If there is a file starting with a in the directory you run apt list a*, then a* is expanded due to bash filename expansion.
This was revealed using set -x on bash before command execution.
With -x bash informs you - prints out - the command that is going to be executed.

And this filename expansion confuses people since apt list a* is actually interpreted as apt list allfilesbeginningwitha, but apt list xfce* works without quotes if there are not files beginning with xfce.

You can disable this behavior using "set -f" , but this command will also disable the globbing in general, meaning that ls a* will result to literal a* and not global *

Or you can just run apt list "a*" and this will work fine.

Most used debuging commands: set -fvx (f for filename expansion disable, v for verbose, x for xtrace

To print all bash set parameters run #echo $SHELLOPTS && echo $-
Typical Output: 
braceexpand:emacs:hashall:histexpand:history:interactive-comments:monitor
himBHs

By command 'set' you get a variables list (including predefined startup functions of .bash_aliases file)

Print environmental variables : export -p , printenv or just env, ( set -o posix ; set ) , declare -p (or -xp)

To print all bash shopt parameters run #shopt. Combine with -s to see options set to ON or -u to see options set to OFF
Typical Output:
extquote       	on
force_fignore  	on
hostcomplete   	on
...................
dirspell       	off
dotglob        	off
execfail       	off
extdebug       	off
extglob        	off
failglob       	off
------------------------------------------------------------------------------------------------------------------------
BASH:SHELL OPTIONS - GLOB SPECIFIC OPTIONS
http://stackoverflow.com/questions/42835962/deleting-all-files-except-ones-mentioned-in-config-file/42839794#42839794
https://www.gnu.org/software/bash/manual/html_node/The-Shopt-Builtin.html
http://stackoverflow.com/questions/42969978/how-to-preserve-directory-structure-but-delete-some-of-its-contents-using-bas#comment73036677_42969978
http://www.linuxjournal.com/content/bash-extended-globbing

When we need to iterate through folders, we can enable bash globstar option using shopt -s globstar
Also BASH has some more options that affect globbing:

According to man bash:
.B globstar
   If set, the pattern ‘**’ used in a filename expansion context will match all files and zero or more directories and subdirectories.
   If the pattern is followed by a ‘/’, only directories and subdirectories match.
.B dotglob
   If set, Bash includes filenames beginning with a ‘.’ in the results of filename expansion.
.B extglob
   If set, the extended pattern matching features described above (see Pattern Matching) are enabled.
   Extended globbing Pattern Matching as described by the bash man page:
     ?(pattern-list)   Matches zero or one occurrence of the given patterns
     *(pattern-list)   Matches zero or more occurrences of the given patterns
     +(pattern-list)   Matches one or more occurrences of the given patterns
     @(pattern-list)   Matches one of the given patterns
     !(pattern-list)   Matches anything except one of the given patterns
   Most common use of extglob is using rm !(file1|file2|file*) in which rm will ignore the files given in the ( ) list.
   You can echo the rm command first, and if extglob is enabled and working even with echo the files to be deleted will be printed.
   More extglob examples:
     $ ls +(ab|def)*+(.jpg|.gif)   ==> equivalent to ls ab*.jpg ab*.gif def*.jpg def*.gif
     $ ls ab+(2|3).jpg             ==> This can not be done with regular globbing. Matches ab2.jpg, ab3.jpg, ab2222.jpg, ab333.jpg, etc.
     $ ls !(*.jpg|*.gif)           ==> list all files except those ending in jpg and gif. Impossible with normal globbing (we would use find intead)
     
##Example of iterating through files ignoring files/patterns stored in a file (gitignore case)
shopt -s globstar
declare -A filelist=()                                              # Create an associative array where filenames become array keys
for file in **; do filelist[$file]=1; done                          # Build a list of all files...
while read -r file; do unset filelist[$file]; done < .rmignore      # Remove files to be ignored.
echo rm -v "${!filelist[@]}"                                        # Delete what is left in the array keys

Remark: Needs testing to be sure that this method will correctly exclude patterns in .rmignore (i.e dir1/file*)

Alternative while loop above that will also use for (seems more correct - proposed by ghoti @ SO
while read -r glob; do for file in $glob; do unset filelist[$file];done;done < .rmignore
This method should work for sure even on patterns included in .rmignore due to the call of the for loop = glob iteration with pattern that was read by .rmignore file.
------------------------------------------------------------------------------------------------------------------------
BASH:FUNCTIONS AND ALIASES
##Alias
You can use alias some='your alias or your function here'
You can store all your custom alias in .bashrc file or even better in the .bash_aliases file.
Usually .bashrc file has a check for existance of .bash_aliases file, and if found , then it sources this file.
Check the .bash_aliases file for cool aliases.
To see current alias use "alias". To unset an alias use unalias <name>

##Functions
Can be declared either with function myf { } or directly as myf () {...}
Function can get arguments ($@, $*, $#,$1,etc) that can be used by function in the same way that a script receives arguments.

##functions in shell - like aliase
In the same file (.bashrc or .bash_aliases) you can have functions.
Those functions work directly from command line - can be exposed using set
Tip: You can not use the bash functions in a script, but you can source them by sourcing the .bash_aliases.

##Functions in scripts
See [: http://mywiki.wooledge.org/BashFAQ/084 :]
Functions you can have also in your scripts.
Functions can return as exit status an integer directly from number 1..255 . 
PS: Function return code an be captured by $? i suppose.
Indirectly , functions can return strings with simple echo if they are called in script mode : 
$(myfunction)  

The reason is that echoing within a function will send the data to stdout but in script mode $( ) echos will be caught by script buffer/pipe.
foo() {
   echo "running foo()..."  >&2        # send user prompts and error messages to stderr
   echo "this is my data"              # variable will be assigned this value below
}
x=$(foo)                               # prints:  running foo()...
echo "foo returned '$x'"               # prints:  foo returned 'this is my data'

Functions when called like $(foo) runs in subshell,  which means that any variable assignments, etc. performed in the function will not take effect in the caller's environment. 
Redirecting unwanted messages to >&2 will cause them to print on screen or wherever &2 might have been redirected, but will not captured by $( ) buffer/pipe.

Instead of capturing the function in a script format, you can use function to assign data to a global variable, and then in the main script you can just read the value of this variable.
foo() { return="this is my data";}; foo && echo "foo returned '$return'" 
This will print this is my data because function is not executed in a subshell but var $return is just read.

Tip:
It is not true that functions can not change global variables. Mind this test:
$ function foo { echo "i am inside function. a recevied = $a";a=$(($a*3));};a=1000;foo;echo "function changed a to :$a"
Will change the value of a from 1000 to 3000.
The "childs can not change parrents" rule applies only to subshells (i.e pipes)

##Function Local Variables. 
You can have local variables inside a function by assigning local var=value
If you don't specify local , then the var is considered as global, even if it is not previously defined in the main script.

##Use a function as a pipe
To do this all you have to do is to read the /dev/stdin , like for example var=$(</dev/stdin)
Then var can be used inside the function.
See this date to epoch function converter that works as a pipe:
function dtoe {
[[ -z $1 ]] && local dt=$(</dev/stdin) || local dt="$1" 
#if $1 is empty, use dev/stdin = work like a pipe. Otherwise use $1 as value for $dt
date -d "$(echo $dt | sed -e 's,/,-,g' -e 's,:, ,')" +"%s"
}

##How to ignore alias : http://mywiki.wooledge.org/BashFAQ/086
You might need to run a command and not it's alias that you might have set.
You can do that by unalias, or by $ command ls, or by \ls , etc

##Force Different behavior of functions/aliases if called by command line or script
With function you can have something like:
ls() { if test -t 1; then command ls -FC "$@"; else command ls "$@";fi;}
In this way you use the -FC args only if ls runs from a terminal and not from a script ; More generally when the &1 is not redirected to /dev/stdout


BASH:EXAMPLES
##NORMAL GLOBBING
http://stackoverflow.com/questions/20796200/how-to-iterate-over-files-in-a-directory-with-bash

for f in * , for f in */* , */*.txt , *.txt or ./*.txt , or ./*/*/*.txt (3rd sub dir in the tree), etc
for filename in /Data/*.txt; do
This method does not go automatically inside subdirs recursivelly 

##FIND ALL DIRECTORIES (AND/OR FILES) RECURSIVELLY
http://stackoverflow.com/questions/43134407/how-to-rename-directory-and-subdirectories-recursively-in-linux/43134693#43134693

Option 1 : Recurse using Find
while IFS= read -r -d '' subd;do 
  #do your stuff here with var $subd
done < <(find . -type d -print0)      #You can either apply -type f or no type at all

Option 2 : Recurse using Bash globstar option
shopt -s globstar
for subd in **/ ; do          # You can use ** instead of **/ to return both files and directories
  #Do you stuff here with $subd directories
done

With globstar you can limit the results to file only (like -type f of find) using **/*.txt or **/*.*
Mind that **/* will return also directories. This creates a bug : not possible to isolate extensionless files
Mind also that this format is acceptable for globstar :for ff in tmp/**/*.* or ./**/tmp/** 


Or you can use ** and filter the results: if [[ -e "$subd" ]];then #do commands for files;fi
Or you can use **/ to list directories and stepinto directories with CD and execute another for ./* loop.

##DOUBLE QUOTING FAILURES in GLOBING
for ff in ./**/tmp/** ;do echo Type - "$(file "$ff")" ;done  ---> Type - ./tmp/tmp2/file09 99: empty
for ff in ./**/tmp/** ;do echo Type - "$(file $ff)" ;done    ---> 
Type - ./tmp/tmp2/file09: cannot open `./tmp/tmp2/file09' (No such file or directory)
99:                cannot open `99' (No such file or directory)
