WEBSERVER:BASICS: CHEAT SHEET by gv

##WEBSERVERS (APACHE2 , NGINX)
Both Apache2 and nginx (engine x) are web servers - they serve the same purpose but have different philosphy , settings, etc.

.B APACHE2

.I Apache2 Installation
sudo apt update 
sudo apt install apache2
sudo systemctl status apache2   #You should see that apache2 is running and listening on default port 80

PS: typical apache2 installation folder is /etc/apache2

.I Apache2 Important Directories and Files:
In /etc/apache2 you can find apache2 global conf files, module directories including module configurations, sites directories.

/etc/apache2/apache2.conf:      The main Apache configuration file (global settings that apply to the whole server)
                                $ cat /etc/apache2/apache2.conf |grep -v -e '^$' -e '^#'  #skip empty and commented lines
/etc/apache2/ports.conf:        Configuration for ports Apache listens on.
/etc/apache2/sites-available/:  Directory for virtual host configuration files.
/etc/apache2/sites-enabled/:    Directory containing symbolic links to enabled virtual host files.
/etc/apache2/mods-available/:   Directory for available Apache modules.
/etc/apache2/mods-enabled/:     Directory containing symbolic links to enabled modules.
/var/www/html/:                 The default document root (where your website files go).
/var/log/apache2/:              Directory for Apache log files (error.log and access.log).


Tip1: /etc/apache2/sites-available/***.conf: Virtual Host Configuration. This file defines a virtual host. 
                                             Virtual Host allows you to run multiple websites on a single server, each with its own configuration.
                                             For this to work you need different domain names. Apache2 will read domain name in the header of the request
                                             and will serve the corresponding directory to the requestor (web browser).  
                                             Every apache2 installation includes 000-default.conf file in /etc/apache2/sites-available.

Tip2: conf/mods/sites directories are following the "-available" & "-enabled" logic.
In most of the cases when you enable a site/conf/module it creates a sym link from -enabled to -available directory.
This allows you to have i.e 5 site configurations in /etc/apache2/sites-available (5 different sites hosted) but you can enable i.e only two of them.
In any case you should not create sym links on your own but you should enable/disable sites,modules and configurations with apache2 "a2" commands.

Tip3: List of Apache2 Commands (run directly in your shell)
    Command     Function                                                        Operates On
    a2enmod     Enables an Apache module                                        Modules
    a2dismod    Disables an Apache module                                       Modules
    a2query     Used with switch -m <module>: Checks if module is enabled       Modules
    apache2ctl  Used with switch -M: Lists all currently enabled modules.       Modules
    a2enconf    Enables an Apache configuration file                            Configuration Files
    a2disconf   Disables an Apache configuration file                           Configuration Files
    a2ensite    Enables an Apache virtual host (site)                           Virtual Hosts
    a2dissite   Disables an Apache virtual host (site)                          Virtual Hosts
    a2query     Used with switch -s <site>: Checks if a site/host is enabled    Virtual Hosts

Tip4:   Enabling an apache2 module does not automatically enable the conf file of this module, which has to be enabled separately (if exists).
        To get a list of the modules available on your system, you can use the following command:
            apache2ctl -M   #This will list all loaded (enabled) modules. 
        To see all available modules (enabled and not enabled), you can look in the /etc/apache2/mods-available directory. 
        Each file in that directory (ending in .load or .conf) represents a module.
        Enabling unnecessary modules can have a performance impact. Only enable the modules you actually need.
        Some modules depend on other modules. For example, ssl requires socache_shmcb and other modules. 
        a2enmod will usually handle these dependencies automatically.
        MPMs modules (mpm_prefork, mpm_worker, mpm_event) are mutually exclusive. You can only have one MPM enabled at a time.

Tip5: Testing Apache2 configurations
When you have set up everything , you can test your configuration like bellow:
    sudo apachectl configtest
    sudo systemctl restart apache2
You can also visit http://localhost or http://127.0.0.1 from your own server (using a web browser) to verify that apache2 is working.

.I Details how to enable Virtual Hosts (web sites) - 1 site only
All apache2 installations include the file /etc/apache2/sites-available/000-default.conf
Which usually looks like this:
    <VirtualHost *:80>
        #ServerName www.example.com
        ServerAdmin webmaster@localhost
        DocumentRoot /var/www/html
        ErrorLog ${APACHE_LOG_DIR}/error.log
        CustomLog ${APACHE_LOG_DIR}/access.log combined
    </VirtualHost>

When i built a project i prefer to create a folder like this /var/www/html/myproject 
in /myproject/ i hold all the project files (node.js , html , css , js scripts, etc).

So in case that your web server machine is hosting only one site, you can edit this 000-default.conf like bellow:
    DocumentRoot /var/www/html/myproject

If your server is assigned to a domain name you can also uncomment this line:
    ServerName www.myproject.net

.I Enabling Virtual Hosts (sites) - More than 1 site
You can host more sites in the same machine under different directories:
Project A : /var/www/html/myprojectA
Project B : /var/www/html/myprojectB

In this case you have to create two new conf files inside /etc/apache2/sites-available/ directory.
sudo nano /etc/apache2/sites-available/myprojectA.conf
    <VirtualHost *:80>
        ServerName myprojectA.net
        DocumentRoot /var/www/html/myprojectA
    </VirtualHost>

sudo nano /etc/apache2/sites-available/myprojectB.conf
    <VirtualHost *:80>
        ServerName myprojectB.org
        DocumentRoot /var/www/html/myprojectB
    </VirtualHost>

You have then to enable those sites / virtual hosts:
    sudo a2ensite degermonitor.conf
    sudo a2ensite thingthis.conf
Tip: In reality a2ensite just creates links inside /etc/apache2/sites-enabled to /etc/apache2/sites-available.
Tip: file 000-default.conf needs no modifications - can remain blank or typical pointing to /var/www/html

With this way: 
    (a) when you visit www.myprojectA.net it will load the files / data available in /var/www/html/myprojectA
    (b) when you visit www.myprojectB.org it will load the files / data available in /var/www/html/myprojectB
    (c) when you visit http://IPADDRESS of your web server, then file 000-default.conf will come into play and serve default dir == /var/www/html
        Alternatively, you can add this line to 000-default.conf in order to redirect requests to the project you want to be the "default" project:
            Redirect permanent / http://myprojectB.org/
    

.I User Apache2 as a reverse proxy for communication between front end (html/js) to backend (node.js, etc).
Usefull when your server (back-end == node.js) runs on a custom port (i.e 3000) to serve end points in front-end (html, js).

For example instead of using api calls from frontend like fetch('http://XXX.XXX.XXX:3000/fetch-data') you can just use fetch('/fetch-data')

Anotherway would be to adjust your node.js at back-end to use the same port as your webserver (80 or 443) but this raises a lot of security concers.
The best practice is to use reverse proxy configuration in your Apache2 / site conf file.

Tip: For Apache2 reverse proxy to work, you need to enable proxy modules of apache2:
        sudo a2enmod proxy proxy_http proxy_ajp
        sudo systemctl restart apache2

As a general idea you don't mess up with 000-default.conf and you should have a separate conf file for your project, something like
/etc/apache2/sites-available/myprojectA.conf

<VirtualHost *:80> # Or *:443 for HTTPS
    ServerName myprojectA.net # Replace with your domain
    #ServerName XXX.XXX.XXX.XXX # Use IP if you don't have a domain name yet
    
    # Set web server root to your website's root directory
    DocumentRoot /var/www/html/myprojectA

    # Reverse proxy configuration
    # Forward requests to /api to your backend
    ProxyPass /api http://localhost:3000/api
    # Important for redirects and headers
    ProxyPassReverse /api http://localhost:3000/api

    # If your backend is on a different server:
    # ProxyPass /api http://backend_server_ip:3000/api
    # ProxyPassReverse /api http://backend_server_ip:3000/api
    # ... other virtual host configurations
</VirtualHost>

If in your server you have only a web site , is a good idea instead of creating a new file to modify 000-default.conf.
If you prefer to have a new conf file , is a good idea to disable your 000-default.conf (sudo a2dissite 000-default.conf).
If you create a new conf file based on ip access you can do something like this:

sudo nano /etc/apache2/sites-available/51-21-41-110.conf
<VirtualHost *:80>
    ServerName 51.21.41.110
    DocumentRoot /var/www/html/frontend

    ProxyPass /fetch-data http://localhost:3000/fetch-data
    ProxyPassReverse /fetch-data http://localhost:3000/fetch-data
</VirtualHost>

sudo a2ensite 51-21-41-110.conf  # If you named the file 51-21-41-110.conf
#or sudo a2ensite frontend.conf If you named the file frontend.conf
sudo systemctl restart apache2
sudo a2dissite 000-default.conf  # disable the default conf if not needed.

.I Proxying multiple api calls to the same back-end server
You absolutely do not need to create separate `ProxyPass` entries for each endpoint if they all point to the same backend server. 
**The Trick: Proxying a Common Path or the Root:**

The key is to proxy a common path that encompasses all your API endpoints.

For this to work , all your API endpoints should start with a common path segment (e.g., `/api`), and then you can proxy that path:

<VirtualHost *:80>
    ServerName yourdomain.com # Or your IP address
    DocumentRoot /var/www/html/frontend
    ProxyPass /api http://localhost:3000/ # Proxy all requests starting with /api
    ProxyPassReverse /api http://localhost:3000/
</VirtualHost>

With this configuration:

*   `fetch('/api/fetch-data')` will be proxied to `http://localhost:3000/fetch-data`.
*   `fetch('/api/load-files')` will be proxied to `http://localhost:3000/load-files`.
*   `fetch('/api/save-files')` will be proxied to `http://localhost:3000/save-files`.

See bellow real life examples for more examples and tested use cases

.B Restarting Apache2
sudo systemctl restart apache2
sudo system apache2 stop && sleep 5 && sudo system apache2 start

.B Real Life Examples 
In my server, without owning a domain name , i used to have under /var/www/html two subfolders:
    /thingit
    /deger
I used to access my deger project files with command http://51.21.41.110/deger/degerfetcher.html

My default apache2 conf file at available-sites (000-default.conf) was like bellow:
<VirtualHost *:80>
	ServerAdmin webmaster@localhost
	DocumentRoot /var/www/html
	ErrorLog ${APACHE_LOG_DIR}/error.log
	CustomLog ${APACHE_LOG_DIR}/access.log combined
</VirtualHost>

I modified 000-default.conf file like bellow:
<VirtualHost *:80>
	ServerAdmin webmaster@localhost
	DocumentRoot /var/www/html/deger
    # Directory Index loads files in order of preference
	DirectoryIndex degerfetcher.html index.html
	ErrorLog ${APACHE_LOG_DIR}/error.log
	CustomLog ${APACHE_LOG_DIR}/access.log combined
</VirtualHost>

Changes:        DocumentRoot has been changed to /var/www/html/deger directory.
                DirectoryInder defines the file to be loaded by default (if ommitted, index.html will be used as default file by apache2)
Result:         Now i can type in my browser http://51.21.41.110 and my degerfetcher.html page will be loaded. Also the links / api calls seems to work correctly.
Bug:            There is no way to access my files in my second project thingit (thingit foled under /var/www/html).
Bug Solution:   (a) If you have domain names available , you can create two conf files , each for each domain, pointing to the required Document Root.
                (b) If you don't have domain names, you can include in your 000-default.conf file a line like bellow:
                    Alias /thingit /var/www/html/thingit  # Alias for thingit
                With this line you bind http://51.21.41.110/thingit to /var/www/html/thingit directory / project.

.I Working Site Configuration with reverse proxy in my web server
1. First Enable proxy modules on apache2
    sudo a2enmod proxy
    sudo a2enmod proxy_http
    
    sudo systemctl restart apache2

2. Finalize 000-default.conf as bellow:

$ cat /etc/apache2/sites-available/000-default.conf |grep -v '^[[:space:]]*[#]'
<VirtualHost *:80>
	ServerAdmin webmaster@localhost
	DocumentRoot /var/www/html/deger
	DirectoryIndex degerfetcher.html index.html
    <Location /api> 
       	ProxyPass http://localhost:3000/api
       	ProxyPassReverse http://localhost:3000/api
	</Location>
	ErrorLog ${APACHE_LOG_DIR}/error.log
	CustomLog ${APACHE_LOG_DIR}/access.log combined
</VirtualHost>
PS: Above configuration was tested and worked fine for front end call fetch('/api/get-max-wind-data') and back end app.get('/api/get-max-wind-data')

2.B You can get also a working reverse proxy with bellow conf file, without using <Location> tag 
<VirtualHost *:80>
	ServerAdmin webmaster@localhost
	DocumentRoot /var/www/html/deger
	DirectoryIndex degerfetcher.html index.html
   	ProxyPass /api http://localhost:3000/api
   	ProxyPassReverse /api http://localhost:3000/api
	ErrorLog ${APACHE_LOG_DIR}/error.log
	CustomLog ${APACHE_LOG_DIR}/access.log combined
</VirtualHost>
PS1: Above configuration was also tested and worked fine for front end call fetch('/api/get-max-wind-data') and back end app.get('/api/get-max-wind-data')
PS2: Mind the absence of "/" in the end of http://localhost:3000/api - this absence has been done on purpose.

3. Adjust front-end api calls to include /api like this:
    const response = await fetch('/api/get-data');
                
4. Adjust back-end node.js end points to also include /api like this
    app.get('/api/get-data', (req, res) => {

5. Test externally with Browser by hitting directly to http://xxx.xxx.xxx.xxx/api/get-data
You should see JSON data in your browser flowing / received ==> endpoint response is reaching you == this is good.

6. Test internally at a different server window
curl http://localhost/api/get-data
Since we ommit the port (http://localhost:3000), we reach endpoint /api/get-data by the apache2 reverse proxy.
If we use curl http://localhost:3000/api/get-max-wind-data i suppose we talk directly to the back-end node.js server without proxy.

Tip1: Using /api for calling node.js at back-end and front-end as above, you achieve:
(a) speed up the loading of static pages in your project directory (i.e anotherpage.html, myscript.js, mystyle.css, etc) 
(b) "proxying" to back-end server localhost:3000 only /api/endpoint requests and not everything
(c) Programming discrimination between api calls and static files loading from DocumentRoot (i.e secondpage.html, myscript.js, mystyle.css, etc).
(d) With above configuration, direct api calls from front-end like:
    fetch(`http://51.21.41.110:3000/get-data-for-date?date=${date}`)
    Will still work , if node.js at back-end has /get-data-for-date endpoint.

Tip2: You can use reverse proxy with a setup like bellow, without using <Location>
            ProxyPass /api http://localhost:3000
            ProxyPassReverse /api http://localhost:3000
      Front End api calls should include /api i.e fetch(/api/get-data).
      Back End node.js endpoints does not require /api i.e app.get('/get-data')
      Think this proxying thing as a kind of alias.
      This method is not proposed since it can cause quite of confusion having different naming in api calls at front-end vs back-end.
      On the other hand, if your node.js is already built in the format app.get('/get-data'), this method helps not to rewrite node.js app.get commands.
      You only need to rewrite front-end html & js fetch to include /api in the end points to fetch.

Tip3: Some tutorials might propose the following configuration for reverse proxy (without <Location>)
            ProxyPass / http://localhost:3000
            ProxyPassReverse / http://localhost:3000
      In this case, apache2 will forward (proxying) ALL requests to localhost:3000 (due to / = root rule), including api calls and static files call (**)
      Api calls will be handled correctly by back-end server (*), but static files will fail, unless node.js at back-end is configured to serve static files.
      Even if node.js is capable / configured to handle static files requests at document root, this approach will slow down performance.
      (*) To have correct api calls, front-end calls and back-end endpoints should match, like bellow:
            fetch('/get-wind-data') at front-end javascript
            app.get('/get-wind-data') at back-end node.js
      (**) To be checked if finally proxying / will forward ALL requests including or excluding static files requests to http://localhost:3000

Tip4: After adding to my 000-default.conf this line -> DocumentRoot /var/www/html/deger 
i was not capable to open my html files using the classic method http://51.21.41.10/deger/degerfetcher.html  and I had to use only http://51.21.41.10/ 
Though, i added an Alias command in apache2 conf file and now i can use again http://ipaddress/deger/degerfetcher.html (and the various degerfetcherXX htmls)
    Alias /deger /var/www/html/deger 

Tip5: Apache2 supports the usage of .htaccess file - This file works on directory basis and allows configuration commands that should be applied to the directory.
in the main .conf of apache2 (i.e 000-default.conf) you have to explicitly allow conf file overrides by .htaccess file in directory with directive AllowOverride.

Tip6: Final working site conf file in my web server:
$ cat /etc/apache2/sites-enabled/000-default.conf |grep -v '^[[:space:]]*#'
    <VirtualHost *:80>
        ServerName 51.21.41.110
        ServerAdmin webmaster@localhost
        DocumentRoot /var/www/html/deger
        DirectoryIndex degerfetcher.html index.html
        ProxyPass /api http://localhost:3000
        ProxyPassReverse /api http://localhost:3000
        Alias /deger /var/www/html/deger
        Alias /thingit /var/www/html/thingit
        ErrorLog ${APACHE_LOG_DIR}/error.log
        CustomLog ${APACHE_LOG_DIR}/access.log combined
    </VirtualHost>

.B Important : Never add comments at the end of the commands - this causes huge problems. Comment out lines with # at the beginning.

Tip7: List of available commands in apache2 conf files
Directive	                            Description
ServerAdmin                             Email address of the server administrator.
DocumentRoot                    	    The directory from which Apache serves files.
ServerName                  	        The primary domain name or IP address for the virtual host.
ServerAlias	                            Alternative domain names or IP addresses for the virtual host.
ErrorLog	                            Path to the error log file.
CustomLog	                            Path to the access log file and the log format.
DirectoryIndex	                        List of files to look for when a directory is requested (e.g., index.html, index.php).
<Directory> ... </Directory>	        Defines configuration directives that apply to a specific directory or directory tree.
<Files> ... </Files>	                Defines configuration directives that apply to specific files.
<FilesMatch> ... </FilesMatch>	        Defines configuration directives that apply to files matching a regular expression.
<Location> ... </Location>	            Defines configuration directives that apply to specific URLs or URL patterns.
<LocationMatch> ... </LocationMatch>	Defines configuration directives that apply to URLs matching a regular expression.
Alias	                                Maps a virtual directory to a physical directory on the server.
AliasMatch	                            Maps virtual directories matching a regular expression to physical directories.
RewriteEngine       	                Enables or disables the URL rewriting engine.
RewriteCond         	                Defines conditions for URL rewriting rules.
RewriteRule	                            Defines URL rewriting rules.
ProxyPass	                            Configures reverse proxying for specific paths.
ProxyPassReverse	                    Configures reverse proxying for redirects and cookies.
AddDefaultCharset       	            Sets the default character set for responses.
AddType	                                Maps file extensions to MIME types.
Options                                 Controls various directory-specific options (e.g., Indexes, FollowSymLinks, ExecCGI).
AllowOverride                           Controls which directives can be overridden by .htaccess files.
Require                                 Controls access to resources based on various criteria (e.g., IP address, host name, authentication).
LogLevel                                Sets the verbosity level of error logs.
KeepAlive                               Enables persistent connections (keep-alive).
MaxKeepAliveRequests                    Sets the maximum number of requests allowed per persistent connection.
KeepAliveTimeout                        Sets the timeout for persistent connections.
Timeout	                                Sets the timeout for requests.
UseCanonicalName	                    Determines whether Apache should use the ServerName or the hostname from the client's request when creating self-referential URLs.
ServerSignature	                        Controls whether Apache displays server information on error pages.

.B Deny Directory Listing
To protect your server by cyber attacks , it is mandatory to disable directory listing that is enabled by default.
You need to sudo nano /etc/apache2/apache2.conf and add bellow sections in a parent - child inheritance.
Initially, apache2.conf has this code:
    <Directory /var/www/>
        Options Indexes FollowSymLinks
        AllowOverride None
        Require all granted
    </Directory>
    
And has to be modified like bellow:
    <Directory /var/www/>
        Options -Indexes +FollowSymLinks
        AllowOverride None
        Require all granted
    </Directory>

    <Directory /var/www/html/>
        Options -Indexes +FollowSymLinks
        AllowOverride None
        Require all granted
    </Directory>

    <Directory /var/www/html/deger>
        Options -Indexes +FollowSymLinks
        AllowOverride None
        Require all granted
    </Directory>

The keyword  "-Indexes" due to minus in front disables the Indexes option == directory listing.
When directory listing is forbiden you get a "forbiden" message in browser (and not 500 - Server Error or 404 - Not Found).

Similarly, if you want to allow directory listing -- > "Options +Indexes +FollowSymLinks"
Plus sign infront of Indexes enables this option - no sign at all also enables this option.

Keyword "AllowOverride None" forbids .htaccess files inside directories to override apache2.conf settings for this directory. 
(apache2.conf has global priority over the local .htaccess settings)

Tip: I got some error messages in /var/log/apache2/error.log related to misconfigured roundcube.conf (inside /etc/apache2/sites-enabled).
To avoid conflicts i have disabled this roundcube.conf (using a2dissite roundcube.conf).

<p> Require all granted
When you include Require all granted inside a <Directory> block it ensures that any request to access that directory is permitted without restriction. 
Essentially, it means no access limitations are applied—any user or client can access the resources in that directory.

#Alternative Options:
    Require all denied
    Require ip 192.168.1.0/24
    Require not ip 10.0.0.0/8
    Require host example.com
    Require not host badhost.com

#User Authentication
Require users to log in with a username and password, prompts users for credentials, granting access only to authenticated individuals.

    <Directory /var/www/html/deger>
        Options -Indexes +FollowSymLinks
        AllowOverride None
        AuthType Basic
        AuthName "Restricted Area"
        AuthUserFile /path/to/.htpasswd
        Require valid-user
    </Directory>

For this to work, you also have to create an .htpasswd file.
Use the htpasswd utility to create a file containing the usernames and passwords for authentication.
    htpasswd -c /path/to/.htpasswd username

Replace /path/to/.htpasswd with the actual path where you want to store the password file.

    PS1: Instead of AuthType Basic you can use more auth-type options for better security.
    #Digest Authentication - It hashes credentials during transmission, making it less vulnerable to interception. 
        AuthType Digest
        AuthName "Restricted Area"
        AuthDigestDomain /var/www/html/deger
        AuthDigestProvider file
        AuthUserFile /path/to/.htdigest
        Require valid-user

    You'll need an .htdigest file, which is created using the htdigest utility:
        htdigest -c /path/to/.htdigest "Restricted Area" username

    #Certificate-Based Authentication - using SSL/TLS certificates typically for clients accessing your site securely.
    Users must present a valid client certificate issued by your Certificate Authority.
        SSLEngine on
        SSLVerifyClient require
        SSLVerifyDepth 1
        SSLCACertificateFile /path/to/ca.crt

    #Database Authentication
    Authenticate users via a database backend by integrating third-party modules like mod_authn_dbd. 
    The database must be preconfigured to store usernames and hashed passwords.
        AuthType Basic
        AuthName "Database Authentication"
        AuthBasicProvider dbd
        AuthDBDUserPWQuery "SELECT password FROM users WHERE username = ?"
        Require valid-user

    #LDAP Authentication - If you have a centralized user directory like LDAP, you can authenticate users against it.
    You'll need to set up LDAP credentials and server access.

    #OAuth/OpenID Connect
    For modern applications, integrating OAuth or OpenID Connect for authentication is a great option. 
    This requires additional modules like mod_auth_openidc

    #Custom Authentication Modules - build or use third-party modules to create custom authentication mechanisms that fit your specific needs.

    PS3: Don't forget sudo systemctl restart apache2 to reload new configurations....
</p>
#Deny Specific Files or Folders
You can restrict access to certain sensitive files while keeping others accessible:

    <Files "sensitive_file.html">
        Require all denied
    </Files>

<Files> directive can be inserted within a <Directory> block in apache2.conf or it can be inserted outside of <Directory> block for a "global" rule.  
Outside blocks, operates independently and applies to specific files regardless of their location within the directory structure.
This will ensure that any attempts to access sensitive_file.html are denied globally across all directories. It's concise and straightforward.

Using <Files> inside <Directory> block, will restricts access to sensitive_file.html only within /var/www/html/deger. 
It won't apply globally—useful if the file name might exist in multiple directories with different access rules.
Example:
    <Directory /var/www/html/deger>
        <Files "sensitive_file.html">
            Require all denied
        </Files>
    </Directory>

PS: <Files> vs <Directory>
Use <Directory> when you want to control access to an entire directory or path.
Use <Files> or <FilesMatch> when targeting individual files or matching specific file patterns (e.g., based on file extensions or names).

##PHP
In Debian 12 , the php version available is 8.2
All bellow commands should be run as root or with sudo.
The key is that except installing php in your server machine, you have to configure your webserver (apache2) to load the necessary php modules, configurations etc.

Source: https://www.installconfig.com/doku.php?id=configure_ubuntu_server_20_04_host_nextcloud_hub_21

Step 1: apt install php libapache2-mod-php libapache2-mod-fcgid php-fpm 
Tip: You can just apt install php to install typical php that will be handled by Apache2.

Step 2: write a simple test.php file inside /var/www/html (default folder), containing just those two lines:
<?php
phpinfo();

Step 3: open a php interactive shell with php -a and run phpinfo();
If you see output, you have confirmed that php (as languge) is installed in your system.

Step 4: geany /etc/apache2/apache2.conf
To the end of file , add this line:
SetHandler application/x-httpd-php

Step 5: It is a good idea, though not well understood why in this point, to enable php-fpm (CGI Web Server Support) since it is claimed that CGI has better / faster php support over Apache2. 
To enable php-fpm you have to disable older php modules on Apache2 and load new php-fpm modules:
a2dismod mpm_event
a2enmod mpm_prefork
a2enmod php8.2
a2enmod proxy_fcgi setenvif
a2enconf php8.2-fpm
a2query -m php8.2 
#a2query is just a question to the system / query - you expect to get a positive confirmation in your query


Step 6: Edit the php configuration file and comment out / disable the following lines:
geany /etc/apache2/mods-enabled/php8.2.conf
# Running PHP scripts in user directories is disabled by default
# 
# To re-enable PHP in user directories comment the following lines
# (from <IfModule ...> to </IfModule>.) Do NOT set it to On as it
# prevents .htaccess files from disabling it.
#<IfModule mod_userdir.c>
#    <Directory /home/*/public_html>
#        php_admin_flag engine Off
#    </Directory>
#</IfModule>

Step 7: systemctl restart apache2

Step 8: Navigate to http://127.0.0.1/test.php and everything should work ok.
PS: Considering that you had a working apache2 server , meaning that you have a default index.html file and meaning that visiting http://127.0.0.1 works ok.

Step 9: It seems good option to install some php extensions in your system:
sudo apt install php-gd php-mysql php-curl php-mbstring
sudo apt install php-intl php-gmp php-bcmath php-xml phpunit
sudo apt install libapache2-mod-php php-zip php-imagick php-apcu

##PHP Composer
composer is a kind of php libraries installer.

.B Installing Composer according to https://getcomposer.org/
php -r "copy('https://getcomposer.org/installer', 'composer-setup.php');"
php -r "if (hash_file('sha384', 'composer-setup.php') === 'e21205b207c3ff031906575712edab6f13eb0b361f2085f1f1237b7126d785e826a450292b6cfd1d64d92e6563bbde02') { echo 'Installer verified'; } else { echo 'Installer corrupt'; unlink('composer-setup.php'); } echo PHP_EOL;"
php composer-setup.php
php -r "unlink('composer-setup.php');"

If everything is ok, you should receive a success confirmation

Last step: sudo mv composer.phar /usr/local/bin/composer


##PHP MODBUS
As of today (19.10.23) there is a Github Project capable to provide libraries for TCP Modbus Communication.
https://github.com/aldas/modbus-tcp-client

.B Use PHP built-in webserver for PLC Communication

sudo apt-get install php-mbstring
sudo apt install php-curl
sudo apt install php-xml
sudo apt install phpunit
sudo service apache2 restart
composer update OR sudo /home/gv/composer.phar update
composer require aldas/modbus-tcp-client

git clone https://github.com/aldas/modbus-tcp-client.git
cd modbus-tcp-client
composer install
cd modbus—tcp-client/examples && touch .allow-change

File .allow-change is used as a flag, to allow users to change IP address / IP Port.
Without this file present IP address and IP Port of webserver will be grayed out / locked/hardcoded in index.php file.
File .allow-change can be empty.

.B Run the webserver
php -S localhost:8080 -t examples/

-S switch: Run Built-in php webserver
-t switch: Define the root directory of PHP webserver. If not provided cwd will be used as root directory
Now open http://localhost:8080 in browser. See additional query parameters from index.php.

Tip:
aldas "Examples" folder has index.php inside. 
index.php is used with php built-in web server to test out communication with our own PLCs.

Tip2: php -S localhost:8080 /path/to/index.html -> This is a valid synthax to serve a particular file with php -S

##NODE JS
.I https://phoenixnap.com/kb/debian-install-nodejs
.I https://deb.nodesource.com/

.B Install nodejs on Debian & Usefull nodejs commands:
apt install nodejs                  # installation of nodejs core
apt install npm                     # npm is a package manager for nodejs
curl https://raw.githubusercontent.com/creationix/nvm/master/install.sh | bash  # This will install nvm
source ~/.profile                   # after installation of nvm
nvm ls-remote                       # list available nodejs versions
nvm install <nodejs version>        # install nodejs specific version, i.e nvm install 12.1
nvm ls                              # list locally installed versions
nvm use 18.1                        # use a specific nvm version
npm install module                  # npm = nodejs package manager- Can install various nodejs modules (i.e sudo npm install -g ws path http express body-parser yargs fs modbus-serial)
nodejs script.js                    # use nodejs to run file script.js on commandline

sudo npm install pkg                # install particular js packages required by your project. This has to be run in your project directory.            
sudo npm install -g pkg             # install pkg globally , system wide -> folder 
cd /path/to/project && sudo npm ls  # List all pkgs installed in your project directory
sudo npm ls -g                      # Lits all pkgs installed globally (/usr/local/lib) 

.B Websockets [ws module]
The use of websockets servers (i.e inside a server.js file) allows websocket clients to get connected and receive messages by the server.
An html page can be used as websocket client (using javascript).
As a result you can achieve to have html pages with content updated dynamically , as received (push) by the ws server.
Actually with node you can enclose in a simple file tcp (or express ) server, websocket server , http server.
For better results you could use different ports on each server.
Typical data flow inside server js:
* html page (ws client) connects to server ws server
* Json sending APIs get connected to express or tcp servers to post their values
* http is serving the html file you want.
* When new json data arrive in tcp/express then ws server push those data to ws clients and html pages refresh their content by pasring the json string received by the ws server.

Check out thingit server files for more.

.B INCLUDE
Very important, used to include external node modules and use their libraries.

Example of a file called config.js
exports.port = process.env.PORT || 3000;
exports.max_request_length = 100000; // The maximum length of characters allowed for a request or a response.
exports.max_requests_per_second = 10; // The maximum number of requests per second to allow from a given IP.

##Usage of config.js with your main node.js file:
var config = require("./config");

##Usage of config.js variables in main script file
function sendTooBigResponse(res) {
    return writeResponse(res, 413, "the content in the request or response cannot exceed " + config.max_request_length + " characters.");
}

 if (config.blacklist_hostname_regex.test(remoteURL.hostname)) {
            return writeResponse(res, 400, "naughty, naughty...");
        }

WEBSERVER:DOMAIN NAME REGISTRATION – WEBHOSTING – AWS CLOUD HOSTING

##Register a Domain Name
Though i am located in Greece, I didn’t want a .gr domain name but I would like a .io tld, which proved to be quite expensive compared to other tld types like .gr or .online.

I finally ended up with ‘.online’ domain registered with namecheap at 1/8 of Greek registrars price (~1 euro for 1 year). 
Greek Registrars require ~2.5€ per year for .gr domain or 8 euro for .online for 1 year  (ps: i paid just 1 euro for 1 year). 

To complete my domain name registration I used namecheap.
https://ap.www.namecheap.com/Domains/DomainControlPanel/thingit.online/advancedns 

After purchasing your required domain name you have to update dns records in order your domain name to point out to your web hosting server (aws in my case). 

All registrars provide a kind of web-based control panel for managing domain names purchased.
Tip: https://www.namecheap.com/support/knowledgebase/article.aspx/434/2237/how-do-i-set-up-host-records-for-a-domain/

##Hosting Types
a.	Shared Hosting -> The cheapest one – suitable for personal web sites / small business serving static web pages without big demands. The physical server resources (cpu, ram, net speed, bandwitch, etc) are shared to all the server web-pages/users. Performance can be dependent on the total server workload, meaning that other web-pages/users might affect the performance of the physical web server. 
It is quite classical in this type of Hosting the web page admin has not direct access to the server (i.e using ssh) and all the setup takes place by using the server control panel (i.e cPanel).

b.	VPS Hosting (Virtual Private Server) -> Is the next step of Shared Hosting, each user has it’s own virtual server (consider physical server split to different pieces / containers). In this case each “container” has a predefined amount of hardware to be used (cpu, ram, etc) and the performance is not affected by the other users/web-pages activities. 
In VPS it seems that ssh to your virtual server is possible. At least in AWS that was cool.
I could use the server terminal and install anything necessary.

.I Other VPS Services:
    * Cloudzy -> easy to deploy, low cost (~4 USD per month) - no free plan, much cheaper to the AWS paid version.
                 Supports remote terminal through cloudzy platform (web site), allows root login, 

c.	Dedicated Server Hosting -> One step further than the VPS, you do rent a full / complete server on your own and you are using 100% of the server hardware for your own web-pages. Obviously ssh (quite unrestricted I suppose) it is supported.

d.	Cloud Hosting – Not clear yet – seems to be a kind of VPS but running on the cloud, meaning that your web files might be copied to different servers in order guaranteed uptime to be achieved (if one cloud server fails due to error, overload etc then your web page traffic is redirected and served by another cloud server.

##Working with AWS (Amazon Web Services) for webhosting.
Guide: https://middleware.io/blog/virtual-machine-on-aws-guide/
Starting Point: https://portal.aws.amazon.com/billing/signup?#/start

Amazon Web Services (AWS) is actually providing a cloud based hosting with VPS.

Setup of aws seems to be a headache, but it proved that following the “guides” it works. 

First you have to create your aws account, providing also credit card details (prepaid cards seems to be approved) – credit card will be temporarily charged 1 € and then this 1€ will be refunded. 
If I am not wrong you can make use of aws for 1 year FREE.

Once everything is ready (user registered, email verification, credit card , etc) you arrive at “AWS Management Console” in where you can select if you want to deploy an EC2 Virtual Machines (called instances in aws world). 

Except seeting up a Virtual Machine, a lot of options exist in Management Console. 

Assuming that your target was to deploy a virtual machine, you have to select the correct instance type with the required AMI (Amazon Machine Image). I used Debian AMI since I am quite familiar with Debian.

Then you have to correctly set up your instance / virtual machine (see the detailed Guide) including security group adjustments that affect inbound and outbound rules. 

Once your instance is ready you can “Launch” your virtual machine and you get a confirmation message.

PS: Don’t neglect to set up a key for remote access (select “Create a new key pair”) in order to be able to get connected to your VM using encryption. It has been said that you can not get connected to instances with out a key pair.

On your dashboard, you can see all your instances, you can start / stop them and you can get connected to them.
Connecting to your instance proved to be a headache, but I got easily connected to my instance with ssh:
[*] sudo ssh -i "thingit-debian.pem" admin@ec2-51-21-41-110.eu-north-1.compute.amazonaws.com
[*] sudo ssh -i "thingit-debian.pem" admin@thingit.online

PS1: File ‘thingit-debian.pem’ was generated by AWS and was saved to my linux machine. You can copy-paste this pem file between Linux machines.

PS2: If you doubt about your site / aws beeing online scan it with nmap and not ping; ping seems that is blocked by aws.

PS3: It seems that is a good practice to change your ssh remote port to be different than port 22 since a lot of "hacking" tools automate scanning and btuteforcing 
at port 22. In case you have changed ssh port remember to use -p PORT with ssh and also don't neglect to open this custom PORT in your aws account to allow traffic.


Once connected to your VM by ssh you have raw terminal access to your virtual machine and you have to set up almost everything by the terminal (apache installation, php – perl – java – nodejs installation, etc).

.B Important info about PEM key files:
The remote access key first created in your aws machine setup is very important. 
In reality your aws machine admin/root account is binded to this very first key (pem file).
Is seems that even if you create additional key pairs, those pairs does not work to grant you admin/root access to your aws server.
The only way to gain access as admin/root in your aws machine is to have the very first pem file (ps: copy paste of this pem file to another linux machine works fine).
You can login to your aws server with additional pem key pairs, but you have to set up a user in your remote aws machine and create pem keys for those users.
The pem key file that grants admin/root access to your aws machine is unique and not easily replacable.
Removing the binding of your aws vm to this very first pem key is a real challenge with great failure probabilities.
-
##Assigning an Elastic (static) IP to you instance
Guide: https://aws.plainenglish.io/assigning-a-domain-name-to-an-aws-ec2-instance-via-elastic-ip-d2234b1662cc

Step 1: Allocate an Elastic IP
Go to the AWS Management Console and navigate to the EC2 Dashboard.
From the navigation bar, click on “Elastic IPs” under “Network & Security.”
Click on “Allocate new address” and choose “Amazon’s pool of IPv4 addresses.”
Click on “Allocate” to create a new Elastic IP.

Step 2: Associate the Elastic IP with your EC2 Instance
From the Elastic IPs list, select the newly allocated Elastic IP.
Click on “Actions” and choose “Associate IP address.”
In the “Associate Elastic IP address” dialog, select your EC2 instance from the drop-down menu.
Click on “Associate” to link the Elastic IP with your EC2 instance.

Step 3: Update DNS Records with your Domain Registrar
Log in to your domain registrar’s website (where you registered your domain).
Locate the DNS management settings for your domain.
Add an “A” record with a blank or “@” hostname, pointing to the Elastic IP address you just associated with your EC2 instance.
Save the changes to update your DNS records.

Step 4: Wait for DNS Propagation
After updating your DNS records, it may take some time for the changes to propagate across the internet. DNS propagation typically takes a few minutes to several hours, depending on various factors such as TTL (Time to Live) settings and caching.

Step 5: Verify Domain Name Resolution
Open a web browser and enter your domain name (e.g., example.com) in the address bar.
If everything is configured correctly, your website or application hosted on the EC2 instance should load successfully.

Step 6: Check Domain Resolution with NSlookup
Open a terminal or command prompt on your local machine.
Type the following command, replacing “example.com” with your domain name:

From this point and after connecting my domain name to aws instance, I could use ssh with something like this:
sudo ssh -i "/home/gv/Desktop/thingit-debian.pem" admin@thingit.online

This was my instance name earlier, before Elastic IP assignment: 
http://ec2-51-20-70-223.eu-north-1.compute.amazonaws.com/

.B Tip1: http access
If your instance in running , you have provided html files to /var/www/html directory but you are not able to see the html pages inside this folder, then you have to try with http instead of https.

.B Tip2: allow inbound traffic
If you want to use a custom port in your VM (i.e for a websocket server) you have to allow inbound trafic in this port by configuring Security Group Settings in your aws account/machine

.B Tip3: Server Login and File Copy
To login in your aws server by your local debian use this synthax:
[*] sudo ssh -i "/home/gv/Downloads/thingit-debian.pem" admin@thingit.online
thingit-debian.pem is the pem file of the very first time that a key pair to your aws server was created.

To copy files from your local machine to remote EC2 server you need to run scp like bellow:
[*] sudo scp -i "/home/gv/Downloads/thingit-debian.pem" myfile.html admin@thingit.online:/home/admin
Avoid copying files directly to /var/www/html/yourfolder since scp to root folders usually is not allowed.

.B Tip4: LocalHost
It is common to have a local node.js running on server (i.e node myfile.js) that will be opened for listening at a custom port like 3000 and the 
corresponding html file, will try to fetch data from http://localhost:3000/end-point.
The term "localhost:3000" cause a lot of problems when trying to access from outside world. 
It is better to replace the localhost with public ip of your server inside your html file.
So , instead of this : fetch('http://localhost:3000/list-files')
write this : fetch('http://51.21.41.110:3000/list-files') or fetch('http://thingit.online:3000/list-files')
Also better to adjust port 3000 to be open for incoming connections at your server configuration (though i am not sure if this is required).

.B Tip5:  For node.js + html troubleshooting: 
Login to your server with ssh by two terminals at the same time and go to /var/www/html.....
Terminal one: run node helloworld.js (you should see a message that server is listening on port 3000)
Terminal two: run links http://localhost:3000/list-files -> you should receive the hello world message received by node.js on terminal 1. 
In this case the term localhost works ok, since both terminals are connected to the same server and thus localhost has a meaning.
This is a test to make sure that node.js server is up and running at port 3000.

Then, you can then try to see if hello world message can be delivered by node.js in external browsers. 
Keep teminal 1 open, running with node.js -serving at port 3000 
Use any external web browser and try to open: 
http://thingit.online/isotec/helloworld.html (this is a small web page with a simple button that when pressed the end point /list-files is served = Hello World message
or try directly http://thingit.online:3000/list-files (you should see Hello World message directly in your browser)

.B Tip6: Permission Issues
Custom node.js scripts that are trying to save files to your web server root directories like /var/www/html/.... have write permission issues.
To overcome those issues, you have to start node.js script on your server with sudo (i.e sudo node myfile.js) to ensure write access to /var/www/html/....
Alternativelly i suppose you can adjust the /save-file and /load-file endpoints in your node.js file to use a different directory with write access for the user or for everybody.
As a general idea, being capable to write files to your root directories by a web html page that posts data to node.js , sounds a bad and vulnerable idea.

.B Tip7: Perform a task on remote server without keeping the ssh connection open:
[*] sudo ssh -i "/home/gv/Downloads/thingit-debian.pem" admin@thingit.online "cat /var/www/html/isotec/isotec2.js"

.B Tip8: Compare local files to server files:
[*] sudo ssh -i "/home/gv/Downloads/thingit-debian.pem" admin@thingit.online "cat /var/www/html/isotec/isotec2.js" |diff - ./isotec2.js

.B Tip9: Copy file to your aws server by local machine with a single command:
[*] sudo scp -i "/home/gv/Downloads/thingit-debian.pem" isotec3.js admin@thingit.online: && sudo ssh -i "thingit-debian.pem" admin@thingit.online "sudo cp -v /home/admin/isotec3.js /var/www/html/isotec/"

First command performs a secure copy using ssh with a special command called scp (usuall synthax is scp source-path target-path).
in our case target-path is admin@thingit.online:
The symbol : is equivalent to home directory of the logged in user  --> /home/admin
sudo scp -i "thingit-debian.pem" isotec3.js admin@thingit.online:   --> 
[*] sudo scp -i "/home/gv/Downloads/thingit-debian.pem" isotec3.js admin@thingit.online:/home/admin
PS: You can not perform scp to server root directories like /var/www/html , and this is why we need the second command - to copy the file from home directory to the /var/www/html directory.

.B Tip10: Use git for easy files update between local machine and remote aws server.

.B Update aws server with changes made in files of Local Machine:
[*] cd /home/gv/Desktop/deger && gitsend && sudo ssh -i "/home/gv/Downloads/thingit-debian.pem" admin@thingit.online -p 6111 "cd /var/www/html/deger/ && sudo git pull"

For this to work, both machines (local & remote server) should have git clone deger repository.

PS1: mind the different port used for ssh- this has be changed on purpose since a loot of tools automate attacks to default ssh port 22.
PS2: gitsend is one of my custom aliases: alias gitsend='git add . && git commit -m "update" && git push && git show --name-only'
PS3: git pull is standard git command - in server, this has been aliased to sudo git pull

.B Update local files with new files (i.e log files) created in your aws server.
Login to aws server, git send the files modified to git , pull the changes to local machine:
[*] sudo ssh -i "/home/gv/Downloads/thingit-debian.pem" admin@thingit.online "cd /var/www/html/deger/ && sudo git add . && sudo git commit -m 'update' && sudo git push && sudo git show --name-only" && cd /home/gv/Desktop/deger && git pull

.B Tip11: Using PM2 to manage your node apps
With PM2 (npm install pm2 -g) you can start your node.js app and keep it alive for ever.....
This process manager takes care to keep app running, including retarting in case of app crash.

[*] sudo pm2 stop isotec4.js && sleep 5 && sudo pm2 start isotec4.js --name isotec4 --time --output pm2logs/out.log --error pm2logs/error.log
You don't need to explicitly define /var/www/html/deger/pm2logs/out.log , since pm2 keeps the /var/www/html/deger path.

To troubleshoot logging (in custom directory) run 
[*] sudo pm2 logs
This will open the real logs , and the path of the real log will be displayed (PM2 default logging directory is /root/.pm2/logs)
You can now check if the real log path matches the log path you provided when your sudo pm2 start.

If PM2 ignores your --output and --error settings, bellow procedure usually helps to restore logging in your custom directory:
[*] sudo pm2 stop all
[*] sudo pm2 delete all
[*] sudo pm2 save --force
[*] sudo chmod -R 777 /var/www/html/deger/pm2logs
[*] sudo pm2 start isotec4.js --name isotec4 --time --output pm2logs/out.log --error pm2logs/error.log
[*] sudo pm2 save

PS: Things got complicated when you run sudo pm2 update.... Avoid to use it, till to be checked

More pm2 commands:
[*] sudo npm install -g pm2
[*] sudo pm2 start /var/www/html/isotec/isotec.js --name isotec --time --log pm2logs/out.log
[*] sudo pm2 start isotec3.js --name isotec3 --time --output pm2logs/out.log --error pm2logs/error.log
[*] sudo pm2 startup
[*] sudo pm2 monit
[*] sudo pm2 logs (or pm2 logs isotec4)

Start all processes: pm2 start all
Stop all processes: pm2 stop all
Restart all processes: pm2 restart all
Delete all processes: pm2 delete all

.B Tip12: Create a file like degerfetcher.html that will be actually a soft link to your latest degerfetcher html page.
With this way you don't have to worry what is the "latest" version of your html file.
Just make a link, and always open degerfetcher.html. Your browser will not notice any difference.
[*] sudo rm degerfetcher.html
[*] sudo ln -s degerfetcher10.html degerfetcher.html


##WEB:COOL TOOLS

##XTERM.JS
https://github.com/xtermjs/xterm.js
Library made to provide kind of terminal inside a web page. Install using npm install -g xterm

## SEASHELLS
https://seashells.io
Seashells is based on term.js and provides a web-based view-only terminal.
You can pipe data to seashells and this data will be published on a dynamic seashells page.

Usage:  echo 'Hello, Seashells!' | nc seashells.io 1337
Response: serving at https://seashells.io/v/{url}
htop | seashells --delay 5

seashells can be used to display on a web page output data of various scripts, i.e nodejs scripts.
Being able to see the results of node program.js in a web page, you don’t have to use ssh.

##Bastillion
https://github.com/bastillion-io/Bastillion
Bastilion is a full working web-based CLI, based on xterm.js
Depends on java sdk.
With Bastillion you can set up various terminals that will provide you remote CLI access.
It is like ssh but on a web page (!).


.B Installation
Download Bastillion from github: https://github.com/bastillion-io/Bastillion/releases
Download the latest tar file and extracted in a dedicated directory. Run it using ./startBastillion.sh

.B Using Bastillion
Open browser to https://<whatever ip>:8443

Login with
username:admin
password:changeme

Note: When using the AMI instance, the password is defaulted to the <Instance ID>. Also, the AMI uses port 443 as in https://<Instance IP>:443


##More cool resources based on xterm.js
https://github.com/xtermjs/xterm.js#real-world-uses
Check rtty: https://github.com/zhaojh329/rtty

##DNS Hosting
https://www.hostingkey.com/dnhosting.php
Register to hostingkey name server your web site (not obtained by hostingkey).

##WEB:SET UP YOUR OWN MAIL SERVER

##POSTFIX
Postfix on debian is a complete smtp package/server/client that can accept incoming emails and can send emails to the world.
You need to have your tcp port 25 (incoming) open to receive emails (verify this with online port scanner https://www.ipvoid.com/port-scan/)
You also need to open tcp port 25 (outgoing) for sending emails. This can be tested with Telnet (i.e telnet gmail-smtp-in.l.google.com 25 -> should proceed and not hanging)
PS: Amazon Web Services (AWS) do not allow traffic on outgoing port 25 so practically you can not send email by AWS with PostFix.
You can though overcome this issue by using sendinblue = Brevo.

You also have to adjust your DNS records (through your registrar, i.e namecheap) so the world to be able to find and use your mail server.
.I https://www.namecheap.com/support/knowledgebase/article.aspx/322/2237/how-can-i-set-up-mx-records-required-for-mail-service/

.B PostFix Installation
.I https://www.linuxbabe.com/mail-server/build-email-server-from-scratch-debian-postfix-smtp

    sudo apt install postfix libsasl2-modules     #Tip : Select Internet Email during Installation

Edit postfix configuration:
    sudo nano /etc/postfix/main.cf

Replace relayhost with this line:
    relayhost = [smtp-relay.sendinblue.com]:587

Then add the following lines to the end of this file.
    # outbound relay configurations
    smtp_sasl_auth_enable = yes
    smtp_sasl_password_maps = hash:/etc/postfix/sasl_passwd
    smtp_sasl_security_options = noanonymous
    smtp_tls_security_level = may
    header_size_limit = 4096000

Also add this line since absent of this line to avoid errors
    smtpd_recipient_restrictions = permit_sasl_authenticated
    
Then create the /etc/postfix/sasl_passwd file.
    sudo nano /etc/postfix/sasl_passwd

Add the SMTP relay host and SMTP credentials to this file like below. 
Replace smtp_username and smtp_password with your own username and password that are given by SendinBlue. 

    [smtp-relay.sendinblue.com]:587      smtp_username:smtp_password    #tip: in case of sending email problems, try to delete the port 587

Create the corresponding hash db file with postmap.
    sudo postmap /etc/postfix/sasl_passwd --> Now you should have a file /etc/postfix/sasl_passwd.db. 

Restart Postfix for the changes to take effect.
    sudo systemctl restart postfix

Tip:    By default, sasl_passwd and sasl_passwd.db file can be read by any user on the server. 
        Change the permission to 600 so only root can read and write to these two files.
            sudo chmod 0600 /etc/postfix/sasl_passwd /etc/postfix/sasl_passwd.db

Use ss (Socket Statistics) utility to verify that Postfix master process is listening on TCP port 25.
    sudo ss -lnpt | grep master

.B Sending Test Email
    sudo apt install bsd-mailx mailutils mutt
    echo "this is a test email." | mailx -r from-address -s hello to-address
    echo "test email" | sendmail your-account@gmail.com   #sendmail is provided by postfix package among other utils in /usr/sbin/
    echo "this is test message by thingit.online" |mutt -s "Wind Alert" -- ge.vasiliou@gmail.com    # -s = subject

Tip: 
Edit the Mutt Configuration File ~/.muttrc and set the Sender Name: 
    set from = "Your Name <your-email@example.com>"
This is the easiest way to define the Sender appearance in remote systems.

.B Reading New Emails
The inbox for each user is located at /var/spool/mail/<username> or /var/mail/<username> file. 
If you are unsure where to look for the inbox, use this command.
    sudo postconf mail_spool_directory

Email Readers:
Classic way: mail or mailx -> Provides a list , you open the email by providing the email id (number).
Nice Way: mutt             -> looks like a complete email client that runs on the terminal. You can open / read emails, reply, forward, compose new mails, etc

Tip1:   mail & mailx Display the full message with headers. There is not any way to display only the message body.
        On the other hand, mutt "hides" the header data and displays only the body message & attachments (if any).

.B Reading Saved Emails (in mailbox).
postfix creates a mailbox to store read emails under home directory of the user i.e /home/admin/mbox
You can open those emails by specifying the mailbox to mutt:
    mutt -f ~/mbox
    mail -f ~/mbox

##BREVO
Detailed Installation Instructions to make it work with PostFix:
.I https://www.linuxbabe.com/mail-server/set-up-postfix-smtp-relay-debian-sendinblue

Brevo is the evolution of "sendinblue" service, used old-days.
The main service that Brevo offers is an smtp relay.
This is quite usefull in case that your web hosting service (like AWS) does not allow outgoing traffic on port 25.
So actually you make an account to Brevo, you get your username and password , you provide those details to postfix and you are capable
to send emails from AWS EC2 Debian Server using Brevo smtp relay.

Brevo offers a free plan with 300 emails / day - good for sending newsletters in various customers / mailing lists.

For Brevo to work, you need to "add" your domain (thingit.online) on the Brevo trusted sites, in Brevo Settings (web page).
Brevo will automatically authorize your domain by detecting registrar (namecheap) and make the necessary DNS changes for smtp relaying.
PS: You need to allow Brevo to get connected to namecheap with your credentials to add the required DNS records.

While the smtp changes made by Brevo for smtp relay work fine, the MX DNS records registered by Brevo does not work as expected.

It has been noted that replying to an email (i.e using gmail) received by thingit gives error -- > 554 5.7.1 : Relay access denied

This was solved by adjusting DNS MX records at namecheap manually to directly point to thingit.online instead of using the Brevo MX settings.
After this change, you can reply to thingit.online emails by gmail, corporate emails, etc without problem.

Tip: With all this tests, it is clear that having only an smtp server for sending / receiving email is enough. You don't need POP3 settings.
POP3 is used by GUI Email Readers to retrieve the emails from the server.

Tip: Since postfix and brevo are bind together in the server (postfix uses the brevo username and password stored in server) you can send an email by
your Debian Server without the need to provide smtp username and password everytime.

##MAILGUN
.I https://www.npmjs.com/package/mailgun.js?activeTab=readme

    npm install mailgun.js form-data

If you need a way to send email notification by node.js , then you need to provide your smtp credentials for i.e gmail.
In this aspect, either you set up your own mail server / smtp server at thingit or you can use 3rd party services like MailGun.

Mailgun provides API (or classic smtp support) to send email notifications without providing any username and password of your clients.

Mailgun webpage offers sample code for php, java, node.js , curl etc.
This is a sample node.js app :
const formData = require('form-data');
  const Mailgun = require('mailgun.js');
  const mailgun = new Mailgun(formData);
  const mg = mailgun.client({username: 'api', key: '64a24..........................52627ba0'}); //username must be api and not your mailgun username

  mg.messages.create('sandbox3fbd203650454002b7d8b2e05a1b82b2.mailgun.org', {
  	from: "Excited User <mailgun@sandbox3fbd203650454002b7d8b2e05a1b82b2.mailgun.org>",
  	to: ["someuser@gmail.com"],
  	subject: "Hello",
  	text: "Testing some Mailgun awesomeness!",
  	html: "<h1>Testing some Mailgun awesomeness!</h1>"
  })
  .then(msg => console.log(msg)) // logs response data
  .catch(err => console.log(err)); // logs any error

PS: Above code gives error claiming that the user is not identified and that sandbox accounts are for testing only.

