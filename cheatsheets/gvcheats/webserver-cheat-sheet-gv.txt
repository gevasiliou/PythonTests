WEBSERVER:BASICS: CHEAT SHEET by gv

##WEBSERVERS (APACHE2 , NGINX)
Both Apache2 and nginx (engine x) are web servers - they serve the same purpose but have different philosphy , settings, etc.
.B APACHE2
typical html directory is /var/www/html
typical apache2 configuration directory is /etc/apache2
in /etc/apache2 you can find apache2.conf, conf directories, module directories, sites directories.
Directories are following the available - enabled logic.
You can enable / disable confs, mods, sites using a2 commands:
    Command     Function                                Operates On
    a2enmod     Enables an Apache module                Modules
    a2dismod    Disables an Apache module               Modules
    a2enconf    Enables an Apache configuration file    Configuration Files
    a2disconf   Disables an Apache configuration file   Configuration Files
    a2ensite    Enables an Apache virtual host (site)   Virtual Hosts
    a2dissite   Disables an Apache virtual host (site)  Virtual Hosts

Enabling an apache2 module does not automatically enable the conf file of this module.
You need to explicitly enable conf of module with a2enconf.

.I Enabling Virtual Hosts (sites) - 1 site only
All apache2 installations include the file /etc/apache2/sites-available/000-default.conf
Which usually looks like this:
    <VirtualHost *:80>
        #ServerName www.example.com
        ServerAdmin webmaster@localhost
        DocumentRoot /var/www/html
        ErrorLog ${APACHE_LOG_DIR}/error.log
        CustomLog ${APACHE_LOG_DIR}/access.log combined
    </VirtualHost>

When i built a project i prefer to create a folder like this /var/www/html/myproject 
in /myproject/ i hold all the project files (node.js , html , css , js scripts, etc).

So in case that your web server machine is hosting only one site, you can edit this 000-default.conf like bellow:
    DocumentRoot /var/www/html/myproject

If your server is assigned to a domain name you can also uncomment this line:
    ServerName www.myproject.net

.I Enabling Virtual Hosts (sites) - More than 1 site
You can host more sites in the same machine under different directories:
Project A : /var/www/html/myprojectA
Project B : /var/www/html/myprojectB

In this case you have to create two new conf files inside /etc/apache2/sites-available/ directory.
sudo nano /etc/apache2/sites-available/myprojectA.conf
    <VirtualHost *:80>
        ServerName myprojectA.net
        DocumentRoot /var/www/html/myprojectA
    </VirtualHost>

sudo nano /etc/apache2/sites-available/myprojectB.conf
    <VirtualHost *:80>
        ServerName myprojectB.org
        DocumentRoot /var/www/html/myprojectB
    </VirtualHost>

You have then to enable those sites / virtual hosts:
    sudo a2ensite degermonitor.conf
    sudo a2ensite thingthis.conf
Tip: In reality a2ensite just creates links inside /etc/apache2/sites-enabled to /etc/apache2/sites-available.
Tip: file 000-default.conf needs no modifications - can remain blank or typical pointing to /var/www/html

With this way: 
    (a) when you visit www.myprojectA.net it will load the files / data available in /var/www/html/myprojectA
    (b) when you visit www.myprojectB.org it will load the files / data available in /var/www/html/myprojectB
    (c) when you visit http://IPADDRESS of your web server, then file 000-default.conf will come into play and serve default dir == /var/www/html
        Alternatively, you can add this line to 000-default.conf in order to redirect requests to the project you want to be the "default" project:
            Redirect permanent / http://myprojectB.org/
    

.I User Apache2 as a reverse proxy
Usefull when your server (back-end == node.js) runs on a custom port (i.e 3000) to serve end points in front-end (html, js).

With bellow technic instead of using api calls from frontend like fetch('http://XXX.XXX.XXX:3000/fetch-data')
you can just use fetch('/fetch-data')

Anotherway would be to adjust your node.js at back-end to use the same port as your webserver (80 or 443) but this raises a lot of security concers.
The best practice is to use reverse proxy configuration in your Apache2 / site conf file.

As a general idea you don't mess up with 000-default.conf and you should have a separate conf file for your project, something like
/etc/apache2/sites-available/myprojectA.conf

<VirtualHost *:80> # Or *:443 for HTTPS
    ServerName myprojectA.net # Replace with your domain
    #ServerName XXX.XXX.XXX.XXX # Use IP if you don't have a domain name yet

    DocumentRoot /var/www/html/myprojectA # Your website's root directory

    # Reverse proxy configuration
    ProxyPass /api http://localhost:3000/api # Forward requests to /api to your backend
    ProxyPassReverse /api http://localhost:3000/api # Important for redirects and headers

    # If your backend is on a different server:
    # ProxyPass /api http://backend_server_ip:3000/api
    # ProxyPassReverse /api http://backend_server_ip:3000/api
    # ... other virtual host configurations
</VirtualHost>

If in your server you have only a web site , is a good idea instead of creating a new file to modify 000-default.conf.
If you prefer to have a new conf file , is a good idea to disable your 000-default.conf (sudo a2dissite 000-default.conf).
If you create a new conf file based on ip access you can do something like this:

sudo nano /etc/apache2/sites-available/51-21-41-110.conf
<VirtualHost *:80>
    ServerName 51.21.41.110
    DocumentRoot /var/www/html/frontend

    ProxyPass /fetch-data http://localhost:3000/fetch-data
    ProxyPassReverse /fetch-data http://localhost:3000/fetch-data
</VirtualHost>

sudo a2ensite 51-21-41-110.conf  # If you named the file 51-21-41-110.conf
#or sudo a2ensite frontend.conf If you named the file frontend.conf
sudo systemctl restart apache2
sudo a2dissite 000-default.conf

.I Proxying multiple api calls to the same back-end server
You absolutely do not need to create separate `ProxyPass` entries for each endpoint if they all point to the same backend server. 
**The Trick: Proxying a Common Path or the Root:**

The key is to proxy a common path that encompasses all your API endpoints.

For this to work , all your API endpoints should start with a common path segment (e.g., `/api`), and then you can proxy that path:

<VirtualHost *:80>
    ServerName yourdomain.com # Or your IP address
    DocumentRoot /var/www/html/frontend
    ProxyPass /api http://localhost:3000/ # Proxy all requests starting with /api
    ProxyPassReverse /api http://localhost:3000/
</VirtualHost>

With this configuration:

*   `fetch('/api/fetch-data')` will be proxied to `http://localhost:3000/fetch-data`.
*   `fetch('/api/load-files')` will be proxied to `http://localhost:3000/load-files`.
*   `fetch('/api/save-files')` will be proxied to `http://localhost:3000/save-files`.

See bellow real life examples for more examples and tested use cases

.B Restarting Apache2
sudo systemctl restart apache2
sudo system apache2 stop && sleep 5 && sudo system apache2 start

.B Typical apache2.conf (working in my server)
$ cat /etc/apache2/apache2.conf |grep -v '^[#]' |grep -v '^[[:space:]]*$'
DefaultRuntimeDir ${APACHE_RUN_DIR}
PidFile ${APACHE_PID_FILE}
Timeout 300
KeepAlive On
MaxKeepAliveRequests 100
KeepAliveTimeout 5
User ${APACHE_RUN_USER}
Group ${APACHE_RUN_GROUP}
HostnameLookups Off
ErrorLog ${APACHE_LOG_DIR}/error.log
LogLevel warn
IncludeOptional mods-enabled/*.load
IncludeOptional mods-enabled/*.conf
Include ports.conf
<Directory />
	Options FollowSymLinks
	AllowOverride None
	Require all denied
</Directory>
<Directory /usr/share>
	AllowOverride None
	Require all granted
</Directory>
<Directory /var/www/>
	Options Indexes FollowSymLinks
	AllowOverride None
	Require all granted
</Directory>
AccessFileName .htaccess
<FilesMatch "^\.ht">
	Require all denied
</FilesMatch>
LogFormat "%v:%p %h %l %u %t \"%r\" %>s %O \"%{Referer}i\" \"%{User-Agent}i\"" vhost_combined
LogFormat "%h %l %u %t \"%r\" %>s %O \"%{Referer}i\" \"%{User-Agent}i\"" combined
LogFormat "%h %l %u %t \"%r\" %>s %O" common
LogFormat "%{Referer}i -> %U" referer
LogFormat "%{User-agent}i" agent
IncludeOptional conf-enabled/*.conf
IncludeOptional sites-enabled/*.conf

.B Real Life Examples 
In my server, without owning a domain name , i used to have under /var/www/html two subfolders:
thingit
deger
I used to access my deger project files with command http://51.21.41.110/deger/degerfetcher.html

My default apache2 conf file at available-sites (000-default.conf) was like bellow:
<VirtualHost *:80>
	ServerAdmin webmaster@localhost
	DocumentRoot /var/www/html
	ErrorLog ${APACHE_LOG_DIR}/error.log
	CustomLog ${APACHE_LOG_DIR}/access.log combined
</VirtualHost>

I modified 000-default.conf file like bellow:
<VirtualHost *:80>
	ServerAdmin webmaster@localhost
	DocumentRoot /var/www/html/deger
	DirectoryIndex degerfetcher.html index.html # List files in order of preference
	ErrorLog ${APACHE_LOG_DIR}/error.log
	CustomLog ${APACHE_LOG_DIR}/access.log combined
</VirtualHost>

Changes:        DocumentRoot has been changed to /var/www/html/deger directory.
                DirectoryInder defines the file to be loaded by default (if ommitted, index.html will be used as default file by apache2)
Result:         Now i can type in my browser http://51.21.41.110 and my degerfetcher.html page will be loaded. Also the links / api calls seems to work correctly.
Bug:            There is no way to access my files in my second project thingit (thingit foled under /var/www/html).
Bug Solution:   (a) If you have domain names available , you can create two conf files , each for each domain, pointing to the required Document Root.
                (b) If you don't have domain names, you can include in your 000-default.conf file a line like bellow:
                    Alias /thingit /var/www/html/thingit  # Alias for thingit
                With this line you bind http://51.21.41.110/thingit to /var/www/html/thingit directory / project.

.I Working Configuration with reverse proxy in my web server
1. First Enable proxy modules on apache2
    sudo a2enmod proxy
    sudo a2enmod proxy_http
    sudo systemctl restart apache2

2. Finalize 000-default.conf as bellow:

$ cat /etc/apache2/sites-available/000-default.conf |grep -v '^[[:space:]]*[#]'
<VirtualHost *:80>
	ServerAdmin webmaster@localhost
	DocumentRoot /var/www/html/deger
	DirectoryIndex degerfetcher.html index.html # List files in order of preference
    <Location /api> # Or any other path prefix for your API
       	ProxyPass http://localhost:3000/api
       	ProxyPassReverse http://localhost:3000/api
	</Location>
	ErrorLog ${APACHE_LOG_DIR}/error.log
	CustomLog ${APACHE_LOG_DIR}/access.log combined
</VirtualHost>
PS: Above configuration was tested and worked fine for front end call fetch('/api/get-max-wind-data') and back end app.get('/api/get-max-wind-data')

2.B You can get also a working reverse proxy with bellow conf file, without using <Location> tag 
<VirtualHost *:80>
	ServerAdmin webmaster@localhost
	DocumentRoot /var/www/html/deger
	DirectoryIndex degerfetcher.html index.html # List files in order of preference
   	ProxyPass /api http://localhost:3000/api
   	ProxyPassReverse /api http://localhost:3000/api
	ErrorLog ${APACHE_LOG_DIR}/error.log
	CustomLog ${APACHE_LOG_DIR}/access.log combined
</VirtualHost>
PS: Above configuration was also tested and worked fine for front end call fetch('/api/get-max-wind-data') and back end app.get('/api/get-max-wind-data')

3. Adjust front-end api calls to include /api like this:
    const response = await fetch('/api/get-max-wind-data');
                
4. Adjust back-end node.js end points to also include /api like this
    app.get('/api/get-max-wind-data', (req, res) => {
        res.sendFile(windDataFile, (err) => {
            if (err) {
                console.error('Error sending max-wind-values.json:', err);
                res.status(500).send({ error: 'Could not load max wind data' });
            }
        });
    });

5. Test externally with Browser by hitting directly to http://xxx.xxx.xxx.xxx/api/get-max-wind-data
You should see data in your browser flowing / received ==> This is good.

6. Test internally at a different server window
curl http://localhost/api/get-max-wind-data
Since we ommir the port (http://localhost:3000), we reach endpoint /api/get-max-wind-data by the apache2 reverse proxy.
If we use curl http://localhost:3000/api/get-max-wind-data i suppose we talk directly to the back-end node.js without proxy.

Tip1: Using /api for calling node.js at back-end and front-end as above, you achieve:
(a) speed up the loading of static pages in your project directory (i.e anotherpage.html, myscript.js, mystyle.css, etc) 
(b) "proxying" to back-end server localhost:3000 only /api/endpoint requests and not everything
(c) Programming discrimination between api calls and static files loading from DocumentRoot (i.e secondpage.html, myscript.js, mystyle.css, etc).
(d) With above configuration, direct api calls from front-end like:
    fetch(`http://51.21.41.110:3000/get-data-for-date?date=${date}`)
    Will still work , if node.js at back-end has /get-data-for-date endpoint.

Tip2: You could even try to use reverse proxy with a setup like bellow, without using <Location>
            ProxyPass /api http://localhost:3000
            ProxyPassReverse /api http://localhost:3000
      In this scenario api calls in front-end should include /api i.e fetch(/api/get-wind-data) but back-end node.js end points does not require /api i.e app.get('/get-wind-data')
      Think this proxying thing as a kind of alias.
      This method is not proposed since it can cause quite of confusion having different naming in api calls at front-end vs back-end 

Tip3: Some tutorials might propose the following configuration for reverse proxy (without <Location>)
            ProxyPass / http://localhost:3000
            ProxyPassReverse / http://localhost:3000
      In this case, apache2 will forward (proxying) ALL requests to localhost:3000 (due to / = root rule), including api calls and static files call (**)
      Api calls will be handled correctly by back-end server (*), but static files will fail, unless node.js at back-end is configured to serve static files.
      Even if node.js is capable / configured to handle static files requests at document root, this approach will slow down performance.
      (*) To have correct api calls, front-end calls and back-end endpoints should match, like bellow:
            fetch('/get-wind-data') at front-end javascript
            app.get('/get-wind-data') at back-end node.js
      (**) To be checked if finally proxying / will forward ALL requests including or excluding static files requests to http://localhost:3000

##PHP
In Debian 12 , the php version available is 8.2
All bellow commands should be run as root or with sudo.
The key is that except installing php in your server machine, you have to configure your webserver (apache2) to load the necessary php modules, configurations etc.

Source: https://www.installconfig.com/doku.php?id=configure_ubuntu_server_20_04_host_nextcloud_hub_21

Step 1: apt install php libapache2-mod-php libapache2-mod-fcgid php-fpm 
Tip: You can just apt install php to install typical php that will be handled by Apache2.

Step 2: write a simple test.php file inside /var/www/html (default folder), containing just those two lines:
<?php
phpinfo();

Step 3: open a php interactive shell with php -a and run phpinfo();
If you see output, you have confirmed that php (as languge) is installed in your system.

Step 4: geany /etc/apache2/apache2.conf
To the end of file , add this line:
SetHandler application/x-httpd-php

Step 5: It is a good idea, though not well understood why in this point, to enable php-fpm (CGI Web Server Support) since it is claimed that CGI has better / faster php support over Apache2. 
To enable php-fpm you have to disable older php modules on Apache2 and load new php-fpm modules:
a2dismod mpm_event
a2enmod mpm_prefork
a2enmod php8.2
a2enmod proxy_fcgi setenvif
a2enconf php8.2-fpm
a2query -m php8.2 
#a2query is just a question to the system / query - you expect to get a positive confirmation in your query


Step 6: Edit the php configuration file and comment out / disable the following lines:
geany /etc/apache2/mods-enabled/php8.2.conf
# Running PHP scripts in user directories is disabled by default
# 
# To re-enable PHP in user directories comment the following lines
# (from <IfModule ...> to </IfModule>.) Do NOT set it to On as it
# prevents .htaccess files from disabling it.
#<IfModule mod_userdir.c>
#    <Directory /home/*/public_html>
#        php_admin_flag engine Off
#    </Directory>
#</IfModule>

Step 7: systemctl restart apache2

Step 8: Navigate to http://127.0.0.1/test.php and everything should work ok.
PS: Considering that you had a working apache2 server , meaning that you have a default index.html file and meaning that visiting http://127.0.0.1 works ok.

Step 9: It seems good option to install some php extensions in your system:
sudo apt install php-gd php-mysql php-curl php-mbstring
sudo apt install php-intl php-gmp php-bcmath php-xml phpunit
sudo apt install libapache2-mod-php php-zip php-imagick php-apcu

#PHP Composer
composer is a kind of php libraries installer.

.B Installing Composer according to https://getcomposer.org/
php -r "copy('https://getcomposer.org/installer', 'composer-setup.php');"
php -r "if (hash_file('sha384', 'composer-setup.php') === 'e21205b207c3ff031906575712edab6f13eb0b361f2085f1f1237b7126d785e826a450292b6cfd1d64d92e6563bbde02') { echo 'Installer verified'; } else { echo 'Installer corrupt'; unlink('composer-setup.php'); } echo PHP_EOL;"
php composer-setup.php
php -r "unlink('composer-setup.php');"

If everything is ok, you should receive a success confirmation

Last step: sudo mv composer.phar /usr/local/bin/composer


##PHP MODBUS
As of today (19.10.23) there is a Github Project capable to provide libraries for TCP Modbus Communication.
https://github.com/aldas/modbus-tcp-client

.B Use PHP built-in webserver for PLC Communication

sudo apt-get install php-mbstring
sudo apt install php-curl
sudo apt install php-xml
sudo apt install phpunit
sudo service apache2 restart
composer update OR sudo /home/gv/composer.phar update
composer require aldas/modbus-tcp-client

git clone https://github.com/aldas/modbus-tcp-client.git
cd modbus-tcp-client
composer install
cd modbus—tcp-client/examples && touch .allow-change

File .allow-change is used as a flag, to allow users to change IP address / IP Port.
Without this file present IP address and IP Port of webserver will be grayed out / locked/hardcoded in index.php file.
File .allow-change can be empty.

.B Run the webserver
php -S localhost:8080 -t examples/

-S switch: Run Built-in php webserver
-t switch: Define the root directory of PHP webserver. If not provided cwd will be used as root directory
Now open http://localhost:8080 in browser. See additional query parameters from index.php.

Tip:
aldas "Examples" folder has index.php inside. 
index.php is used with php built-in web server to test out communication with our own PLCs.

Tip2: php -S localhost:8080 /path/to/index.html -> This is a valid synthax to serve a particular file with php -S

##NODE JS
.I https://phoenixnap.com/kb/debian-install-nodejs
.I https://deb.nodesource.com/

.B Install nodejs on Debian & Usefull nodejs commands:
apt install nodejs                  # installation of nodejs core
apt install npm                     # npm is a package manager for nodejs
curl https://raw.githubusercontent.com/creationix/nvm/master/install.sh | bash  # This will install nvm
source ~/.profile                   # after installation of nvm
nvm ls-remote                       # list available nodejs versions
nvm install <nodejs version>        # install nodejs specific version, i.e nvm install 12.1
nvm ls                              # list locally installed versions
nvm use 18.1                        # use a specific nvm version
npm install module                  # npm = nodejs package manager- Can install various nodejs modules (i.e sudo npm install -g ws path http express body-parser yargs fs modbus-serial)
nodejs script.js                    # use nodejs to run file script.js on commandline

sudo npm install pkg                # install particular js packages required by your project. This has to be run in your project directory.            
sudo npm install -g pkg             # install pkg globally , system wide -> folder 
cd /path/to/project && sudo npm ls  # List all pkgs installed in your project directory
sudo npm ls -g                      # Lits all pkgs installed globally (/usr/local/lib) 

.B Websockets [ws module]
The use of websockets servers (i.e inside a server.js file) allows websocket clients to get connected and receive messages by the server.
An html page can be used as websocket client (using javascript).
As a result you can achieve to have html pages with content updated dynamically , as received (push) by the ws server.
Actually with node you can enclose in a simple file tcp (or express ) server, websocket server , http server.
For better results you could use different ports on each server.
Typical data flow inside server js:
* html page (ws client) connects to server ws server
* Json sending APIs get connected to express or tcp servers to post their values
* http is serving the html file you want.
* When new json data arrive in tcp/express then ws server push those data to ws clients and html pages refresh their content by pasring the json string received by the ws server.

Check out thingit server files for more.

.B INCLUDE
Very important, used to include external node modules and use their libraries.

Example of a file called config.js
exports.port = process.env.PORT || 3000;
exports.max_request_length = 100000; // The maximum length of characters allowed for a request or a response.
exports.max_requests_per_second = 10; // The maximum number of requests per second to allow from a given IP.

##Usage of config.js with your main node.js file:
var config = require("./config");

##Usage of config.js variables in main script file
function sendTooBigResponse(res) {
    return writeResponse(res, 413, "the content in the request or response cannot exceed " + config.max_request_length + " characters.");
}

 if (config.blacklist_hostname_regex.test(remoteURL.hostname)) {
            return writeResponse(res, 400, "naughty, naughty...");
        }

WEBSERVER:DOMAIN NAME REGISTRATION – WEBHOSTING – AWS CLOUD HOSTING

##Register a Domain Name
Though i am located in Greece, I didn’t want a .gr domain name but I would like a .io tld, which proved to be quite expensive compared to other tld types like .gr or .online.

I finally ended up with ‘.online’ domain registered with namecheap at 1/8 of Greek registrars price (~1 euro for 1 year). 
Greek Registrars require ~2.5€ per year for .gr domain or 8 euro for .online for 1 year  (ps: i paid just 1 euro for 1 year). 

To complete my domain name registration I used namecheap.
https://ap.www.namecheap.com/Domains/DomainControlPanel/thingit.online/advancedns 

After purchasing your required domain name you have to update dns records in order your domain name to point out to your web hosting server (aws in my case). 

All registrars provide a kind of web-based control panel for managing domain names purchased.
Tip: https://www.namecheap.com/support/knowledgebase/article.aspx/434/2237/how-do-i-set-up-host-records-for-a-domain/

##Hosting Types
a.	Shared Hosting -> The cheapest one – suitable for personal web sites / small business serving static web pages without big demands. The physical server resources (cpu, ram, net speed, bandwitch, etc) are shared to all the server web-pages/users. Performance can be dependent on the total server workload, meaning that other web-pages/users might affect the performance of the physical web server. 
It is quite classical in this type of Hosting the web page admin has not direct access to the server (i.e using ssh) and all the setup takes place by using the server control panel (i.e cPanel).

b.	VPS Hosting (Virtual Private Server) -> Is the next step of Shared Hosting, each user has it’s own virtual server (consider physical server split to different pieces / containers). In this case each “container” has a predefined amount of hardware to be used (cpu, ram, etc) and the performance is not affected by the other users/web-pages activities. 
In VPS it seems that ssh to your virtual server is possible. At least in AWS that was cool.
I could use the server terminal and install anything necessary.

c.	Dedicated Server Hosting -> One step further than the VPS, you do rent a full / complete server on your own and you are using 100% of the server hardware for your own web-pages. Obviously ssh (quite unrestricted I suppose) it is supported.

d.	Cloud Hosting – Not clear yet – seems to be a kind of VPS but running on the cloud, meaning that your web files might be copied to different servers in order guaranteed uptime to be achieved (if one cloud server fails due to error, overload etc then your web page traffic is redirected and served by another cloud server.

##Working with AWS (Amazon Web Services) for webhosting.
Guide: https://middleware.io/blog/virtual-machine-on-aws-guide/
Starting Point: https://portal.aws.amazon.com/billing/signup?#/start

Amazon Web Services (AWS) is actually providing a cloud based hosting with VPS.

Setup of aws seems to be a headache, but it proved that following the “guides” it works. 

First you have to create your aws account, providing also credit card details (prepaid cards seems to be approved) – credit card will be temporarily charged 1 € and then this 1€ will be refunded. 
If I am not wrong you can make use of aws for 1 year FREE.

Once everything is ready (user registered, email verification, credit card , etc) you arrive at “AWS Management Console” in where you can select if you want to deploy an EC2 Virtual Machines (called instances in aws world). 

Except seeting up a Virtual Machine, a lot of options exist in Management Console. 

Assuming that your target was to deploy a virtual machine, you have to select the correct instance type with the required AMI (Amazon Machine Image). I used Debian AMI since I am quite familiar with Debian.

Then you have to correctly set up your instance / virtual machine (see the detailed Guide) including security group adjustments that affect inbound and outbound rules. 

Once your instance is ready you can “Launch” your virtual machine and you get a confirmation message.

PS: Don’t neglect to set up a key for remote access (select “Create a new key pair”) in order to be able to get connected to your VM using encryption. It has been said that you can not get connected to instances with out a key pair.

On your dashboard, you can see all your instances, you can start / stop them and you can get connected to them.
Connecting to your instance proved to be a headache, but I got easily connected to my instance with ssh:
[*] sudo ssh -i "thingit-debian.pem" admin@ec2-51-21-41-110.eu-north-1.compute.amazonaws.com
[*] sudo ssh -i "thingit-debian.pem" admin@thingit.online

PS1: File ‘thingit-debian.pem’ was generated by AWS and was saved to my linux machine. You can copy-paste this pem file between Linux machines.
PS2: If you doubt about your site / aws beeing online scan it with nmap and not ping; ping seems that is blocked by aws.

Once connected to your VM by ssh you have raw terminal access to your virtual machine and you have to set up almost everything by the terminal (apache installation, php – perl – java – nodejs installation, etc).

.B Important info about PEM key files:
The remote access key first created in your aws machine setup is very important. 
In reality your aws machine admin/root account is binded to this very first key (pem file).
Is seems that even if you create additional key pairs, those pairs does not work to grant you admin/root access to your aws server.
The only way to gain access as admin/root in your aws machine is to have the very first pem file (ps: copy paste of this pem file to another linux machine works fine).
You can login to your aws server with additional pem key pairs, but you have to set up a user in your remote aws machine and create pem keys for those users.
The pem key file that grants admin/root access to your aws machine is unique and not easily replacable.
Removing the binding of your aws vm to this very first pem key is a real challenge with great failure probabilities.

##Assigning an Elastic (static) IP to you instance
Guide: https://aws.plainenglish.io/assigning-a-domain-name-to-an-aws-ec2-instance-via-elastic-ip-d2234b1662cc

Step 1: Allocate an Elastic IP
Go to the AWS Management Console and navigate to the EC2 Dashboard.
From the navigation bar, click on “Elastic IPs” under “Network & Security.”
Click on “Allocate new address” and choose “Amazon’s pool of IPv4 addresses.”
Click on “Allocate” to create a new Elastic IP.

Step 2: Associate the Elastic IP with your EC2 Instance
From the Elastic IPs list, select the newly allocated Elastic IP.
Click on “Actions” and choose “Associate IP address.”
In the “Associate Elastic IP address” dialog, select your EC2 instance from the drop-down menu.
Click on “Associate” to link the Elastic IP with your EC2 instance.

Step 3: Update DNS Records with your Domain Registrar
Log in to your domain registrar’s website (where you registered your domain).
Locate the DNS management settings for your domain.
Add an “A” record with a blank or “@” hostname, pointing to the Elastic IP address you just associated with your EC2 instance.
Save the changes to update your DNS records.

Step 4: Wait for DNS Propagation
After updating your DNS records, it may take some time for the changes to propagate across the internet. DNS propagation typically takes a few minutes to several hours, depending on various factors such as TTL (Time to Live) settings and caching.

Step 5: Verify Domain Name Resolution
Open a web browser and enter your domain name (e.g., example.com) in the address bar.
If everything is configured correctly, your website or application hosted on the EC2 instance should load successfully.

Step 6: Check Domain Resolution with NSlookup
Open a terminal or command prompt on your local machine.
Type the following command, replacing “example.com” with your domain name:

From this point and after connecting my domain name to aws instance, I could use ssh with something like this:
sudo ssh -i "/home/gv/Desktop/thingit-debian.pem" admin@thingit.online

This was my instance name earlier, before Elastic IP assignment: 
http://ec2-51-20-70-223.eu-north-1.compute.amazonaws.com/

.B Tip1: http access
If your instance in running , you have provided html files to /var/www/html directory but you are not able to see the html pages inside this folder, then you have to try with http instead of https.

.B Tip2: allow inbound traffic
If you want to use a custom port in your VM (i.e for a websocket server) you have to allow inbound trafic in this port by configuring Security Group Settings in your aws account/machine

.B Tip3: Server Login and File Copy
To login in your aws server by your local debian use this synthax:
[*] sudo ssh -i "/home/gv/Downloads/thingit-debian.pem" admin@thingit.online
thingit-debian.pem is the pem file of the very first time that a key pair to your aws server was created.

To copy files from your local machine to remote EC2 server you need to run scp like bellow:
[*] sudo scp -i "/home/gv/Downloads/thingit-debian.pem" myfile.html admin@thingit.online:/home/admin
Avoid copying files directly to /var/www/html/yourfolder since scp to root folders usually is not allowed.

.B Tip4: LocalHost
It is common to have a local node.js running on server (i.e node myfile.js) that will be opened for listening at a custom port like 3000 and the 
corresponding html file, will try to fetch data from http://localhost:3000/end-point.
The term "localhost:3000" cause a lot of problems when trying to access from outside world. 
It is better to replace the localhost with public ip of your server inside your html file.
So , instead of this : fetch('http://localhost:3000/list-files')
write this : fetch('http://51.21.41.110:3000/list-files') or fetch('http://thingit.online:3000/list-files')
Also better to adjust port 3000 to be open for incoming connections at your server configuration (though i am not sure if this is required).

.B Tip5:  For node.js + html troubleshooting: 
Login to your server with ssh by two terminals at the same time and go to /var/www/html.....
Terminal one: run node helloworld.js (you should see a message that server is listening on port 3000)
Terminal two: run links http://localhost:3000/list-files -> you should receive the hello world message received by node.js on terminal 1. 
In this case the term localhost works ok, since both terminals are connected to the same server and thus localhost has a meaning.
This is a test to make sure that node.js server is up and running at port 3000.

Then, you can then try to see if hello world message can be delivered by node.js in external browsers. 
Keep teminal 1 open, running with node.js -serving at port 3000 
Use any external web browser and try to open: 
http://thingit.online/isotec/helloworld.html (this is a small web page with a simple button that when pressed the end point /list-files is served = Hello World message
or try directly http://thingit.online:3000/list-files (you should see Hello World message directly in your browser)

.B Tip6: Permission Issues
Custom node.js scripts that are trying to save files to your web server root directories like /var/www/html/.... have write permission issues.
To overcome those issues, you have to start node.js script on your server with sudo (i.e sudo node myfile.js) to ensure write access to /var/www/html/....
Alternativelly i suppose you can adjust the /save-file and /load-file endpoints in your node.js file to use a different directory with write access for the user or for everybody.
As a general idea, being capable to write files to your root directories by a web html page that posts data to node.js , sounds a bad and vulnerable idea.

.B Tip7: Perform a task on remote server without keeping the ssh connection open:
[*] sudo ssh -i "/home/gv/Downloads/thingit-debian.pem" admin@thingit.online "cat /var/www/html/isotec/isotec2.js"

.B Tip8: Compare local files to server files:
[*] sudo ssh -i "/home/gv/Downloads/thingit-debian.pem" admin@thingit.online "cat /var/www/html/isotec/isotec2.js" |diff - ./isotec2.js

.B Tip9: Copy file to your aws server by local machine with a single command:
[*] sudo scp -i "/home/gv/Downloads/thingit-debian.pem" isotec3.js admin@thingit.online: && sudo ssh -i "thingit-debian.pem" admin@thingit.online "sudo cp -v /home/admin/isotec3.js /var/www/html/isotec/"

First command performs a secure copy using ssh with a special command called scp (usuall synthax is scp source-path target-path).
in our case target-path is admin@thingit.online:
The symbol : is equivalent to home directory of the logged in user  --> /home/admin
sudo scp -i "thingit-debian.pem" isotec3.js admin@thingit.online:   --> 
[*] sudo scp -i "/home/gv/Downloads/thingit-debian.pem" isotec3.js admin@thingit.online:/home/admin
PS: You can not perform scp to root directories like /var/www/html , and this is why we need the second command - to copy the file from home directory to the /var/www/html directory.

.B Tip10: Use git for easy files update between local machine and remote aws server.

.I Update aws server with changes made in files of Local Machine:
[*] cd /home/gv/Desktop/deger && gitsend && sudo ssh -i "/home/gv/Downloads/thingit-debian.pem" admin@thingit.online -p 6111 "cd /var/www/html/deger/ && sudo git pull"

For this to work, both machines (local & remote server) should have git clone deger repository.

PS1: mind the different port used for ssh- this has be changed on purpose since a loot of tools automate attacks to default ssh port 22.
PS2: gitsend is one of my custom aliases: alias gitsend='git add . && git commit -m "update" && git push && git show --name-only'
PS3: git pull is standard git command - in server, this has been aliased to sudo git pull

.I Update local files with new files (i.e log files) created in your aws server.
Login to aws server, git send the files modified to git , pull the changes to local machine:
[*] sudo ssh -i "/home/gv/Downloads/thingit-debian.pem" admin@thingit.online "cd /var/www/html/deger/ && sudo git add . && sudo git commit -m 'update' && sudo git push && sudo git show --name-only" && cd /home/gv/Desktop/deger && git pull

.B Tip11: Using PM2 to manage your node apps
With PM2 (npm install pm2 -g) you can start your node.js app and keep it alive for ever.....
This process manager takes care to keep app running, including retarting in case of app crash.

[*] sudo pm2 stop isotec4.js && sleep 5 && sudo pm2 start isotec4.js --name isotec4 --time --output pm2logs/out.log --error pm2logs/error.log
You don't need to explicitly define /var/www/html/deger/pm2logs/out.log , since pm2 keeps the /var/www/html/deger path.

To troubleshoot logging (in custom directory) run 
[*] sudo pm2 logs
This will open the real logs , and the path of the real log will be displayed (PM2 default logging directory is /root/.pm2/logs)
You can now check if the real log path matches the log path you provided when your sudo pm2 start.

If PM2 ignores your --output and --error settings, bellow procedure usually helps to restore logging in your custom directory:
[*] sudo pm2 stop all
[*] sudo pm2 delete all
[*] sudo pm2 save --force
[*] sudo chmod -R 777 /var/www/html/deger/pm2logs
[*] sudo pm2 start isotec4.js --name isotec4 --time --output pm2logs/out.log --error pm2logs/error.log
[*] sudo pm2 save

PS: Things got complicated when you run sudo pm2 update.... Avoid to use it, till to be checked

More pm2 commands:
[*] sudo npm install -g pm2
[*] sudo pm2 start /var/www/html/isotec/isotec.js --name isotec --time --log pm2logs/out.log
[*] sudo pm2 start isotec3.js --name isotec3 --time --output pm2logs/out.log --error pm2logs/error.log
[*] sudo pm2 startup
[*] sudo pm2 monit
[*] sudo pm2 logs (or pm2 logs isotec4)

Start all processes: pm2 start all
Stop all processes: pm2 stop all
Restart all processes: pm2 restart all
Delete all processes: pm2 delete all

.B Tip12: Create a file like degerfetcher.html that will be actually a soft link to your latest degerfetcher html page.
With this way you don't have to worry what is the "latest" version of your html file.
Just make a link, and always open degerfetcher.html. Your browser will not notice any difference.
[*] sudo rm degerfetcher.html
[*] sudo ln -s degerfetcher10.html degerfetcher.html


##WEB:COOL TOOLS

##XTERM.JS
https://github.com/xtermjs/xterm.js
Library made to provide kind of terminal inside a web page. Install using npm install -g xterm

## SEASHELLS
https://seashells.io
Seashells is based on term.js and provides a web-based view-only terminal.
You can pipe data to seashells and this data will be published on a dynamic seashells page.

Usage:  echo 'Hello, Seashells!' | nc seashells.io 1337
Response: serving at https://seashells.io/v/{url}
htop | seashells --delay 5

seashells can be used to display on a web page output data of various scripts, i.e nodejs scripts.
Being able to see the results of node program.js in a web page, you don’t have to use ssh.

##Bastillion
https://github.com/bastillion-io/Bastillion
Bastilion is a full working web-based CLI, based on xterm.js
Depends on java sdk.
With Bastillion you can set up various terminals that will provide you remote CLI access.
It is like ssh but on a web page (!).


.B Installation
Download Bastillion from github: https://github.com/bastillion-io/Bastillion/releases
Download the latest tar file and extracted in a dedicated directory. Run it using ./startBastillion.sh

.B Using Bastillion
Open browser to https://<whatever ip>:8443

Login with
username:admin
password:changeme

Note: When using the AMI instance, the password is defaulted to the <Instance ID>. Also, the AMI uses port 443 as in https://<Instance IP>:443


##More cool resources based on xterm.js
https://github.com/xtermjs/xterm.js#real-world-uses
Check rtty: https://github.com/zhaojh329/rtty

##DNS Hosting
https://www.hostingkey.com/dnhosting.php
Register to hostingkey name server your web site (not obtained by hostingkey).

##WEB:SET UP YOUR OWN MAIL SERVER

##POSTFIX
Postfix on debian is a complete smtp package/server/client that can accept incoming emails and can send emails to the world.
You need to have your tcp port 25 (incoming) open to receive emails (verify this with online port scanner https://www.ipvoid.com/port-scan/)
You also need to open tcp port 25 (outgoing) for sending emails. This can be tested with Telnet (i.e telnet gmail-smtp-in.l.google.com 25 -> should proceed and not hanging)
PS: Amazon Web Services (AWS) do not allow traffic on outgoing port 25 so practically you can not send email by AWS with PostFix.
You can though overcome this issue by using sendinblue = Brevo.

You also have to adjust your DNS records (through your registrar, i.e namecheap) so the world to be able to find and use your mail server.
.I https://www.namecheap.com/support/knowledgebase/article.aspx/322/2237/how-can-i-set-up-mx-records-required-for-mail-service/

.B PostFix Installation
.I https://www.linuxbabe.com/mail-server/build-email-server-from-scratch-debian-postfix-smtp

    sudo apt install postfix libsasl2-modules     #Tip : Select Internet Email during Installation

Edit postfix configuration:
    sudo nano /etc/postfix/main.cf

Replace relayhost with this line:
    relayhost = [smtp-relay.sendinblue.com]:587

Then add the following lines to the end of this file.
    # outbound relay configurations
    smtp_sasl_auth_enable = yes
    smtp_sasl_password_maps = hash:/etc/postfix/sasl_passwd
    smtp_sasl_security_options = noanonymous
    smtp_tls_security_level = may
    header_size_limit = 4096000

Also add this line since absent of this line to avoid errors
    smtpd_recipient_restrictions = permit_sasl_authenticated
    
Then create the /etc/postfix/sasl_passwd file.
    sudo nano /etc/postfix/sasl_passwd

Add the SMTP relay host and SMTP credentials to this file like below. 
Replace smtp_username and smtp_password with your own username and password that are given by SendinBlue. 

    [smtp-relay.sendinblue.com]:587      smtp_username:smtp_password    #tip: in case of sending email problems, try to delete the port 587

Create the corresponding hash db file with postmap.
    sudo postmap /etc/postfix/sasl_passwd --> Now you should have a file /etc/postfix/sasl_passwd.db. 

Restart Postfix for the changes to take effect.
    sudo systemctl restart postfix

Tip:    By default, sasl_passwd and sasl_passwd.db file can be read by any user on the server. 
        Change the permission to 600 so only root can read and write to these two files.
            sudo chmod 0600 /etc/postfix/sasl_passwd /etc/postfix/sasl_passwd.db

Use ss (Socket Statistics) utility to verify that Postfix master process is listening on TCP port 25.
    sudo ss -lnpt | grep master

.B Sending Test Email
    sudo apt install bsd-mailx mailutils mutt
    echo "this is a test email." | mailx -r from-address -s hello to-address
    echo "test email" | sendmail your-account@gmail.com   #sendmail is provided by postfix package among other utils in /usr/sbin/
    echo "this is test message by thingit.online" |mutt -s "Wind Alert" -- ge.vasiliou@gmail.com    # -s = subject

Tip: 
Edit the Mutt Configuration File ~/.muttrc and set the Sender Name: 
    set from = "Your Name <your-email@example.com>"
This is the easiest way to define the Sender appearance in remote systems.

.B Reading New Emails
The inbox for each user is located at /var/spool/mail/<username> or /var/mail/<username> file. 
If you are unsure where to look for the inbox, use this command.
    sudo postconf mail_spool_directory

Email Readers:
Classic way: mail or mailx -> Provides a list , you open the email by providing the email id (number).
Nice Way: mutt             -> looks like a complete email client that runs on the terminal. You can open / read emails, reply, forward, compose new mails, etc

Tip1:   mail & mailx Display the full message with headers. There is not any way to display only the message body.
        On the other hand, mutt "hides" the header data and displays only the body message & attachments (if any).

.B Reading Saved Emails (in mailbox).
postfix creates a mailbox to store read emails under home directory of the user i.e /home/admin/mbox
You can open those emails by specifying the mailbox to mutt:
    mutt -f ~/mbox
    mail -f ~/mbox

##BREVO
Detailed Installation Instructions to make it work with PostFix:
.I https://www.linuxbabe.com/mail-server/set-up-postfix-smtp-relay-debian-sendinblue

Brevo is the evolution of "sendinblue" service, used old-days.
The main service that Brevo offers is an smtp relay.
This is quite usefull in case that your web hosting service (like AWS) does not allow outgoing traffic on port 25.
So actually you make an account to Brevo, you get your username and password , you provide those details to postfix and you are capable
to send emails from AWS EC2 Debian Server using Brevo smtp relay.

Brevo offers a free plan with 300 emails / day - good for sending newsletters in various customers / mailing lists.

For Brevo to work, you need to "add" your domain (thingit.online) on the Brevo trusted sites, in Brevo Settings (web page).
Brevo will automatically authorize your domain by detecting registrar (namecheap) and make the necessary DNS changes for smtp relaying.
PS: You need to allow Brevo to get connected to namecheap with your credentials to add the required DNS records.

While the smtp changes made by Brevo for smtp relay work fine, the MX DNS records registered by Brevo does not work as expected.

It has been noted that replying to an email (i.e using gmail) received by thingit gives error -- > 554 5.7.1 : Relay access denied

This was solved by adjusting DNS MX records at namecheap manually to directly point to thingit.online instead of using the Brevo MX settings.
After this change, you can reply to thingit.online emails by gmail, corporate emails, etc without problem.

Tip: With all this tests, it is clear that having only an smtp server for sending / receiving email is enough. You don't need POP3 settings.
POP3 is used by GUI Email Readers to retrieve the emails from the server.

Tip: Since postfix and brevo are bind together in the server (postfix uses the brevo username and password stored in server) you can send an email by
your Debian Server without the need to provide smtp username and password everytime.

##MAILGUN
.I https://www.npmjs.com/package/mailgun.js?activeTab=readme

    npm install mailgun.js form-data

If you need a way to send email notification by node.js , then you need to provide your smtp credentials for i.e gmail.
In this aspect, either you set up your own mail server / smtp server at thingit or you can use 3rd party services like MailGun.

Mailgun provides API (or classic smtp support) to send email notifications without providing any username and password of your clients.

Mailgun webpage offers sample code for php, java, node.js , curl etc.
This is a sample node.js app :
const formData = require('form-data');
  const Mailgun = require('mailgun.js');
  const mailgun = new Mailgun(formData);
  const mg = mailgun.client({username: 'api', key: '64a24..........................52627ba0'}); //username must be api and not your mailgun username

  mg.messages.create('sandbox3fbd203650454002b7d8b2e05a1b82b2.mailgun.org', {
  	from: "Excited User <mailgun@sandbox3fbd203650454002b7d8b2e05a1b82b2.mailgun.org>",
  	to: ["someuser@gmail.com"],
  	subject: "Hello",
  	text: "Testing some Mailgun awesomeness!",
  	html: "<h1>Testing some Mailgun awesomeness!</h1>"
  })
  .then(msg => console.log(msg)) // logs response data
  .catch(err => console.log(err)); // logs any error

PS: Above code gives error claiming that the user is not identified and that sandbox accounts are for testing only.
